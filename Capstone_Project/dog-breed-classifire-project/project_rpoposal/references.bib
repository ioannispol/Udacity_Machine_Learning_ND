@book{Howse2020,
author = {Howse, Joseph},
booktitle = {Packt},
edition = {3rd},
isbn = {9781789530643},
pages = {372},
publisher = {Packt Publishing Ltd},
title = {{Learning OpenCV 4 Computer Vision with Python 3: Get to grips with tools, techniques, and algorithms for computer vision and machine learning}},
url = {https://sites.google.com/site/01apr20u/3Bgy75Vgy1839},
year = {2020}
}
@misc{OpenSourceComputerVision2018,
abstract = {OpenCV (Open Source Computer Vision Library: http://opencv.org) is an open-source BSD-licensed library that includes several hundreds of computer vision algorithms. The document describes the so-called OpenCV 2.x API, which is essentially a C++ API, as opposed to the C-based OpenCV 1.x API (C API is deprecated and not tested with "C" compiler since OpenCV 2.4 releases)},
author = {{Open Source Computer Vision}},
booktitle = {OpenCV},
title = {{OpenCV: OpenCV modules}},
url = {https://docs.opencv.org/4.3.0/ https://docs.opencv.org/3.4/index.html},
urldate = {2020-05-28},
year = {2018}
}
@article{Xiao2017,
abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
archivePrefix = {arXiv},
arxivId = {1708.07747},
author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
eprint = {1708.07747},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiao, Rasul, Vollgraf - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmarking Machine Learning Algorithms.pdf:pdf},
isbn = {1708.07747v2},
title = {{Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}},
url = {https://trends.google.com/trends/explore?date=all{\&}q=mnist,CIFAR,ImageNet http://arxiv.org/abs/1708.07747},
year = {2017}
}
@inproceedings{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2014 - Microsoft COCO Common objects in context.pdf:pdf},
issn = {16113349},
month = {may},
number = {PART 5},
pages = {740--755},
publisher = {Springer Verlag},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@inproceedings{Girshick,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012 - achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://www.cs.berkeley.edu/˜rbg/rcnn.},
year = {2014}
}
@article{Janai2017,
abstract = {Recent years have witnessed enormous progress in AI-related fields such as computer vision, machine learning, and autonomous vehicles. As with any rapidly growing field, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several survey papers on particular sub-problems have appeared, no comprehensive survey on problems, datasets, and methods in computer vision for autonomous vehicles has been published. This book attempts to narrow this gap by providing a survey on the state-of-the-art datasets and techniques. Our survey includes both the historically most relevant literature as well as the current state of the art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding, and end-to-end learning for autonomous driving. Towards this goal, we analyze the performance of the state of the art on several challenging benchmarking datasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we also provide a website that allows navigating topics as well as methods and provides additional information.},
archivePrefix = {arXiv},
arxivId = {1704.05519},
author = {Janai, Joel and G{\"{u}}ney, Fatma and Behl, Aseem and Geiger, Andreas},
eprint = {1704.05519},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Janai et al. - 2017 - Computer Vision for Autonomous Vehicles Problems, Datasets and State of the Art(3).pdf:pdf},
month = {apr},
title = {{Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art}},
url = {http://arxiv.org/abs/1704.05519},
year = {2017}
}
@article{Nguyen2019,
abstract = {The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.},
author = {Nguyen, Giang and Dlugolinsky, Stefan and Bob{\'{a}}k, Martin and Tran, Viet and {L{\'{o}}pez Garc{\'{i}}a}, {\'{A}}lvaro and Heredia, Ignacio and Mal{\'{i}}k, Peter and Hluch{\'{y}}, Ladislav},
doi = {10.1007/s10462-018-09679-z},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - 2019 - Machine Learning and Deep Learning frameworks and libraries for large-scale data mining a survey.pdf:pdf},
issn = {15737462},
journal = {Artificial Intelligence Review},
keywords = {Artificial Intelligence software,Deep Learning,Graphics processing unit (GPU),Intensive computing,Large-scale data mining,Machine Learning,Parallel processing},
month = {jun},
number = {1},
pages = {77--124},
publisher = {Springer Netherlands},
title = {{Machine Learning and Deep Learning frameworks and libraries for large-scale data mining: a survey}},
volume = {52},
year = {2019}
}
@book{Planche2019,
address = {Birmingham},
author = {Planche, Benjamin and Andres, Eliot},
isbn = {978-78883-064-5},
pages = {353},
publisher = {Packt Publishing},
title = {{Hands-On Computer Vision with TensorFlow 2}},
url = {https://tensorflowcomputervision.com/index.html},
year = {2019}
}
@inproceedings{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
eprint = {1312.6114},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Welling - 2014 - Auto-encoding variational bayes.pdf:pdf},
month = {dec},
title = {{Auto-encoding variational bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2014}
}
@book{Parker2010,
address = {New York, UNITED STATES},
author = {Parker, J R},
isbn = {9781118019627},
keywords = {Computer algorithms.,Computer vision.,Image processing -- Digital techniques.},
publisher = {John Wiley {\&} Sons, Incorporated},
title = {{Algorithms for Image Processing and Computer Vision}},
url = {http://ebookcentral.proquest.com/lib/ncl/detail.action?docID=706758},
year = {2010}
}
@article{Janai2019,
abstract = {Recent years have witnessed enormous progress in AI-related fields such as computer vision, machine learning, and autonomous vehicles. As with any rapidly growing field, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several survey papers on particular sub-problems have appeared, no comprehensive survey on problems, datasets, and methods in computer vision for autonomous vehicles has been published. This book attempts to narrow this gap by providing a survey on the state-of-the-art datasets and techniques. Our survey includes both the historically most relevant literature as well as the current state of the art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding, and end-to-end learning for autonomous driving. Towards this goal, we analyze the performance of the state of the art on several challenging benchmarking datasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we also provide a website that allows navigating topics as well as methods and provides additional information.},
archivePrefix = {arXiv},
arxivId = {1704.05519},
author = {Janai, Joel and G{\"{u}}ney, Fatma and Behl, Aseem and Geiger, Andreas},
eprint = {1704.05519},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Janai et al. - 2017 - Computer Vision for Autonomous Vehicles Problems, Datasets and State of the Art.pdf:pdf},
title = {{Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art}},
url = {http://arxiv.org/abs/1704.05519},
year = {2017}
}
@misc{Dmitriev2020,
author = {Dmitriev, Denis},
title = {{NEURAL NETWORKS IN ROBOTICS}},
url = {https://www.cybercontrols.org/neuralnetworks},
urldate = {2020-03-02},
year = {2020}
}
@misc{JPL2009,
abstract = {An artist's concept portrays a NASA Mars Exploration Rover on the surface of Mars. Two rovers were launched in 2003 and arrived at sites on Mars in January 2004. Each rover was built to have the mobility and toolkit for functioning as a robotic geologist.},
author = {JPL and NASA and Cornell, University},
doi = {10.1007/978-3-662-44185-5_100678},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/JPL, NASA, Cornell - 2003 - Mars Exploration Rovers (MERs).jpg:jpg},
title = {{Mars Exploration Rovers (MERs)}},
url = {https://photojournal.jpl.nasa.gov/jpeg/PIA04413.jpg},
urldate = {2020-05-03},
year = {2003}
}
@article{Tomasi1992,
abstract = {Inferring scene geometry and camera motion from a stream of images is possible in principle, but it is an ill-conditioned problem when the objects are distant with respect to their size. We have developed a factorization method that can overcome this difficulty by recovering shape and motion without computing depth as an intermediate step. An image stream can be represented by the 2F x P measurement matrix of the image coordinates of P points tracked through F frames. Under orthographic projection this matrix is of rank 3. Using this observation, the factorization method uses the singular value decomposition technique to factor the measurement matrix into two matrices, which represent object shape and camera motion, respectively. The method can also handle and obtain a full solution from a partially filled-in measurement matrix, which occurs when features appear and disappear in the image sequence due to occlusions or tracking failures. The method gives accurate results and does not introduce smoothing in either shape or motion. We demonstrate this with a series of experiments on laboratory and outdoor image streams, with and without occlusions.},
author = {Tomasi, Carlo and Kanade, Takeo},
doi = {10.1073/pnas.90.21.9795},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tomasi, Kanade - 1993 - Shape and motion from image streams A factorization method.pdf:pdf},
issn = {00278424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {21},
pages = {9795--9802},
title = {{Shape and motion from image streams: A factorization method}},
volume = {90},
year = {1993}
}
@techreport{Turk1991,
abstract = {We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting},
author = {Turk, Matthew and Pentland, Alex},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turk, Pentland - 1991 - Eigenfaces for Recognition.pdf:pdf},
title = {{Eigenfaces for Recognition}},
year = {1991}
}
@article{Geman1984,
abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, non-linear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low -energy states (‘annealing ), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ‘relaxation' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios. {\textcopyright} 1993, Taylor {\&} Francis Group, LLC. All rights reserved.},
author = {Geman, Donald and Geman, Stuart},
doi = {10.1080/02664769300000058},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Geman, Geman - 1984 - Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images.pdf:pdf},
issn = {13600532},
journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
number = {5-6},
pages = {721--741},
title = {{Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images}},
volume = {6},
year = {1984}
}
@article{Terzopoulos1986,
abstract = {Inverse problems, such as the reconstruction problems that arise in early vision, tend to be mathematically ill-posed. Through regularization, they may be reformulated as well-posed variational principles whose solutions are computable. Standard regularization theory employs quadratic stabilizing functionals that impose global smoothness constraints on possible solutions. Discontinuities present serious difficulties to standard regularization, however, since their reconstruction requires a precise spatial control over the smoothing properties of stabilizers. This paper proposes a general class of controlled-continuity stabilizers which provide the necessary control over smoothness. These nonquadratic stabilizing functionals comprise multiple generalized spline kernels combined with (noncontinuous) continuity control functions. In the context of computational vision, they may be thought of as controlled-continuity constraints. These generic constraints are applicable to visual reconstruction problems that involve both continuous regions and discontinuities, for which global smoothness constraints fail. {\textcopyright} 1986 IEEE},
author = {Terzopoulos, Demetri},
doi = {10.1109/TPAMI.1986.4767807},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Terzopoulos - 1986 - Regularization of Inverse Visual Problems Involving Discontinuities.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Controlled-continuity constraints,discontinuities,early vision,inverse problems,reconstruction,regularization},
number = {4},
pages = {413--424},
title = {{Regularization of Inverse Visual Problems Involving Discontinuities}},
volume = {PAMI-8},
year = {1986}
}
@article{Terzopoulos1983,
abstract = {A computational theory of visual surface reconstruction is presented. This theory extends in a natural way the multilevel structure of the earliest processing stages in vision to later stages that reconstruct full, retinocentric surface representations from local information about surface shape at scattered locations. The surface reconstruction problem is formulated as a variational principle describing the equilibria of a thin flexible plate. Optimal discrete approximations to the variational principle are obtained via the finite element method which utilizes local (finite element) representations of surfaces. The resulting discrete problem takes the form of a large, sparse system of linear equations.-from Author},
author = {Terzopoulos, D.},
doi = {10.1016/0734-189X(83)90020-8},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Terzopoulos - 1983 - Multilevel computational processes for visual surface reconstruction.pdf:pdf},
issn = {0734189X},
journal = {Computer Vision, Graphics, {\&} Image Processing},
month = {oct},
number = {1},
pages = {52--96},
publisher = {Academic Press},
title = {{Multilevel computational processes for visual surface reconstruction.}},
volume = {24},
year = {1983}
}
@article{Rosenfeld1966,
author = {Rosenfeld, Azriel and Pfaltz, John L.},
doi = {10.1145/321356.321357},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosenfeld, Pfaltz - 1966 - Sequential Operations in Digital Picture Processing.pdf:pdf},
issn = {1557735X},
journal = {Journal of the ACM (JACM)},
month = {oct},
number = {4},
pages = {471--494},
title = {{Sequential Operations in Digital Picture Processing}},
volume = {13},
year = {1966}
}
@techreport{Huang1997,
abstract = {In this paper we give a somewhat personal and perhaps biased overview of the field of Computer Vision. First, we define computer vision and give a very brief history of it. Then, we outline some of the reasons why computer vision is a very difficult research field. Finally, we discuss past, present, and future applications of computer vision. Especially, we give some examples of future applications which we think are very promising.},
author = {Huang, T S},
booktitle = {Report},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang - 1997 - Computer Vision Evolution and Promise.pdf:pdf},
pages = {5},
title = {{Computer Vision: Evolution and Promise}},
year = {1997}
}
@techreport{Forsyth2012,
author = {Forsyth, David A and Ponce, Jean and Columbus, Boston and New, Indianapolis and {Rk San}, Yo and Upper, Francisco and River, Saddle and Cape, Amsterdam and Dubai, Town and Madrid, London and Munich, Milan and Montreal, Paris and Delhi, Toronto and Sao, Mexico City and Sydney, Paulo and Kong, Hong and Singapore, Seoul and Tokyo, Taipei},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Forsyth et al. - 2012 - Computer Vision A MODERN APPROACH second edition Ecole Normale Sup{\'{e}}rieure.pdf:pdf},
isbn = {013608592X},
title = {{Computer Vision A MODERN APPROACH second edition Ecole Normale Sup{\'{e}}rieure}},
year = {2012}
}
@article{Paoletti2018,
abstract = {Artificial neural networks (ANNs) have been widely used for the analysis of remotely sensed imagery. In particular, convolutional neural networks (CNNs) are gaining more and more attention in this field. CNNs have proved to be very effective in areas such as image recognition and classification, especially for the classification of large sets composed by two-dimensional images. However, their application to multispectral and hyperspectral images faces some challenges, especially related to the processing of the high-dimensional information contained in multidimensional data cubes. This results in a significant increase in computation time. In this paper, we present a new CNN architecture for the classification of hyperspectral images. The proposed CNN is a 3-D network that uses both spectral and spatial information. It also implements a border mirroring strategy to effectively process border areas in the image, and has been efficiently implemented using graphics processing units (GPUs). Our experimental results indicate that the proposed network performs accurately and efficiently, achieving a reduction of the computation time and increasing the accuracy in the classification of hyperspectral images when compared to other traditional ANN techniques.},
author = {Paoletti, M. E. and Haut, J. M. and Plaza, J. and Plaza, A.},
doi = {10.1016/j.isprsjprs.2017.11.021},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paoletti et al. - 2018 - A new deep convolutional neural network for fast hyperspectral image classification.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Classification,Convolutional neural networks (CNNs),Deep learning,Graphics processing units (GPUs),Hyperspectral imaging},
month = {nov},
pages = {120--147},
publisher = {Elsevier B.V.},
title = {{A new deep convolutional neural network for fast hyperspectral image classification}},
volume = {145},
year = {2018}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
eprint = {1603.07285},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dumoulin, Visin - 2016 - A guide to convolution arithmetic for deep learning.pdf:pdf},
title = {{A guide to convolution arithmetic for deep learning}},
url = {http://ethanschoonover.com/solarized http://arxiv.org/abs/1603.07285},
year = {2016}
}
@techreport{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
booktitle = {Journal of Machine Learning Research},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@techreport{Haber2019,
abstract = {Deep neural networks have become invaluable tools for supervised machine learning, e.g., classification of text or images. While often offering superior results over traditional techniques and successfully expressing complicated patterns in data, deep architectures are known to be challenging to design and train such that they generalize well to new data. Critical issues with deep architectures are numerical instabilities in derivative-based learning algorithms commonly called exploding or vanishing gradients. In this paper, we propose new forward propagation techniques inspired by systems of Ordinary Differential Equations (ODE) that overcome this challenge and lead to well-posed learning problems for arbitrarily deep networks. The backbone of our approach is our interpretation of deep learning as a parameter estimation problem of nonlinear dynamical systems. Given this formulation, we analyze stability and well-posedness of deep learning and use this new understanding to develop new network architectures. We relate the exploding and vanishing gradient phenomenon to the stability of the discrete ODE and present several strategies for stabilizing deep learning for very deep networks. While our new architectures restrict the solution space, several numerical experiments show their competitiveness with state-of-the-art networks.},
archivePrefix = {arXiv},
arxivId = {1705.03341v3},
author = {Haber, Eldad and Ruthotto, Lars},
eprint = {1705.03341v3},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haber, Ruthotto - 2019 - Stable Architectures for Deep Neural Networks.pdf:pdf},
keywords = {Deep Neural Networks,Dynamic Inverse Problems,Image Classification,Machine Learning,PDE-Constrained Optimization,Parameter Estimation},
title = {{Stable Architectures for Deep Neural Networks}},
year = {2019}
}
@article{Seal1967,
abstract = {STTMMABY The linear regression model owes so much to Gauss that we believe it should bear his name. Other authors who made substantial contributions are: Cauchy who introduced the idea of orthogonality; Chebyshev who applied it to polynomial models; Pizzetti who found the distribution of the sum of squares of the residuals on the Normal assumption; Karl Pearson who linked the model with the multivariate Normal thereby broadening the field of applications; and R. A. Fisher whose extension of orthogonality to qualitative comparisons laid the foundations of the modern theory of experimental design.},
author = {Seal, Hilary L},
doi = {10.1093/biomet/54.1-2.1},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seal - 1967 - Studies in the history of probability and statistics. XV. The historical velopment of the Gauss linear model.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
number = {1},
pages = {1--24},
title = {{Studies in the history of probability and statistics. XV. The historical velopment of the Gauss linear model.}},
url = {https://academic.oup.com/biomet/article-abstract/54/1-2/1/331494},
volume = {54},
year = {1967}
}
@techreport{Ngiam2011,
abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLet-ters datasets on audiovisual speech classification , demonstrating best published visual speech classification on AVLetters and effective shared representation learning.},
author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ngiam et al. - 2011 - Multimodal Deep Learning.pdf:pdf},
title = {{Multimodal Deep Learning}},
year = {2011}
}
@book{Deng2013,
abstract = {This monograph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning. {\textcopyright} 2014 L. Deng and D. Yu.},
author = {Deng, Li and Yu, Dong},
booktitle = {Foundations and Trends in Signal Processing},
doi = {10.1561/2000000039},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng, Yu - 2013 - Deep learning Methods and applications.pdf:pdf},
issn = {19328354},
number = {3-4},
pages = {197--387},
title = {{Deep learning: Methods and applications}},
volume = {7},
year = {2013}
}
@misc{Lecun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
booktitle = {Nature},
doi = {10.1038/nature14539},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lecun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
issn = {14764687},
month = {may},
number = {7553},
pages = {436--444},
pmid = {26017442},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@misc{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
booktitle = {Neural Networks},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidhuber - 2015 - Deep Learning in neural networks An overview.pdf:pdf},
issn = {18792782},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
month = {jan},
pages = {85--117},
pmid = {25462637},
publisher = {Elsevier Ltd},
title = {{Deep Learning in neural networks: An overview}},
volume = {61},
year = {2015}
}
@techreport{Ng,
abstract = {We now begin our study of deep learning. In this set of notes, we give an overview of neural networks, discuss vectorization and discuss training neural networks with backpropagation.},
author = {Ng, Andrew and Katanforoosh, Kian},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng, Katanforoosh - Unknown - CS229 Lecture Notes Deep Learning.pdf:pdf},
title = {{CS229 Lecture Notes Deep Learning}}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F},
doi = {10.1037/h0042519},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosenblatt - 1958 - The perceptron A probabilistic model for information storage and organization in the brain.pdf:pdf},
issn = {0033295X},
journal = {Psychological Review},
keywords = {AS INFORMATION STORAGE MODEL INFORMATION,IN BRAIN BRAIN,INFORMATION STORAGE IN,MODEL FOR,MODEL FOR LEARNING {\&} MEMORY,PERCEPTION,STORAGE},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
volume = {65},
year = {1958}
}
@techreport{Swati2012,
abstract = {Lung cancer is the most important cause of cancer death for both men and women.},
author = {Swati, Ms and Tidke, P and Vrishali, Prof and Chakkarwar, A 3},
booktitle = {International Journal Of Computational Engineering Research (ijceronline.com},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Swati et al. - 2012 - Classification of Lung Tumor Using SVM.pdf:pdf},
pages = {1254},
title = {{Classification of Lung Tumor Using SVM}},
volume = {2},
year = {2012}
}
@phdthesis{Wang,
abstract = {Recently a new learning method called support vector machines (SVM) has shown comparable or better results than neural networks on some applications. In this thesis we exploit the possibility of using SVM for three important issues of bioinformatics: the prediction of protein secondary structure, multi-class protein fold recognition, and the prediction of human signal peptide cleavage sites. By using similar data, we demonstrate that SVM can easily achieve comparable accuracy as using neural networks. Therefore, in the future it is a promising direction to apply SVM on more bioinformatics applications. ii ACKNOWLEDGEMENTS},
author = {Dasgupta, Shibasish},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dasgupta - 2002 - Application of Support Vector Machines in classification problems.pdf:pdf},
title = {{Application of Support Vector Machines in classification problems}},
year = {2002}
}
@article{Maity2016,
abstract = {The pixel percentage belonging to the user defined area that are assigned to cluster in a confusion matrix for RADARSAT-2 over Vancouver area has been analysed for classification. In this study, supervised Wishart and Support Vector Machine (SVM) classifiers over RADARSAT-2 (RS2) fine quadpol mode Single Look Complex (SLC) product data is computed and compared. In comparison with conventional single channel or dual channel polarization, RADARSAT-2 is fully polarimetric, making it to offer better land feature contrast for classification operation.},
archivePrefix = {arXiv},
arxivId = {1608.00501},
author = {Maity, Abhishek},
eprint = {1608.00501},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maity - 2016 - Supervised Classification of RADARSAT-2 Polarimetric Data for Different Land Features.pdf:pdf},
month = {aug},
title = {{Supervised Classification of RADARSAT-2 Polarimetric Data for Different Land Features}},
url = {http://arxiv.org/abs/1608.00501},
year = {2016}
}
@article{Decoste2002,
abstract = {Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods.},
author = {Decoste, Dennis and Sch{\"{o}}lkopf, Bernhard},
doi = {10.1023/A:1012454411458},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Decoste, Sch{\"{o}}lkopf - 2002 - Training invariant support vector machines.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Image classification,Invariance,Pattern recognition,Prior knowledge,Support vector machines},
number = {1-3},
pages = {161--190},
title = {{Training invariant support vector machines}},
volume = {46},
year = {2002}
}
@article{Moguerza2006,
abstract = {Support vector machines (SVMs) appeared in the early nineties as optimal margin classifiers in the context of Vapnik's statistical learning theory. Since then SVMs have been successfully applied to real-world data analysis problems, often providing improved results compared with other techniques. The SVMs operate within the framework of regularization theory by minimizing an empirical risk in a well-posed and consistent way. A clear advantage of the support vector approach is that sparse solutions to classification and regression problems are usually obtained: only a few samples are involved in the determination of the classification or regression functions. This fact facilitates the application of SVMs to problems that involve a large amount of data, such as text processing and bioinformatics tasks. This paper is intended as an introduction to SVMs and their applications, emphasizing their key features. In addition, some algorithmic extensions and illustrative real-world applications of SVMs are shown. {\textcopyright} Institute of Mathematical Statistics, 2006.},
author = {Moguerza, Javier M and Mu{\~{n}}oz, Alberto},
doi = {10.1214/088342306000000493},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moguerza, Mu{\~{n}}oz - 2006 - Support vector machines with applications.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Classification,Inverse problems,Kernel methods,Regularization theory,Support vector machines},
number = {3},
pages = {322--336},
title = {{Support vector machines with applications}},
volume = {21},
year = {2006}
}
@inproceedings{Evgeniou2001,
author = {Evgeniou, Theodoros and Pontil, Massimiliano},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/3-540-44673-7_12},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evgeniou, Pontil - 2001 - Support vector machines Theory and applications.pdf:pdf},
isbn = {3540424903},
issn = {03029743},
pages = {249--257},
title = {{Support vector machines: Theory and applications}},
url = {http://link.springer.com/10.1007/3-540-44673-7{\_}12},
volume = {2049 LNAI},
year = {2001}
}
@techreport{Chang2001,
abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems, theoretical convergence, multi-class classification, probability estimates, and parameter selection are discussed in detail.},
author = {Chang, Chih-Chung and Lin, Chih-Jen},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang, Lin - 2001 - LIBSVM A Library for Support Vector Machines.pdf:pdf},
keywords = {Classification,LIBSVM,SVM,optimization,regression,support vector ma-chines},
title = {{LIBSVM: A Library for Support Vector Machines}},
url = {www.csie.ntu.edu.tw/},
year = {2001}
}
@book{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
author = {et. all. Hastie, Trevor},
booktitle = {The Mathematical Intelligencer},
doi = {10.1007/b94608},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hastie - 2009 - Springer Series in Statistics The Elements of Statistical Learning.pdf:pdf},
isbn = {9780387848570},
issn = {03436993},
number = {2},
pages = {83--85},
pmid = {15512507},
title = {{Springer Series in Statistics The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf},
volume = {27},
year = {2009}
}
@misc{Scikit-learn,
author = {Scikit-learn},
title = {{1.4. Support Vector Machines — scikit-learn 0.22.2 documentation}},
url = {https://scikit-learn.org/stable/modules/svm.html},
urldate = {2020-04-26},
year = {2020}
}
@techreport{Cortes1995,
abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
author = {Cortes, Corinna},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortes - 1995 - Support-Vector Networks.pdf:pdf},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
pages = {273--297},
title = {{Support-Vector Networks}},
volume = {20},
year = {1995}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
eprint = {1609.04747},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruder - 2016 - An overview of gradient descent optimization algorithms.pdf:pdf},
month = {sep},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
year = {2016}
}
@incollection{Kleinbaum2010,
abstract = {Twitter is a public social service that allows users to share information as short-text messages. Previous researchers have tried to analyze the information available on Twitter to discover topic trending. However, these topics are associated with the whole network, and are not associated to a particular place. In this work, we propose a scheme to discover places where something is happening by analyzing geo-tagged tweets in a timely manner. By using the proposed scheme, users will be able to recognize spontaneous events happening nearby, and be notified about it through their smart phones. Experimental results show that by using geo-tagged information associated to tweets, events related to a particular place can be detected using clustering techniques and semantic interpretation of keywords.},
author = {Kleinbaum, David G. and Klein, Mitchel},
doi = {10.1007/978-1-4419-1742-3_1},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kleinbaum, Klein - 2010 - Introduction to Logistic Regression.pdf:pdf},
pages = {1--39},
title = {{Introduction to Logistic Regression}},
year = {2010}
}
@misc{Tolles2016,
author = {Tolles, Juliana and Meurer, William J.},
booktitle = {JAMA - Journal of the American Medical Association},
doi = {10.1001/jama.2016.7653},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tolles, Meurer - 2016 - Logistic regression Relating patient characteristics to outcomes.pdf:pdf},
issn = {15383598},
keywords = {inferential statistics,logistic regression,multivariate analysis,statistics},
month = {aug},
number = {5},
pages = {533--534},
publisher = {American Medical Association},
title = {{Logistic regression: Relating patient characteristics to outcomes}},
volume = {316},
year = {2016}
}
@incollection{Oladipupo2010,
abstract = {This chapter gives some background about machine learning as a whole and some of the techniques that touch all parts of it. "Machine learning" has become a catchall term that covers a lot of different areas, ranging from classification to clustering. Machine learning was partly born out of the initial failures of the artificial intelligence (AI) movement. Most machine learning algorithms are designed to handle pretty much any tabular dataset, but they ONLY handle tabular data. Tabular data lends itself to all kinds of mathematical analysis, since the rows of a table with n rows and d columns can be viewed as locations in d-dimensional space. This is why machine learning is easily the most mathematically sophisticated thing a data scientist is likely to do. Machine learning is much more probabilistic in the way it makes models and inferences. There are two main types of machine learning, called supervised and unsupervised.},
author = {Oladipupo, Taiwo},
booktitle = {New Advances in Machine Learning},
doi = {10.5772/9374},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oladipupo - 2010 - Machine Learning Overview.pdf:pdf},
month = {feb},
publisher = {InTech},
title = {{Machine Learning Overview}},
year = {2010}
}
@inproceedings{Ganin2018,
abstract = {Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and Tenderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator's output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, OMNIGLOT, CELEBA) and synthetic 3D datasets. A video of the agent can be found at https://youtu.be/iSyvwAwa7vk.},
archivePrefix = {arXiv},
arxivId = {1804.01118},
author = {Ganin, Yaroslav and Kulkarni, Tejas and Babuschkin, Igor and Eslami, S. M.Ali and Vinyals, Oriol},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1804.01118},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganin et al. - 2018 - Synthesizing Programs for Images using Reinforced Adversarial Learning.pdf:pdf},
isbn = {9781510867963},
pages = {2684--2695},
title = {{Synthesizing Programs for Images using Reinforced Adversarial Learning}},
url = {https://youtu.be/iSyvwAwa7vk.},
volume = {4},
year = {2018}
}
@techreport{Mnih,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - Unknown - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
title = {{Playing Atari with Deep Reinforcement Learning}}
}
@techreport{Jin2018,
abstract = {Real-time advertising allows advertisers to bid for each impression for a visiting user. To optimize specific goals such as maximizing revenue and return on investment (ROI) led by ad placements, advertisers not only need to estimate the relevance between the ads and user's interests, but most importantly require a strategic response with respect to other advertisers bidding in the market. In this paper, we formulate bidding optimization with multi-agent reinforcement learning. To deal with a large number of advertisers, we propose a clustering method and assign each cluster with a strategic bidding agent. A practical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed and implemented to balance the trade-off between the competition and cooperation among advertisers. The empirical study on our industry-scaled real-world data has demonstrated the effectiveness of our methods. Our results show cluster-based bidding would largely outperform single-agent and bandit approaches, and the coordinated bidding achieves better overall objectives than purely self-interested bidding agents.},
archivePrefix = {arXiv},
arxivId = {1802.09756v2},
author = {Jin, Junqi and Song, Chengru and Li, Han and Gai, Kun and Wang, Jun and Zhang, Weinan},
eprint = {1802.09756v2},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin et al. - 2018 - Real-Time Bidding with Multi-Agent Reinforcement Learning in Display Advertising.pdf:pdf},
isbn = {1802.09756v2},
keywords = {Bid Optimization,Display Advertising,Multi-Agent Reinforcement Learning,Real-Time Bidding},
title = {{Real-Time Bidding with Multi-Agent Reinforcement Learning in Display Advertising}},
year = {2018}
}
@article{Zhou2017,
abstract = {Deep reinforcement learning was employed to optimize chemical reactions. Our model iteratively records the results of a chemical reaction and chooses new experimental conditions to improve the reaction outcome. This model outperformed a state-of-the-art blackbox optimization algorithm by using 71{\%} fewer steps on both simulations and real reactions. Furthermore, we introduced an efficient exploration strategy by drawing the reaction conditions from certain probability distributions, which resulted in an improvement on regret from 0.062 to 0.039 compared with a deterministic policy. Combining the efficient exploration policy with accelerated microdroplet reactions, optimal reaction conditions were determined in 30 min for the four reactions considered, and a better understanding of the factors that control microdroplet reactions was reached. Moreover, our model showed a better performance after training on reactions with similar or even dissimilar underlying mechanisms, which demonstrates its learning ability.},
author = {Zhou, Zhenpeng and Li, Xiaocheng and Zare, Richard N.},
doi = {10.1021/acscentsci.7b00492},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Li, Zare - 2017 - Optimizing Chemical Reactions with Deep Reinforcement Learning.pdf:pdf},
issn = {23747951},
journal = {ACS Central Science},
month = {dec},
number = {12},
pages = {1337--1344},
publisher = {American Chemical Society},
title = {{Optimizing Chemical Reactions with Deep Reinforcement Learning}},
volume = {3},
year = {2017}
}
@techreport{Kober2013,
abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value function-based and policy search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
author = {Kober, Jens and {Andrew Bagnell}, † J and Peters, Jan},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kober, Andrew Bagnell, Peters - 2013 - Reinforcement Learning in Robotics A Survey.pdf:pdf},
keywords = {learning control,reinforcement learning,robot,survey},
title = {{Reinforcement Learning in Robotics: A Survey}},
year = {2013}
}
@techreport{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702v5},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
booktitle = {Journal of Machine Learning Research},
eprint = {1504.00702v5},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Levine et al. - 2016 - End-to-End Training of Deep Visuomotor Policies.pdf:pdf},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
pages = {1--40},
title = {{End-to-End Training of Deep Visuomotor Policies}},
volume = {17},
year = {2016}
}
@article{Arel2009,
abstract = {A challenging application of artificial intelligence systems involves the scheduling of traffic signals in multi-intersection vehicular networks. This paper introduces a novel use of a multi-agent system and reinforcement learning (RL) framework to obtain an efficient traffic signal control policy. The latter is aimed at minimising the average delay, congestion and likelihood of intersection cross-blocking. A five-intersection traffic network has been studied in which each intersection is governed by an autonomous intelligent agent. Two types of agents, a central agent and an outbound agent, were employed. The outbound agents schedule traffic signals by following the longest-queue-first (LQF) algorithm, which has been proved to guarantee stability and fairness, and collaborate with the central agent by providing it local traffic statistics. The central agent learns a value function driven by its local and neighbours' traffic conditions. The novel methodology proposed here utilises the Q-Learning algorithm with a feedforward neural network for value function approximation. Experimental results clearly demonstrate the advantages of multi-agent RL-based control over LQF governed isolated single-intersection control, thus paving the way for efficient distributed traffic signal control in complex settings.},
author = {Arel, I and Liu, C and Urbanik, T and Kohls, A G},
doi = {10.1049/iet-its.2009.0070},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arel et al. - 2009 - Reinforcement learning-based multi-agent system for network traffic signal control.pdf:pdf},
title = {{Reinforcement learning-based multi-agent system for network traffic signal control}},
url = {www.ietdl.org},
year = {2009}
}
@article{Mao,
abstract = {Resource management problems in systems and networking often manifest as difficult online decision making tasks where appropriate solutions depend on understanding the workload and environment. Inspired by recent advances in deep reinforcement learning for AI problems, we consider building systems that learn to manage resources directly from experience. We present DeepRM, an example solution that translates the problem of packing tasks with multiple resource demands into a learning problem. Our initial results show that DeepRM performs comparably to state-of-the-art heuristics, adapts to different conditions, converges quickly, and learns strategies that are sensible in hindsight.},
author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
doi = {10.1145/3005745.3005750},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - Unknown - Resource Management with Deep Reinforcement Learning.pdf:pdf},
isbn = {9781450346610},
title = {{Resource Management with Deep Reinforcement Learning}},
url = {http://dx.doi.org/10.1145/3005745.3005750}
}
@techreport{Bertsekas,
abstract = {There has been a great deal of research recently on dynamic programming methods that replace the optimal cost-to-go function with a suitable approximation. These methods are collectively known as neuro-dynamic programming or reinforcement learning, and are described in a number of sources, including the books by Bertsekas and Tsitsiklis (1996) and Sutton and Barto (1988). In this paper, we provide an overview of the major conceptual issues, and we survey a number of recent developments, including rollout algorithms which are related to recent advances in model predictive control for chemical processes. Neuro-dynamic programming (NDP for short) is a relatively new class of dynamic programming methods for control and sequential decision making under uncertainty. These methods have the potential of dealing with problems that for a long time were thought to be intractable due to either a large state space or the lack of an accurate model. They combine ideas from the fields of neural networks, artificial intelligence, cognitive science, simulation, and approximation theory. We will delineate the major conceptual issues, survey a number of recent developments, describe some computational experience, and address a number of open questions. We consider systems where decisions are made in stages. The outcome of each decision is not fully predictable but can be anticipated to some extent before the next decision is made. Each decision results in some immediate cost but also affects the context in which future decisions are to be made and therefore affects the cost incurred in future stages. Dynamic programming (DP for short) provides a mathematical formalization of the tradeoff between immediate and future costs. Generally, in DP formulations there is a discrete-time dynamic system whose state evolves according to given transition probabilities that depend on a deci-sion/control u. In particular, if we are in state i and we choose decision u, we move to state j with given probability p ij (u). Simultaneously with this transition, we incur a cost g(i, u, j). In comparing, however, the available decisions u, it is not enough to look at the magnitude of the cost g(i, u, j); we must also take into account how desirable the next state j is. We thus need a way to rank or rate states j. This is done by using the optimal cost (over all remaining stages) starting from state j, which is denoted by J * (j). These costs can be shown to satisfy some form of Bellman's equation J * (i) = min u E{\{}g(i, u, j) + J * (j) | i, u{\}}, for all i, where j is the state subsequent to i, and E{\{}{\textperiodcentered} | i, u{\}} de-* bertsekas@lids.mit.edu noted expected value with respect to j, given i and u. Generally, at each state i, it is optimal to use a control u that attains the minimum above. Thus, decisions are ranked based on the sum of the expected cost of the present period, and the optimal expected cost of all subsequent periods. The objective of DP is to calculate numerically the optimal cost function J *. This computation can be done off-line, i.e., before the real system starts operating. An optimal policy, that is, an optimal choice of u for each i, is computed either simultaneously with J * , or in real time by minimizing in the right-hand side of Bellman's equation. It is well known, however, that for many important problems the computational requirements of DP are overwhelming, mainly because of a very large number of states and controls (Bellman's "curse of dimen-sionality"). In such situations a suboptimal solution is required. Cost Approximations in Dynamic Programming NDP methods are suboptimal methods that center around the approximate evaluation of the optimal cost function J * , possibly through the use of neural networks and/or simulation. In particular, we replace the optimal cost J * (j) with a suitable approximatio{\~{n}} J(j, r), where r is a vector of parameters, and we use at state i the (suboptimal) control˜µcontrol˜ control˜µ(i) that attains the minimum in the (approximate) right-hand side of Bellman's equation ˜ µ(i) = arg min u E{\{}g(i, u, j) + ˜ J(j, r) | i, u{\}}. The functio{\~{n}} J will be called the scoring function, and the value˜Jvalue˜ value˜J(j, r) will be called the score of state j. The general form of˜Jof˜ of˜J is known and is such that once the parameter vector r is determined, the evaluation of˜Jof˜ of˜J(j, r) of any state j is fairly simple. We note that in some problems the minimization over 92},
author = {Bertsekas, Dimitri P},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertsekas - Unknown - Neuro-Dynamic Programming An Overview.pdf:pdf},
keywords = {Dynamic programming,Neuro-dynamic programming,Optimal control,Reinforcement learning,Suboptimal control},
title = {{Neuro-Dynamic Programming: An Overview}}
}
@book{Sutton1998,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S and Barto, Andrew G},
booktitle = {Trends in Cognitive Sciences},
doi = {10.1016/S1364-6613(99)01331-5},
eprint = {1603.02199},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton, Barto - 1998 - Reinforcement Learning An Introduction, Second edition in progress.pdf:pdf},
issn = {13646613},
number = {9},
pages = {360},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction, Second edition in progress}},
volume = {3},
year = {1998}
}
@article{Kaelbling1996,
abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word "reinforcement." The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
doi = {10.1613/jair.301},
eprint = {9605103},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaelbling, Littman, Moore - 1996 - Reinforcement learning A survey.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
month = {may},
pages = {237--285},
primaryClass = {cs},
title = {{Reinforcement learning: A survey}},
volume = {4},
year = {1996}
}
@inproceedings{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {1--9},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@inproceedings{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in neural information processing systems},
doi = {10.1145/3065386},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet classification with deep convolutional neural networks.pdf:pdf},
issn = {15577317},
number = {6},
pages = {1097----1105},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://code.google.com/p/cuda-convnet/},
volume = {60},
year = {2012}
}
@incollection{Coates2012,
abstract = {Many algorithms are available to learn deep hierarchies of features from unlabeled data, especially images. In many cases, these algorithms involve multi-layered networks of features (e.g., neural networks) that are sometimes tricky to train and tune and are difficult to scale up to many machines effectively. Recently, it has been found that K-means clustering can be used as a fast alternative training method. The main advantage of this approach is that it is very fast and easily implemented at large scale. On the other hand, employing this method in practice is not completely trivial: K-means has several limitations, and care must be taken to combine the right ingredients to get the system to work well. This chapter will summarize recent results and technical tricks that are needed to make effective use of K-means clustering for learning large-scale representations of images. We will also connect these results to other well-known algorithms to make clear when K-means can be most useful and convey intuitions about its behavior that are useful for debugging and engineering new systems.},
author = {Coates, Adam and Ng, Andrew Y},
doi = {10.1007/978-3-642-35289-8_30},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Coates, Ng - 2012 - Learning Feature Representations with K-Means.pdf:pdf},
pages = {561--580},
title = {{Learning Feature Representations with K-Means}},
year = {2012}
}
@inproceedings{Radford2016,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1511.06434},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Radford, Metz, Chintala - 2016 - Unsupervised representation learning with deep convolutional generative adversarial networks.pdf:pdf},
title = {{Unsupervised representation learning with deep convolutional generative adversarial networks}},
year = {2016}
}
@article{Oja2002,
abstract = {In this article, we consider unsupervised learning from the point of view of applying neural computation on signal and data analysis problems. The article is an introductory survey, concentrating on the main principles and categories of unsupervised learning. In neural computation, there are two classical categories for unsupervised learning methods and models: first, extensions of principal component analysis and factor analysis, and second, learning vector coding or clustering methods that are based on competitive learning. These are covered in this article. The more recent trend in unsupervised learning is to consider this problem in the framework of probabilistic generative models. If it is possible to build and estimate a model that explains the data in terms of some latent variables, key insights may be obtained into the true nature and structure of the data. This approach is also briefly reviewed. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Oja, Erkki},
doi = {10.1016/S0304-3975(02)00160-3},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oja - 2002 - Unsupervised learning in neural computation.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
number = {1},
pages = {187--207},
title = {{Unsupervised learning in neural computation}},
url = {www.elsevier.com/locate/tcs},
volume = {287},
year = {2002}
}
@phdthesis{Salakhutdinov2015,
author = {Salakhutdinov, Ruslan},
doi = {10.1146/annurev-statistics-010814-020120},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salakhutdinov - 2008 - Learning Deep Generative Models.pdf:pdf},
issn = {2326-8298},
pages = {84},
school = {University of Toronto},
title = {{Learning Deep Generative Models}},
url = {https://www.cs.cmu.edu/{~}rsalakhu/papers/Russ{\_}thesis.pdf},
year = {2008}
}
@book{Unknown,
address = {Cambridge, Massachusetts},
author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
edition = {2nd},
editor = {Bach, Francis},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohri, Rostamizadeh, Talwalkar - 2018 - Foundations of Machine Learning.pdf:pdf},
isbn = {9780262039406},
pages = {505},
publisher = {The MIT Press},
title = {{Foundations of Machine Learning}},
year = {2018}
}
@techreport{Hinton,
author = {Hinton, Geoffrey and Srivastava, Ni@sh and Swersky, Kevin},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Srivastava, Swersky - Unknown - Neural Networks for Machine Learning Lecture 6a Overview of mini-­-batch gradient descent.pdf:pdf},
title = {{Neural Networks for Machine Learning Lecture 6a Overview of mini-­-batch gradient descent}}
}
@article{Kotsiantis2007,
abstract = {Supervised machine learning is the search for algorithms that reason from externally supplied instances to produce general hypotheses, which then make predictions about future instances. In other words, the goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single article cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.},
author = {Kotsiantis, S B},
doi = {10.31449/inf.v31i3.148},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kotsiantis - 2007 - Supervised machine learning A review of classification techniques.pdf:pdf},
issn = {03505596},
journal = {Informatica (Ljubljana)},
keywords = {Classifiers,Data mining techniques,Intelligent data analysis,Learning algorithms},
number = {3},
pages = {249--268},
title = {{Supervised machine learning: A review of classification techniques}},
volume = {31},
year = {2007}
}
@article{Singh,
author = {Singh, A and Thakur, N and International, A Sharma - 2016 3rd and undefined 2016},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh et al. - Unknown - A review of supervised machine learning algorithms.pdf:pdf},
journal = {ieeexplore.ieee.org},
title = {{A review of supervised machine learning algorithms}},
url = {https://ieeexplore.ieee.org/abstract/document/7724478/}
}
@inproceedings{Yogaswara2018,
author = {Yogaswara, Reza Dea and Wibawa, Adhi Dharma},
booktitle = {2018 International Conference on Computer Engineering, Network and Intelligent Multimedia, CENIM 2018 - Proceeding},
doi = {10.1109/CENIM.2018.8711387},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yogaswara, Wibawa - 2018 - Comparison of Supervised Learning Image Classification Algorithms for Food and Non-Food Objects.pdf:pdf},
isbn = {9781538675090},
keywords = {convolutional neural network,deep learning,image recognition,machine learning,object classification},
month = {jul},
pages = {317--324},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Comparison of Supervised Learning Image Classification Algorithms for Food and Non-Food Objects}},
year = {2018}
}
@techreport{Hutter,
author = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hutter, Kotthoff, Vanschoren - Unknown - The Springer Series on Challenges in Machine Learning Automated Machine Learning Methods, Syste.pdf:pdf},
title = {{The Springer Series on Challenges in Machine Learning Automated Machine Learning Methods, Systems, Challenges}},
url = {http://www.springer.com/series/15602}
}
@book{Russell2010,
author = {Russell, Stuart J and Norvig, Peter},
edition = {3rd},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Russell, Norvig - 2010 - Artificial Intelligence-A Modern Approach (3rd internat. edn.).pdf:pdf},
isbn = {978-0-13-604259-4},
pages = {1132},
publisher = {Pearson Education},
title = {{Artificial Intelligence-A Modern Approach (3rd internat. edn.)}},
year = {2010}
}
@techreport{Hsu,
abstract = {This paper presents a neural network-based end-to-end clustering framework. We design a novel strategy to utilize the contrastive criteria for pushing data-forming clusters directly from raw data, in addition to learning a feature embedding suitable for such clustering. The network is trained with weak labels, specifically partial pairwise relationships between data instances. The cluster assignments and their probabilities are then obtained at the output layer by feed-forwarding the data. The framework has the interesting characteristic that no cluster centers need to be explicitly specified, thus the resulting cluster distribution is purely data-driven and no distance metrics need to be predefined. The experiments show that the proposed approach beats the conventional two-stage method (feature embedding with k-means) by a significant margin. It also compares favorably to the performance of the standard cross entropy loss for classification. Robustness analysis also shows that the method is largely insensitive to the number of clusters. Specifically, we show that the number of dominant clusters is close to the true number of clusters even when a large k is used for clustering.},
archivePrefix = {arXiv},
arxivId = {1511.06321v5},
author = {Hsu, Yen-Chang and Kira, Zsolt},
eprint = {1511.06321v5},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu, Kira - Unknown - Workshop track-ICLR 2016 NEURAL NETWORK-BASED CLUSTERING USING PAIR-WISE CONSTRAINTS.pdf:pdf},
title = {{Workshop track-ICLR 2016 NEURAL NETWORK-BASED CLUSTERING USING PAIR-WISE CONSTRAINTS}}
}
@techreport{Gharamani2018,
author = {Gharamani, Zoubin},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gharamani - 2018 - Automatic Machine Learning Methods, Systems, Challenges.pdf:pdf},
pages = {250},
title = {{Automatic Machine Learning: Methods, Systems, Challenges}},
year = {2018}
}
@book{Geron2019,
author = {G{\'{e}}ron, Aur{\'{e}}lien},
edition = {2nd},
editor = {Tache, Nicole},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/G{\'{e}}ron - 2019 - Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow Concepts, Tools, and Techniques to Build Intelligent S.pdf:pdf},
isbn = {1492032611},
pages = {820},
publisher = {O'Reilly Media},
title = {{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems}},
year = {2019}
}
@book{Mitchell1997,
author = {Mitchell, Tom M},
booktitle = {McGraw-Hill},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitchell - 1997 - Machine Learning Machine Learning.pdf:pdf},
isbn = {0070428077},
number = {April},
pages = {421},
title = {{Machine Learning Machine Learning}},
year = {1997}
}
@article{Zhang2019,
abstract = {Porous carbons with different textural properties exhibit great differences in CO2 adsorption capacity. It is generally known that narrow micropores contribute to higher CO2 adsorption capacity. However, it is still unclear what role each variable in the textural properties plays in CO2 adsorption. Herein, a deep neural network is trained as a generative model to direct the relationship between CO2 adsorption of porous carbons and corresponding textural properties. The trained neural network is further employed as an implicit model to estimate its ability to predict the CO2 adsorption capacity of unknown porous carbons. Interestingly, the practical CO2 adsorption amounts are in good agreement with predicted values using surface area, micropore and mesopore volumes as the input values simultaneously. This unprecedented deep learning neural network (DNN) approach, a type of machine learning algorithm, exhibits great potential to predict gas adsorption and guide the development of next-generation carbons.},
author = {Zhang, Zihao and Schott, Jennifer A. and Liu, Miaomiao and Chen, Hao and Lu, Xiuyang and Sumpter, Bobby G. and Fu, Jie and Dai, Sheng},
doi = {10.1002/anie.201812363},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2019 - Prediction of Carbon Dioxide Adsorption via Deep Learning.pdf:pdf},
issn = {15213773},
journal = {Angewandte Chemie - International Edition},
keywords = {CO2 adsorption,machine learning,porous carbon,textural properties},
month = {jan},
number = {1},
pages = {259--263},
publisher = {Wiley-VCH Verlag},
title = {{Prediction of Carbon Dioxide Adsorption via Deep Learning}},
url = {http://doi.wiley.com/10.1002/anie.201812363},
volume = {58},
year = {2019}
}
@article{Liu2018,
abstract = {Machine learning, an important branch of artificial intelligence, is increasingly being applied in sciences such as forest ecology. Here, we review and discuss three commonly used methods of machine learning (ML) including decision-tree learning, artificial neural network, and support vector machine and their applications in four different aspects of forest ecology over the last decade. These applications include: (i) species distribution models, (ii) carbon cycles, (iii) hazard assessment and prediction, and (iv) other applications in forest management. Although ML approaches are useful for classification, modeling, and prediction in forest ecology research, further expansion of ML technologies is limited by the lack of suitable data and the relatively "higher threshold" of applications. However, the combined use of multiple algorithms and improved communication and cooperation between ecological researchers and ML developers still present major challenges and tasks for the betterment of future ecological research. We suggest that future applications of ML in ecology will become an increasingly attractive tool for ecologists in the face of "big data" and that ecologists will gain access to more types of data such as sound and video in the near future, possibly opening new avenues of research in forest ecology.},
author = {Liu, Zelin and Peng, Changhui and Work, Timothy and Candau, Jean Noel and Desrochers, Annie and Kneeshaw, Daniel},
doi = {10.1139/er-2018-0034},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2018 - Application of machine-learning methods in forest ecology Recent progress and future challenges.pdf:pdf},
issn = {11818700},
journal = {Environmental Reviews},
keywords = {Artificial neural network,Decision-trees learning,Forest management,Hazard assessment,Species classification,Support vector machine},
number = {4},
pages = {339--350},
title = {{Application of machine-learning methods in forest ecology: Recent progress and future challenges}},
url = {https://mc06.manuscriptcentral.com/er-pubs},
volume = {26},
year = {2018}
}
@article{Rehman2019,
abstract = {With being rapid increasing population in worldwide, the need for satisfactory level of crop production with decreased amount of agricultural lands. Machine vision would ensure the increase of crop production by using an automated, non-destructive and cost-effective technique. In last few years, remarkable results have been achieved in different sectors of agriculture. These achievements are integrated with machine learning techniques on machine vision approach that cope with colour, shape, texture and spectral analysis from the image of objects. Despite having many applications of different machine learning techniques, this review only described the statistical machine learning technologies with machine vision systems in agriculture due to broad area of machine learning applications. Two types of statistical machine learning techniques such as supervised and unsupervised learning have been utilized for agriculture. This paper comprehensively surveyed current application of statistical machine learning techniques in machine vision systems, analyses each technique potential for specific application and represents an overview of instructive examples in different agricultural areas. Suggestions of specific statistical machine learning technique for specific purpose and limitations of each technique are also given. Future trends of statistical machine learning technology applications are discussed.},
author = {Rehman, Tanzeel U. and Mahmud, Md Sultan and Chang, Young K. and Jin, Jian and Shin, Jaemyung},
doi = {10.1016/j.compag.2018.12.006},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rehman et al. - 2019 - Current and future applications of statistical machine learning algorithms for agricultural machine vision system.pdf:pdf},
issn = {01681699},
journal = {Computers and Electronics in Agriculture},
keywords = {Discriminant analysis,Fuzzy clustering,Gaussian mixture model,K-means clustering,Machine vision,Na{\"{i}}ve Bayes,Statistical machine learning,Support vector machines,k-Nearest Neighbour},
month = {jan},
pages = {585--605},
publisher = {Elsevier B.V.},
title = {{Current and future applications of statistical machine learning algorithms for agricultural machine vision systems}},
volume = {156},
year = {2019}
}
@article{Zhou2016,
abstract = {Current machine learning techniques have achieved great success; however, there are many deficiencies. First, to train a strong model, a large amount of training examples are required , whereas collecting the data, particularly data with labels , is expensive or even difficult in many real tasks. Second , once a model has been trained, if environment changes, which often happens in real tasks, the model can hardly perform well or even become useless. Third, the trained models are usually black-boxes, whereas people usually want to know what have been learned by the models, particularly in real tasks where decision reliability is crucial and rigorous judgments by human beings are critical. In addition to the above deficiencies, there are several relevant issues requiring attention. First, some data have to be shared in most current machine learning studies if one hopes to pass helpful information from one task to another. The data privacy or data proprietary, however, usually disables public data sharing. Thus, it is hard for people to build their learning tasks based on the results of other people. Second, machine learning is still a kind of magic: Even with sufficient training data, most end users, except machine learning experts, can hardly produce strong models. Considering the above issues, here we propose learnware. A learnware is a well-performed pre-trained machine learning model with a specification which explains the purpose and/or specialty of the model. The specification can be logic-based descriptions, and/or statistics that reveal the target to which the model aimed, and/or even a few simplified training samples that disclose the scenario for which the model was trained. The owner of a learnware can put it into a market, with little risk of data privacy leakage. As the comic illustrates , when a person is going to tackle a machine learning task, rather than build his model from scratch, he can do it in this way: Figure out his own requirement, and then browse/search the market, identify and take a good learnware whose specification matches his requirement. In some cases he can use the learnware directly, whereas in more cases he may need to use his own data to adapt/polish the learnware. Nevertheless, the whole process can be much less expensive and more efficient than building a model from scratch by himself. For this purpose, a learnware should have at least three important properties: Reusable, Evolvable, and Comprehen-sible.},
author = {Zhou, Zhi Hua},
doi = {10.1007/s11704-016-6906-3},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou - 2016 - Learnware on the future of machine learning.pdf:pdf},
issn = {20952236},
journal = {Frontiers of Computer Science},
number = {4},
pages = {589--590},
title = {{Learnware: on the future of machine learning}},
volume = {10},
year = {2016}
}
@book{Burkov2019,
author = {Burkov, Andriy},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burkov - 2019 - The Hundred-Page Machine Learning Book.pdf:pdf},
isbn = {978-1-9995795-0-0},
pages = {141},
publisher = {Andriy Burkov},
title = {{The Hundred-Page Machine Learning Book}},
url = {http://themlbook.com/wiki/doku.php},
year = {2019}
}
@inproceedings{Hu2018,
archivePrefix = {arXiv},
arxivId = {1709.01507},
author = {Hu, Jie and Shen, Li and Sun, Gang},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00745},
eprint = {1709.01507},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu, Shen, Sun - 2018 - Squeeze-and-Excitation Networks.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
keywords = {Attention,Convolutional Neural Networks !,Image representations,Index Terms-Squeeze-and-Excitation},
pages = {7132--7141},
title = {{Squeeze-and-Excitation Networks}},
url = {http://image-net.org/challenges/LSVRC/2017/results},
year = {2018}
}
@inproceedings{Simonyan2015a,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1409.1556},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - Very deep convolutional networks for large-scale image recognition(2).pdf:pdf},
keywords = {()},
title = {{Very deep convolutional networks for large-scale image recognition}},
url = {http://www.robots.ox.ac.uk/},
year = {2015}
}
@article{Silver2017,
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {Van Den Driessche}, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7676},
pages = {354--359},
pmid = {29052630},
title = {{Mastering the game of Go without human knowledge}},
volume = {550},
year = {2017}
}
@inproceedings{JiaDeng2009,
author = {{Jia Deng} and {Wei Dong} and Socher, Richard and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
doi = {10.1109/cvprw.2009.5206848},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jia Deng et al. - 2009 - ImageNet A large-scale hierarchical image database.pdf:pdf},
pages = {248--255},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://www.image-net.org.},
year = {2009}
}
@article{Michalski1983,
author = {Michalski, R. S. and Carbonell, J. G. and Mitchell, T. M.},
doi = {10.1016/0022-2496(83)90037-8},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Michalski, Carbonell, Mitchell - 1983 - Machine learning An artificial intelligence approach.pdf:pdf},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
month = {dec},
number = {4},
pages = {456--460},
publisher = {Elsevier BV},
title = {{Machine learning: An artificial intelligence approach}},
volume = {27},
year = {1983}
}
@article{Langley2011,
author = {Langley, Pat},
doi = {10.1007/s10994-011-5242-y},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Langley - 2011 - The changing science of machine learning.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
month = {mar},
number = {3},
pages = {275--279},
title = {{The changing science of machine learning}},
volume = {82},
year = {2011}
}
@inproceedings{Vincent2008,
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite. Copyright 2008 by the author(s)/owner(s).},
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre Antoine},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
doi = {10.1145/1390156.1390294},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vincent et al. - 2008 - Extracting and composing robust features with denoising autoencoders.pdf:pdf},
isbn = {9781605582054},
pages = {1096--1103},
title = {{Extracting and composing robust features with denoising autoencoders}},
year = {2008}
}
@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G. E. and Salakhutdinov, R. R.},
doi = {10.1126/science.1127647},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Salakhutdinov - 2006 - Reducing the dimensionality of data with neural networks.pdf:pdf},
issn = {00368075},
journal = {Science},
month = {jul},
number = {5786},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the dimensionality of data with neural networks}},
volume = {313},
year = {2006}
}
@inproceedings{Petraglia2017,
author = {Petraglia, Felipe R. and Campos, Roberto and Gomes, Jose Gabriel R.C. and Petraglia, Mariane R.},
booktitle = {Proceedings - IEEE International Symposium on Circuits and Systems},
doi = {10.1109/ISCAS.2017.8050761},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Petraglia et al. - 2017 - Pipeline tracking and event classification for an automatic inspection vision system.pdf:pdf},
isbn = {9781467368520},
issn = {02714310},
keywords = {Underwater pipeline inspection,deep convolutional neural network,edge detection,machine learning,multilayer perceptron},
month = {sep},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Pipeline tracking and event classification for an automatic inspection vision system}},
year = {2017}
}
@inproceedings{Dahl2013,
abstract = {Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random 'dropout' procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid over-fitting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectified linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modified deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2{\%} relative improvement over a DNN trained with sigmoid units, and a 14.4{\%} relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code. {\textcopyright} 2013 IEEE.},
author = {Dahl, George E. and Sainath, Tara N. and Hinton, Geoffrey E.},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2013.6639346},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dahl, Sainath, Hinton - 2013 - Improving deep neural networks for LVCSR using rectified linear units and dropout.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
keywords = {Bayesian optimization,LVCSR,acoustic modeling,broadcast news,deep learning,dropout,neural networks,rectified linear units},
pages = {8609--8613},
title = {{Improving deep neural networks for LVCSR using rectified linear units and dropout}},
url = {moz-extension://b0fced5a-38c2-4863-a3dc-9aa2ef40ac31/enhanced-reader.html?openApp{\&}pdf=http{\%}3A{\%}2F{\%}2Fwww.cs.toronto.edu{\%}2F{~}fritz{\%}2Fabsps{\%}2Fmomentum.pdf moz-extension://b0fced5a-38c2-4863-a3dc-9aa2ef40ac31/enhanced-reader.html?openApp{\&}pdf=http{\%}3A{\%}2F{\%}2Fwww.cs},
year = {2013}
}
@techreport{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1409.1556},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - Very deep convolutional networks for large-scale image recognition.pdf:pdf},
keywords = {()},
title = {{Very deep convolutional networks for large-scale image recognition}},
url = {http://www.robots.ox.ac.uk/},
year = {2015}
}
@article{Marani2009,
abstract = {Many underwater intervention tasks are today performed using manned submersibles or remotely operated vehicles in teleoperation mode. Autonomous underwater vehicles are mostly employed in survey applications. In fact, the low bandwidth and significant time delay inherent in acoustic subsea communications represent a considerable obstacle to remotely operate a manipulation system, making it impossible for remote controllers to react to problems in a timely manner. Nevertheless, vehicles with no physical link and with no human occupants permit intervention in dangerous areas, such as in deep ocean, under ice, in missions to retrieve hazardous objects, or in classified areas. The key element in underwater intervention performed with autonomous vehicles is autonomous manipulation. This is a challenging technology milestone, which refers to the capability of a robot system that performs intervention tasks requiring physical contacts with unstructured environments without continuous human supervision. Today, only few AUVs are equipped with manipulators. SAUVIM (Semi Autonomous Underwater Vehicle for Intervention Mission, University of Hawaii) is one of the first underwater vehicle capable of autonomous manipulation. This paper presents the solutions chosen within the development of the system in order to address the problems intrinsic to autonomous underwater manipulation. In the proposed approach, the most noticeable aspect is the increase in the level of information transferred between the system and the human supervisor. We describe one of the first trials of autonomous intervention performed by SAUVIM in the oceanic environment. To the best knowledge of the authors, no sea trials in underwater autonomous manipulation have been presented in the literature. The presented operation is an underwater recovery mission, which consists in a sequence of autonomous tasks finalized to search for the target and to securely hook a cable to it in order to bring the target to the surface. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Marani, Giacomo and Choi, Song K. and Yuh, Junku},
doi = {10.1016/j.oceaneng.2008.08.007},
issn = {00298018},
journal = {Ocean Engineering},
keywords = {AUV,Autonomous manipulation,Localization,ROV,Teleprogramming,Tracking,Underwater intervention},
month = {jan},
number = {1},
pages = {15--23},
title = {{Underwater autonomous manipulation for intervention missions AUVs}},
volume = {36},
year = {2009}
}
@article{hinton2006,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
doi = {10.1162/neco.2006.18.7.1527},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Osindero, Teh - 2006 - A fast learning algorithm for deep belief nets.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
number = {7},
pages = {1527--1554},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets}},
volume = {18},
year = {2006}
}
@incollection{Turing1950,
abstract = {The new form of the problem can be described in terms of a game which we call the ' imitation game '. It is played with three people, a man (A), a woman (B), and an interrogator (C) who may be of either sex. The interrogator stays in a room apart from the other two. The object of the game for the interrogator is to determine which of the other two is the man and which is the woman. He knows them by labels X and Y, and at the end of the game he says either ' X is A and Y is B ' or ' X is B and Y is A '. The interrogator is allowed to put questions to A and B thus: C: Will X please tell me the length of his or her hair? Now suppose X is actually A, then A must answer. It is A's I.COMPUTING MACHINERY AND INTELLIGENCE object in the game to try and cause C to make the wrong identification. His answer might therefore be ' My hair is shingled, and the longest strands are about nine inches long.'},
author = {Turing, A. M.},
booktitle = {Machine Intelligence: Perspectives on the Computational Model},
doi = {10.1093/mind/lix.236.433},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turing - 2012 - Computing machinery and intelligence.pdf:pdf},
isbn = {9781136525049},
issn = {0026-4423},
month = {jan},
pages = {1--28},
publisher = {Taylor and Francis},
title = {{Computing machinery and intelligence}},
year = {1950}
}
@inproceedings{lecun1990,
author = {LeCun, Yann and Boser, Bernhard E and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne E and Jackel, Lawrence D},
booktitle = {Advances in neural information processing systems},
doi = {10.4324/9781351195553-1},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 1990 - Handwritten Digit Recognition with a Back-Propagation Network.pdf:pdf},
issn = {15244725},
pages = {396----404},
pmid = {23301817},
title = {{Handwritten Digit Recognition with a Back-Propagation Network}},
year = {1990}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 1998 - Gradient-based learning applied to document recognition.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@incollection{LeCun1995,
abstract = {This paper compares the performance of several classifies algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold.},
author = {LeCun, Y. and Jackel, L. D. and Bottou, L. and Cortes, C. and Denker, J. S. and Drucker, H. and Guyon, I. and Muller, U. a. and Sackinger, E. and Simard, P. and Vapnik, V.},
booktitle = {Neural networks: the statistical mechanics perspective},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 1995 - Learning algorithms for classification A comparison on handwritten digit recognition.pdf:pdf},
pages = {261--276},
publisher = {World Scientific},
title = {{Learning algorithms for classification: A comparison on handwritten digit recognition}},
year = {1995}
}
@article{Hinton1990,
abstract = {Three different ways of mapping part-whole hierarchies into connectionist networks are described. The simplest scheme uses a fixed mapping and is inadequate for most tasks because it fails to share units and connections between different pieces of the part-whole hierarchy. Two alternative schemes are described, each of which involves a different method of time-sharing connections and units. The scheme we finally arrive at suggests that neural networks have two quite different methods for performing inference. Simple "intuitive" inferences can be performed by a single settling of a network without changing the way in which the world is mapped into the network. More complex "rational" inferences involve a sequence of such settlings with mapping changes after each settling. {\textcopyright} 1990.},
author = {Hinton, Geoffrey E.},
doi = {10.1016/0004-3702(90)90004-J},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton - 1990 - Mapping part-whole hierarchies into connectionist networks.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1-2},
pages = {47--75},
title = {{Mapping part-whole hierarchies into connectionist networks}},
volume = {46},
year = {1990}
}
@article{Sejnowski1986,
abstract = {-},
author = {Sejnowski, TJ and Rosenberg, CR},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sejnowski, Rosenberg - 1986 - NETtalk A parallel network that learns to read aloud.pdf:pdf},
isbn = {0262010976},
journal = {The Johns Hopkins University Electrical Engineering and Computer Science Technical Report},
pages = {663--672},
title = {{NETtalk: A parallel network that learns to read aloud}},
url = {http://dl.acm.org/citation.cfm?id=104448},
volume = {01},
year = {1986}
}
@article{McCulloch1943,
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McCulloch, Pitts - 1943 - A logical calculus of the ideas immanent in nervous activity.pdf:pdf},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
keywords = {Cell Biology,Life Sciences,Mathematical and Computational Biology,general},
month = {dec},
number = {4},
pages = {115--133},
publisher = {Kluwer Academic Publishers},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@phdthesis{Socher2014,
abstract = {I introduced a max-margin,structure prediction framework based on Recursive Neural Networks (RNNs) for nding hierarchical structure in multiple modalities. Recursion in this case pertains to the idea that the same neural network is applied repeatedly on different components of a sentence. Since this model showed much promise for both language and image understanding, I decided to further investigate the space of recursive deep learning models. In this thesis, I explore model variations along three major axes in order to gain insights into hierarchical feature learning, building fast, practical, state of the art NLP systems and semantic compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them. I explore unsupervised learning of word and sentence vectors using reconstruction errors. My my parsing work uses a simple linear scoring function and sentiment and relation classi cation use softmax classi ers to predict a label for each node and phrase in the tree. The standard RNN composition function is based on a single neural network layer that takes as input two phrase or word vectors and uses the same set of weights at every node in the parse tree to compute higher order phrase vectors. It is not expressive enough to capture all types of compositions. I explored several variants of composition functions. I have worked on constituency parsing, whose goal it is to learn the correct grammatical analysis of a sentence and produce a tree structure. Another approach allowed the actual task, such as sentiment prediction or reconstruction error, to determine the tree structure. CHANGES vs. 2011a [Dynamic pooling and unfolding recursive autoencoders for paraphrase detection], mentioned by Zhou, Troyanskaya - Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction [2014] 1. For optimization of objective function, the diagonal variant of AdaGrad has much better optima in various experiments of this thesis, and converges more quickly than L-BFGS used in 2011a 2. The standard RNN composition function [used in 2011a] it is not expressive enough to capture all types of compositions. Hence, I explored several variants of composition functions.},
author = {Socher, Richard},
booktitle = {PhD thesis},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher - 2014 - Recursive Deep Learning for Natural Language Processing and Computer Vision.pdf:pdf},
keywords = {Richard Socher},
number = {August},
pages = {189},
school = {Stanford University},
title = {{Recursive Deep Learning for Natural Language Processing and Computer Vision}},
year = {2014}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rumelhart, Hinton, Williams - 1986 - Learning representations by back-propagating errors.pdf:pdf},
issn = {00280836},
journal = {Nature},
keywords = {Humanities and Social Sciences,Science,multidisciplinary},
number = {6088},
pages = {533--536},
publisher = {Nature Publishing Group},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@inproceedings{Aziz2018,
abstract = {Machine learning is a field within artificial intelligence that allows machines to learn on their own from existing information to make predictions or/and decisions. There are three main categories of machine learning techniques: Collaborative filtering (for making recommendations), Clustering (for discovering structure in collections of data) and Classification (form of supervised learning). Machine learning helps users to make better decisions, Machine learning algorithms create patterns based on previous information and use them to design predictive models, then, use this models to obtain predictions about future data. A huge amount of data from several sources need methods and techniques to be processed correctly, in order to exploit this data efficiently, machine learning is a great technology for exploiting the needs in big data analysis. This paper describes the implementation of Apache Spark MLlib and Apache Mahout in order to process Big Data using Machine Learning algorithms. Furthermore, we conduct experimental simulations to show the difference between this two Machine Learning frameworks. Subsequently, we discuss the most striking observations that emerge from the comparison of these technologies through several experimental studies.},
address = {New York, New York, USA},
author = {Aziz, Khadija and Zaidouni, Dounia and Bellafkih, Mostafa},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3289402.3289525},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aziz, Zaidouni, Bellafkih - 2018 - Big Data Processing using Machine Learning algorithms MLlib and mahout use case.pdf:pdf},
isbn = {9781450364621},
keywords = {Big Data,Classification,Clustering,Collaborative Filtering,Hadoop,MLlib,Machine Learning,Mahout,Spark},
month = {oct},
pages = {1--6},
publisher = {Association for Computing Machinery},
title = {{Big Data Processing using Machine Learning algorithms: MLlib and mahout use case}},
url = {http://dl.acm.org/citation.cfm?doid=3289402.3289525},
year = {2018}
}
@inproceedings{Srivastava2019,
abstract = {Machine learning uses artificial intelligence and helps systems to develop the ability to do self learning and improve from past experience without much of programming. It can access data and use it for themselves. In this research paper, we will be discussing the different machine learning techniques and how they are used in different day to day applications.},
author = {Srivastava, Aditya and Saini, Sonia and Gupta, Deepa},
booktitle = {Proceedings of the 3rd International Conference on Electronics and Communication and Aerospace Technology, ICECA 2019},
doi = {10.1109/ICECA.2019.8822068},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava, Saini, Gupta - 2019 - Comparison of Various Machine Learning Techniques and Its Uses in Different Fields.pdf:pdf},
isbn = {9781728101675},
keywords = {Data Mining,Machine Learning Techniques,Sports Data Analysis,Supervised learning,Unsupervised learning},
month = {jun},
pages = {81--86},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Comparison of Various Machine Learning Techniques and Its Uses in Different Fields}},
year = {2019}
}
@article{Atyabi2018,
abstract = {Advances in hardware technology have enabled more integration of sophisticated software, triggering progresses in development and employment of Unmanned Vehicles (UVs), and mitigating restraints for onboard intelligence. As a result, UVs can now take part in more complex mission where continuous transformation in environmental condition calls for higher level of situational responsiveness. This paper serves as an introduction to UVs mission planning and management systems aiming to highlight some of the recent developments in the field of autonomous underwater and aerial vehicles in addition to stressing some possible future directions and discussing the learned lessons. A comprehensive survey over autonomy assessment of UVs, and different aspects of autonomy such as situation awareness, cognition, and decision-making has been provided in this study. The paper separately explains the humanoid and autonomous system's performance and highlights the role and impact of a human in UVs operations.},
author = {Atyabi, Adham and MahmoudZadeh, Somaiyeh and Nefti-Meziani, Samia},
doi = {10.1016/j.arcontrol.2018.07.002},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Atyabi, MahmoudZadeh, Nefti-Meziani - 2018 - Current advancements on autonomous mission planning and management systems An AUV and UAV p.pdf:pdf},
issn = {13675788},
journal = {Annual Reviews in Control},
keywords = {Autonomy,Cognition,Contingency management,Mission management,Situational awareness},
month = {jan},
pages = {196--215},
publisher = {Elsevier Ltd},
title = {{Current advancements on autonomous mission planning and management systems: An AUV and UAV perspective}},
volume = {46},
year = {2018}
}
@article{Kim2008,
abstract = {Abstract Image processing has been one of hot issues for real world robot applications such as navigation and visual servoing. In case of underwater robot application, however, conventional optical camera-based images have many limitations for real application due to visibility in turbid water, image saturation under underwater light in the deep water, and short visible range in the water. Thus, most of underwater image applications use high frequency sonar to get precise acoustic image. There have been some approaches to apply optical image processing methods to acoustic image, but performance is still not good enough for automatic classification/recognition. In this paper, a neural network-based image processing algorithm is proposed for acoustic image classification. Especially, shadow of an acoustic object is mainly used as a cue of the classification. The neural network classifies a pre-taught image from noisy and/or occlude object images. In order to get fast learning and retrieving, a Bidirectional Associative Memory (BAM) is used. It is remarked that the BAM doesn't need many learning trials, but just simple multiplication of two vectors for generating a correlation matrix. However, because of the simple calculation, it is not guaranteed to learn and recall all data set. Thus, it is needed to modify the BAM for improving its performance. In this paper, complement data set and weighted learning factor are used to improve the BAM performance. The test results show that the proposed method successfully classified 4 pre-taught object images from various underwater object images with up to 50{\%} of B/W noise.},
author = {Kim, Tae Won and Yu, Son-Cheol and Yuh, Junku},
doi = {10.3182/20080706-5-kr-1001.02703},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Yu, Yuh - 2008 - Neural network-based underwater image classification for Autonomous Underwater Vehicles.pdf:pdf},
issn = {14746670},
journal = {IFAC Proceedings Volumes},
number = {2},
pages = {15991--15995},
publisher = {Elsevier BV},
title = {{Neural network-based underwater image classification for Autonomous Underwater Vehicles}},
volume = {41},
year = {2008}
}
@article{Islam2019,
abstract = {This letter explores the design and development of a class of robust diver detection algorithms for autonomous diver-following applications. By considering the operational challenges for underwater visual tracking in diverse real-world settings, we formulate a set of desired features of a generic diver-following algorithm. We attempt to accommodate these features and maximize general tracking performance by exploiting the state-of-the-art deep object detection models. We fine tune the building blocks of these models with a goal of balancing the tradeoff between robustness and efficiency in an on-board setting under real-time constraints. Subsequently, we design an architecturally simple convolutional neural network based diver detection model that is much faster than the state-of-the-art deep models yet provides comparable detection performances. In addition, we validate the performance and effectiveness of the proposed model through a number of diver-following experiments in closed-water and open-water environments.},
archivePrefix = {arXiv},
arxivId = {1809.06849},
author = {Islam, Md Jahidul and Fulton, Michael and Sattar, Junaed},
doi = {10.1109/LRA.2018.2882856},
eprint = {1809.06849},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Islam, Fulton, Sattar - 2019 - Toward a Generic Diver-Following Algorithm Balancing Robustness and Efficiency in Deep Visual Detection.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Human detection and tracking,field robots,marine robotics},
month = {jan},
number = {1},
pages = {113--120},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Toward a Generic Diver-Following Algorithm: Balancing Robustness and Efficiency in Deep Visual Detection}},
volume = {4},
year = {2019}
}
@inproceedings{Kondo2002,
abstract = {To observe underwater structures such as artificial piles in harbors by taking visual images, the Autonomous Underwater Vehicles (AUVs) have advantages that they have no problem related to the umbilical cable which may get entangled with the target object and surrounding obstacles. It is usually difficult to measure precise configuration of such objects by using acoustic measurement systems because of poor resolution and diffused reflection of sound. This paper proposes sensing and navigation methods to observe such objects based on images by a CCD camera with laser pointers, on the assumption that the visibility is enough for taking their visual images. The navigation system cannot provide accurate enough position. Thus, AUVs change over their navigation basis from navigation system to relative position to the target so as to trace the shape of that. The proposed method was implemented in the testbed AUV Tri-Dog 1 and proved experimentally through tank tests. The vehicle accomplished the mission that autonomously detects 3 piles of 1 meter in diameter that are modeled on actual piles in a harbor, and turns around them. Based on the proposed system, AUVs will be able to carry out survey of the target object without any help from outside.},
author = {Kondo, Hayato and Ura, Tamaki},
booktitle = {Underwater Technology},
doi = {10.1109/ut.2002.1002423},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kondo, Ura - 2002 - Underwater structure observation by the AUV with laser pointing device.pdf:pdf},
isbn = {0780373979},
issn = {17560551},
pages = {178--183},
publisher = {Society for Underwater Technology},
title = {{Underwater structure observation by the AUV with laser pointing device}},
volume = {2002-Janua},
year = {2002}
}
@article{Zhang,
abstract = {There are many automation systems that are required to work under poor visual conditions, such as auto-navigation in foggy or underwater environment. The low-visibility poses challenges for traditional feature modeling methods, which commonly contribute as a key component to such autonomous systems. It can thus negatively impact the performance of those computing systems. For example, the matching precision (matching quality) and the successfully identified matches (matching quantity) can both drop dramatically in low-visibility. On the other hand, human vision system can robustly identify visual features correctly despite the variations in lighting conditions. Inspired by human knowledge of perceiving visual features, this paper presents a novel feature modeling solution under poor visual conditions. Based on a color constancy enhanced illumination alignment, a new concept called Superpixel Flow (SPF) is proposed to model the visual features in images. SPF is generated considering the content motions across frame pairs, which make it easier to track across frames compared with classic Superpixels. The matching is achieved by a cycle-labeling strategy using Markov Random Field (MRF) with energy functions composed according to human knowledge of compare visual features. An outlier removal follows to further improve the matching accuracy. Competitive performance is demonstrated in the experiments compared with state-of-the-art approaches.},
author = {Zhang, Shu and Yu, Hui and Wang, Ting and Dong, Junyu},
doi = {10.1016/j.knosys.2020.105699},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Feature modeling,Illumination variation,Low-visibility,Optical flow,Outlier removal,Superpixel},
title = {{Augmented visual feature modeling for matching in low-visibility based on cycle-labeling of Superpixel Flow}},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120301301},
year = {2020}
}
@inproceedings{Ohata2005,
abstract = {Various kinds of robots have been developed parallel with progress of computer and the operations with robots in the extreme environment such as rescue, space and ocean are getting practical solutions. The underwater robots are expected as one of solutions for underwater activities i.e., maintenance of underwater structures, observations, scientific research, and investigated the efficiency during recent decades. Especially, the underwater structures are getting large-scale and large-depth. In order to do safe and efficient works, the works are desired to be carried out automatically. In these works, underwater images are one of the effective information, and also useful for various applications, including undersea exploration navigation, wreckage visualization. This paper describes development of autonomous underwater vehicle for observation of underwater environment.},
author = {Ohata, Satomi and Ishii, Kazuo and Sakai, Hiroshi and Tanaka, Toshinari and Ura, Tamaki},
booktitle = {Proceedings of MTS/IEEE OCEANS, 2005},
doi = {10.1109/OCEANS.2005.1640042},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ohata et al. - 2005 - Development of an autonomous underwater vehicle for observation of underwater structures.pdf:pdf},
isbn = {0933957343},
title = {{Development of an autonomous underwater vehicle for observation of underwater structures}},
volume = {2005},
year = {2005}
}
@article{Chen,
abstract = {Due to the major obstacles originating from the strong light absorption and scattering in a dynamic underwater environment, underwater optical information acquisition and processing suffer from effects such as limited range, non-uniform lighting, low contrast, and diminished colors, causing it to become the bottleneck for marine scientific research and projects. After studying and generalizing the underwater biological visual mechanism, we explore its advantages in light adaption which helps animals to precisely sense the underwater scene and recognize their prey or enemies. Then, aiming to transform the significant advantage of the visual adaptation mechanism into underwater computer vision tasks, a novel knowledge-based information weighting fusion model is established for underwater object extraction. With this bionic model, the dynamical adaptability is given to the underwater object extraction task, making them more robust to the variability of the optical properties in different environments. The capability of the proposed method to adapt to the underwater optical environments is shown, and its outperformance for the object extraction is demonstrated by comparison experiments. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Chen, Zhe and Wang, Huibin and Xu, Lizhong and Shen, Jie},
doi = {10.1016/j.optlastec.2013.07.003},
issn = {00303992},
journal = {Optics and Laser Technology},
keywords = {Image preprocessing,Underwater object extraction,Visual adaptation mechanism},
pages = {119--130},
title = {{Visual-adaptation-mechanism based underwater object extraction}},
url = {https://www.sciencedirect.com/science/article/pii/S0030399213002569},
volume = {56},
year = {2014}
}
@article{Odena2016,
abstract = {In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128 x 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 x 128 samples are more than twice as discriminable as artificially resized 32 x 32 samples. In addition, 84.7{\%} of the classes have samples exhibiting diversity comparable to real Image Net data.},
archivePrefix = {arXiv},
arxivId = {1610.09585},
author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
eprint = {1610.09585},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Odena, Olah, Shlens - 2017 - Conditional image synthesis with auxiliary classifier gans.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
month = {oct},
pages = {4043--4055},
title = {{Conditional image synthesis with auxiliary classifier gans}},
url = {http://arxiv.org/abs/1610.09585},
volume = {6},
year = {2017}
}
@article{Rizzini2015,
abstract = {In this paper, we investigate the potential of vision-based object detection algorithms in underwater environments using several datasets to highlight the issues arising in different scenarios. Underwater computer vision has to cope with distortion and attenuation due to light propagation in water, and with challenging operating conditions. Scene segmentation and shape recognition in a single image must be carefully designed to achieve robust object detection and to facilitate object pose estimation. We describe a novel multi-feature object detection algorithm conceived to find human-made artefacts lying on the seabed. The proposed method searches for a target object according to a few general criteria that are robust to the underwater context, such as salient colour uniformity and sharp contours. We assess the performance of the proposed algorithm across different underwater datasets. The datasets have been obtained using stereo cameras of different quality, and diverge for the target object type and colour, acquisition depth and conditions. The effectiveness of the proposed approach has been experimentally demonstrated. Finally, object detection is discussed in connection with the simple colour-based segmentation and with the difficulty of tri-dimensional processing on noisy data.},
author = {Rizzini, Dario Lodi and Kallasi, Fabjan and Oleari, Fabio and Caselli, Stefano},
doi = {10.5772/60526},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rizzini et al. - 2015 - Investigation of vision-based underwater object detection with multiple datasets(2).pdf:pdf},
issn = {17298814},
journal = {International Journal of Advanced Robotic Systems},
keywords = {Image segmentation,Object detection,Underwater computer vision},
month = {jun},
number = {6},
pages = {77},
publisher = {InTech Europe},
title = {{Investigation of vision-based underwater object detection with multiple datasets}},
url = {http://journals.sagepub.com/doi/10.5772/60526},
volume = {12},
year = {2015}
}
@article{Razavian2014,
abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1403.6382},
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
doi = {10.1109/CVPRW.2014.131},
eprint = {1403.6382},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Razavian et al. - 2014 - CNN features off-the-shelf An astounding baseline for recognition.pdf:pdf},
isbn = {9781479943098},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
month = {mar},
pages = {512--519},
title = {{CNN features off-the-shelf: An astounding baseline for recognition}},
url = {http://arxiv.org/abs/1403.6382},
year = {2014}
}
@article{Goodfellow2016a,
abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
archivePrefix = {arXiv},
arxivId = {1701.00160},
author = {Goodfellow, Ian},
eprint = {1701.00160},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Networks.pdf:pdf},
month = {dec},
title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.00160},
year = {2016}
}
@inproceedings{Aldegheri2019,
abstract = {Enabling computer vision applications on low-power embedded systems gives rise to new challenges for embedded SW developers. Such applications implement different functionalities, like image recognition based on deep learning, simultaneous localization and mapping tasks. They are characterized by stringent performance constraints to guarantee real-time behaviors and, at the same time, energy constraints to save battery on the mobile platform. Even though heterogeneous embedded boards are getting pervasive for their high computational power at low power costs, they need a time consuming customization of the whole application (i.e., mapping of application blocks to CPU-GPU processing elements and their synchronization) to efficiently exploit their potentiality. Different languages and environments have been proposed for such an embedded SW customization. Nevertheless, they often find limitations on complex real cases, as their application is mutual exclusive. This paper presents a comprehensive framework that relies on a heterogeneous parallel programming model, which combines OpenMP, PThreads, OpenVX, OpenCV, and CUDA to best exploit different levels of parallelism while guaranteeing a semi-automatic customization. The paper shows how such languages and API platforms have been interfaced, synchronized, and applied to customize an ORB-SLAM application for an NVIDIA Jetson TX2 board.},
author = {Aldegheri, Stefano and Manzato, Silvia and Bombieri, Nicola},
booktitle = {IEEE/IFIP International Conference on VLSI and System-on-Chip, VLSI-SoC},
doi = {10.1109/VLSI-SoC.2018.8644937},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aldegheri, Manzato, Bombieri - 2019 - Enhancing Performance of Computer Vision Applications on Low-Power Embedded Systems Through Hetero.pdf:pdf},
isbn = {9781538647561},
issn = {23248440},
month = {feb},
pages = {119--124},
publisher = {IEEE Computer Society},
title = {{Enhancing Performance of Computer Vision Applications on Low-Power Embedded Systems Through Heterogeneous Parallel Programming}},
volume = {2018-Octob},
year = {2019}
}
@article{Luan2017,
abstract = {In steerable filters, a filter of arbitrary orientation can be generated by a linear combination of a set of 'basis filters.' Steerable properties dominate the design of the traditional filters, e.g., Gabor filters and endow features the capability of handling spatial transformations. However, such properties have not yet been well explored in the deep convolutional neural networks (DCNNs). In this paper, we develop a new deep model, namely, Gabor convolutional networks (GCNs or Gabor CNNs), with Gabor filters incorporated into DCNNs such that the robustness of learned features against the orientation and scale changes can be reinforced. By manipulating the basic element of DCNNs, i.e., the convolution operator, based on Gabor filters, GCNs can be easily implemented and are readily compatible with any popular deep learning architecture. We carry out extensive experiments to demonstrate the promising performance of our GCNs framework, and the results show its superiority in recognizing objects, especially when the scale and rotation changes take place frequently. Moreover, the proposed GCNs have much fewer network parameters to be learned and can effectively reduce the training complexity of the network, leading to a more compact deep learning model while still maintaining a high feature representation capacity. The source code can be found at https://github.com/bczhangbczhang.},
archivePrefix = {arXiv},
arxivId = {1705.01450},
author = {Luan, Shangzhen and Chen, Chen and Zhang, Baochang and Han, Jungong and Liu, Jianzhuang},
doi = {10.1109/TIP.2018.2835143},
eprint = {1705.01450},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luan et al. - 2018 - Gabor Convolutional Networks.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Gabor CNNs,Gabor filters,convolutional neural networks,kernel modulation,orientation},
month = {may},
number = {9},
pages = {4357--4366},
title = {{Gabor Convolutional Networks}},
url = {http://arxiv.org/abs/1705.01450 http://dx.doi.org/10.1109/TIP.2018.2835143},
volume = {27},
year = {2018}
}
@inproceedings{Goswami2018,
abstract = {With deep learning techniques, a revolution has taken place in the field of image processing and computer vision. The survey paper emphasizes the importance of representation learning methods for machine learning tasks. Deep learning, the modern machine learning is commonly used in the vision tasks—semantic segmentation, image captioning, object detection, recognition, and image classification. The paper focuses on the recent developments in the domain of remote sensing, retinal image understanding, and scene understanding based on newly proposed deep architectures. The author finds it quite intriguing of the classical building blocks of image segmentation (Gabor, K-means), shifting gear, and contributing to image recognition tasks based on deep learning (Gabor convolutional network, K-means dictionary learning). The survey makes an attempt to serve as a concise guide in providing latest works in computer vision applications based on deep learning and giving futuristic insights.},
author = {Goswami, Tilottama},
booktitle = {Lecture Notes in Electrical Engineering},
doi = {10.1007/978-981-10-7329-8_48},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goswami - 2018 - Impact of Deep Learning in Image Processing and Computer Vision.pdf:pdf},
isbn = {9789811073281},
issn = {18761119},
keywords = {Computer vision,Deep learning,Image processing,Representation learning},
pages = {475--485},
publisher = {Springer Verlag},
title = {{Impact of Deep Learning in Image Processing and Computer Vision}},
volume = {471},
year = {2018}
}
@inproceedings{MilzStefanandRudigerTobiasandSuss2018,
author = {{Milz, Stefan and Rudiger, Tobias and Suss}, Sebastian},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Milz, Stefan and Rudiger, Tobias and Suss - 2018 - Aerial GANeration Towards Realistic Data Augmentation Using Conditional GANs.pdf:pdf},
title = {{Aerial GANeration: Towards Realistic Data Augmentation Using Conditional GANs}},
url = {moz-extension://b5fcaa6a-20b9-49d8-b930-cce391cfdd60/enhanced-reader.html?openApp{\&}pdf=http{\%}3A{\%}2F{\%}2Fopenaccess.thecvf.com{\%}2Fcontent{\_}ECCVW{\_}2018{\%}2Fpapers{\%}2F11130{\%}2FMilz{\_}Aerial{\_}GANeration{\_}Towards{\_}Realistic{\_}Data{\_}Augmentation{\_}Using{\_}Conditional{\_}GANs{\_}ECCVW{\_}2018{\_}p},
year = {2018}
}
@inproceedings{VanBeeck2019,
abstract = {In this paper we present an overview of the contributed work presented at the UAVision2018 ECCV workshop. This workshop focused on real-time image processing on-board of Unmanned Aerial Vehicles (UAVs). For such applications the computational complexity of state-of-the-art computer vision algorithms often conflicts with the need for real-time operation and the extreme resource limitations of the hardware. Apart from a summary of the accepted workshop papers, this work also aims to identify common challenges and concerns which were addressed by multiple authors during the workshop, and their proposed solutions.},
author = {{Van Beeck}, Kristof and Tuytelaars, Tinne and Scarramuza, Davide and Goedem{\'{e}}, Toon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-11012-3_1},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Beeck et al. - 2019 - Real-time embedded computer vision on UAVs UAVision2018 workshop summary.pdf:pdf},
isbn = {9783030110116},
issn = {16113349},
keywords = {Computer vision,Deep learning,Embedded hardware,GPUs,Hardware optimizations,Real-time,UAVs},
month = {sep},
pages = {3--10},
publisher = {Springer Verlag},
title = {{Real-time embedded computer vision on UAVs: UAVision2018 workshop summary}},
volume = {11130 LNCS},
year = {2019}
}
@techreport{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to {\textless}sup{\textgreater}1{\textless}/sup{\textgreater}/{\textless}inf{\textgreater}2{\textless}/inf{\textgreater} everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems},
doi = {10.3156/jsoft.29.5_177_2},
eprint = {1406.2661},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative adversarial nets.pdf:pdf},
issn = {10495258},
number = {January},
pages = {2672--2680},
title = {{Generative adversarial nets}},
url = {http://www.github.com/goodfeli/adversarial},
volume = {3},
year = {2014}
}
@inproceedings{Redmon2016,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2016 - You only look once Unified, real-time object detection(2).pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
month = {dec},
pages = {779--788},
publisher = {IEEE Computer Society},
title = {{You only look once: Unified, real-time object detection}},
volume = {2016-Decem},
year = {2016}
}
@inproceedings{Girdhar2011,
abstract = {We present MARE, an autonomous airboat robot that is suitable for exploration-oriented tasks, such as inspection of coral reefs and shallow seabeds. The combination of this platform's particular mechanical properties and its powerful software framework enables it to function in a multitude of potential capacities, including autonomous surveillance, mapping, and search operations. In this paper we describe two different exploration strategies and their implementation using the MARE platform. First, we discuss the application of an efficient coverage algorithm, for the purpose of achieving systematic exploration of a known and bounded environment. Second, we present an exploration strategy driven by surprise, which steers the robot on a path that might lead to potentially surprising observations.},
author = {Girdhar, Yogesh and Xu, Anqi and Dey, Bir Bikram and Meghjani, Malika and Shkurti, Florian and Rekleitis, Ioannis and Dudek, Gregory},
doi = {10.1109/iros.2011.6094914},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Girdhar et al. - 2011 - MARE Marine Autonomous Robotic Explorer.pdf:pdf},
month = {dec},
pages = {5048--5053},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{MARE: Marine Autonomous Robotic Explorer}},
year = {2011}
}
@inproceedings{Fabbri2018,
abstract = {Autonomous underwater vehicles (AUVs) rely on a variety of sensors - acoustic, inertial and visual - for intelligent decision making. Due to its non-intrusive, passive nature and high information content, vision is an attractive sensing modality, particularly at shallower depths. However, factors such as light refraction and absorption, suspended particles in the water, and color distortion affect the quality of visual data, resulting in noisy and distorted images. AUVs that rely on visual sensing thus face difficult challenges and consequently exhibit poor performance on vision-driven tasks. This paper proposes a method to improve the quality of visual underwater scenes using Generative Adversarial Networks (GANs), with the goal of improving input to vision-driven behaviors further down the autonomy pipeline. Furthermore, we show how recently proposed methods are able to generate a dataset for the purpose of such underwater image restoration. For any visually-guided underwater robots, this improvement can result in increased safety and reliability through robust visual perception. To that effect, we present quantitative and qualitative data which demonstrates that images corrected through the proposed approach generate more visually appealing images, and also provide increased accuracy for a diver tracking algorithm.},
archivePrefix = {arXiv},
arxivId = {1801.04011},
author = {Fabbri, Cameron and Islam, Md Jahidul and Sattar, Junaed},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2018.8460552},
eprint = {1801.04011},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fabbri, Islam, Sattar - 2018 - Enhancing Underwater Imagery Using Generative Adversarial Networks.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
month = {sep},
pages = {7159--7165},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Enhancing Underwater Imagery Using Generative Adversarial Networks}},
year = {2018}
}
@article{Long2017,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30{\%} relative improvement to 67.2{\%} mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shelhamer, Long, Darrell - 2017 - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
month = {nov},
number = {4},
pages = {640--651},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {http://arxiv.org/abs/1411.4038},
volume = {39},
year = {2017}
}
@inproceedings{Fulton2019,
abstract = {Trash deposits in aquatic environments have a destructive effect on marine ecosystems and pose a long-term economic and environmental threat. Autonomous underwater vehicles (AUVs) could very well contribute to the solution of this problem by finding and eventually removing trash. This paper evaluates a number of deep-learning algorithms performing the task of visually detecting trash in realistic underwater environments, with the eventual goal of exploration, mapping, and extraction of such debris by using AUVs. A large and publicly-available dataset of actual debris in open-water locations is annotated for training a number of convolutional neural network architectures for object detection. The trained networks are then evaluated on a set of images from other portions of that dataset, providing insight into approaches for developing the detection capabilities of an AUV for underwater trash removal. In addition, the evaluation is performed on three different platforms of varying processing power, which serves to assess these algorithms' fitness for real-time applications.},
archivePrefix = {arXiv},
arxivId = {1804.01079},
author = {Fulton, Michael and Hong, Jungseok and Islam, Md Jahidul and Sattar, Junaed},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2019.8793975},
eprint = {1804.01079},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fulton et al. - 2019 - Robotic detection of marine litter using deep visual detection models.pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
month = {may},
pages = {5752--5758},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Robotic detection of marine litter using deep visual detection models}},
volume = {2019-May},
year = {2019}
}
@inproceedings{Ray2011,
abstract = {In the last few years, the field of mobile robotics has made lots of advancements. These advancements are due to the extensive application of mobile robots for autonomous exploration. Mobile robots are being popularly used for applications in space, underwater explorations, underground coal mines monitoring, inspection in chemical/toxic/ nuclear factories etc. But if these environments are unknown/unpredictable, conventional/ classical robotics may not serve the purpose. In such cases robot learning is the best option. Learning from the past experiences, is one such way for real time application of robots for completely unknown environments. Reinforcement learning is one of the best learning methods for robots using a constant system-environment interaction. Both single and multi-agent concepts are available for implementation of learning. The current research work describes a multi-agent based reinforcement learning using the concept of behaviour-based robotics for autonomous exploration of mobile robots. The concept has also been tested both in indoor and outdoor environments using real-time robots. {\textcopyright} 2011 IEEE.},
author = {Ray, Dip N. and Mandal, Amit and Majumder, Somajyoti and Mukhopadhyay, Sumit},
booktitle = {2011 IEEE International Conference on Robotics and Biomimetics, ROBIO 2011},
doi = {10.1109/ROBIO.2011.6181717},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ray et al. - 2011 - Human-like gradual multi-agent Q-learning using the concept of behavior-based robotics for autonomous exploration.pdf:pdf},
isbn = {9781457721373},
pages = {2725--2732},
title = {{Human-like gradual multi-agent Q-learning using the concept of behavior-based robotics for autonomous exploration}},
year = {2011}
}
@inproceedings{Torres-Mendez2005,
abstract = {In this paper, we consider the problem of color restoration using statistical priors. This is applied to color recovery for underwater images, using an energy minimization formulation. Underwater images present a challenge when trying to correct the blue-green monochrome look to bring out the color we know marine life has. For aquatic robot tasks, the quality of the images is crucial and needed in real-time. Our method enhances the color of the images by using a Markov Random Field (MRF) to represent the relationship between color depleted and color images. The parameters of the MRF model are learned from the training data and then the most probable color assignment for each pixel in the given color depleted image is inferred by using belief propagation (BP). This allows the system to adapt the color restoration algorithm to the current environmental conditions and also to the task requirements. Experimental results on a variety of underwater scenes demonstrate the feasibility of our method. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
author = {Torres-M{\'{e}}ndez, Luz A. and Dudek, Gregory},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11585978_5},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Torres-M{\'{e}}ndez, Dudek - 2005 - Color correction of underwater images for aquatic robot inspection.pdf:pdf},
isbn = {3540302875},
issn = {03029743},
pages = {60--73},
title = {{Color correction of underwater images for aquatic robot inspection}},
volume = {3757 LNCS},
year = {2005}
}
@inproceedings{Patel2016,
abstract = {This paper presents initial work on the development of an Ultrasonic Testing (UT) system to inspect defects in the inner and outer surfaces of underwater guy wires. The methodology involves placing ultrasonic transducers adjacent to simulated sections of guy wire and the analysis of the resultant echo profiles. The goal of this work is to determine if it is feasible to use ultrasonic testing to develop a machine learning based detection system. From the experimental results, we determined that the amplitude and time of arrival (TOA) of the first received wave signals gives a very significant clue to distinguishing the existence of a defect and its severity. Between the amplitude and time of arrival (TOA), the time of arrival (TOA) give a more promising indication since it was affected by the experimental factors in a consistent manner. Therefore, based on these results, we are encouraged to continue to develop ultrasonic techniques to assess underwater guy wires. In this lab scale study, ultrasound transceivers were placed around a physical model of a guy wire and used to develop and record acoustic signals. A MatlabTM based program was developed to do signal processing to extract the position of echo sources from the signal record. Guy wire condition is an important factor in the stability and safety of floating oil and gas platforms. Our final goal in this work is to develop an automatic system to inspect oil and gas floating platform guy wires to improve safety and avoid ecological and financial disasters such as has been witnessed in recent days.},
author = {Patel, Bhoomibahen and Paranjape, Raman and Mehrandezh, Mehran and Zhang, Lei},
booktitle = {Canadian Conference on Electrical and Computer Engineering},
doi = {10.1109/CCECE.2016.7726688},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Patel et al. - 2016 - Ultrasonic inspection of underwater guy wires with applications to floating oil platforms.pdf:pdf},
isbn = {9781467387217},
issn = {08407789},
keywords = {Acoustic Detection Technique,Guy-wire,Pulser/Receiver,Time of Flight,Transducer},
month = {oct},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Ultrasonic inspection of underwater guy wires with applications to floating oil platforms}},
volume = {2016-Octob},
year = {2016}
}
@inproceedings{Bertram2019,
abstract = {Subsea infrastructure inspection campaigns are conducted on a fixed time schedule to ensure the integrity of offshore assets while guaranteeing compliance with government and other stakeholders' legislation. These thorough assessments of subsea assets are achieved by acquiring and processing survey results alongside the relevant engineering data and environmental conditions. The current approach is not ideal. The infrastructure operators would prefer to have more frequent, on-demand field visits and base the assessments on an up-to-date high resolution, precise asset model. Unfortunately, this has historically not been technically feasible. The underwater inspection vehicles moved relatively slow, the positioning accuracy was limited, and the underwater sensors' accuracy and resolution was not comparable with those used in air. This is not the case anymore. We now operate inspection vehicles traveling with speeds over six knots! We can control them remotely from the office-based command center. Next to the acoustic technology, the vehicles are additionally equipped with optic sensors, dynamically collecting point clouds, images and videos of unprecedented resolution. Gigabytes of data hits the vessel every few minutes. And that's where it used to end{\ldots} Processing and, so called, "Digital Twin" modelling from this data has traditionally been taking weeks for every day of inspection. In many cases it was simply not technically possible since the data files were prohibitively big. Data required subsampling, more data processors had to go offshore, asset operators had to wait longer. Given the increasing capacity of satellite links, limitless potential of commoditized cloud computing and sophisticated Deep Learning algorithms, we are now able to move subsea asset management to the new era. Through one of their latest research {\&} development programmes - Roames, Fugro has been able to revolutionize the traditional workflows using the state-of-the-art digital technology. The resultant product is a bespoke, web-based service that enables asset (initially pipelines) inspection data to be uploaded to the secure cloud environment, processed using Machine Learning, verified by experts on shore and visualized in an intuitive 4D web viewer. All delivered to the geographically spread stakeholders, operators and decision makers in near real time. This approach greatly reduces the cost of infrastructure management practices, lowers the risk exposure and contributes to extended life of the asset. The Roames, Machine Learning based methodology was validated using a vast archive of survey data prior to testing the workflow on a live project. Detailed method statement and field results are presented in this paper.},
author = {Bertram, Stephen James and Fan, Yilun and Raffelt, David and Michalak, Pawel},
booktitle = {Society of Petroleum Engineers - Abu Dhabi International Petroleum Exhibition and Conference 2018, ADIPEC 2018},
doi = {10.2118/193122-ms},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertram et al. - 2019 - An applied machine learning approach to subsea asset inspection.pdf:pdf},
isbn = {9781613996324},
keywords = {Asset integrity,Automation,Cloud based platform,Pipelines,R{\&}D},
month = {nov},
publisher = {Society of Petroleum Engineers},
title = {{An applied machine learning approach to subsea asset inspection}},
year = {2019}
}
@article{Verendeev2017,
abstract = {Although we share evolutionary history with other primates, examples of apparent cognitive and behavioral discontinuity between humans and other species abound. Neuroanatomical and molecular differences that distinguish the human brain are evident at several levels of organization. Changes in overall anatomy include an increase in absolute and relative brain size. In addition, there may be novel parietal lobe areas in humans that are involved in processing of evermore fine-grained visuospatial information. Modifications in microstructure, such as the distribution patterns and morphology of neurons and glial cells are also significant. Finally, changes in expression of both mRNA and proteins reflect increased energy consumption and plasticity. All together, these brain specializations, when coupled with cultural forces, shaped the evolution of human cognition.},
author = {Verendeev, Andrey and Sherwood, Chet C.},
doi = {10.1016/j.cobeha.2017.02.003},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verendeev, Sherwood - 2017 - Human brain evolution.pdf:pdf},
issn = {23521546},
journal = {Current Opinion in Behavioral Sciences},
pages = {41--45},
title = {{Human brain evolution}},
volume = {16},
year = {2017}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam A method for stochastic optimization.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--15},
title = {{Adam: A method for stochastic optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@article{Wei2019,
abstract = {Semantic segmentation is an important computer vision task, which aims to allocate a semantic label to each pixel in an image. When training a segmentation model, it is common to fine-tune a classification network pre-trained on a large-scale dataset. However, as an intrinsic property of the classification model, invariance to spatial perturbation resulting from the lose of detail-sensitivity prevents segmentation networks from achieving high performance. The use of standard poolings is one of the key factors for this invariance. The most common standard poolings are max and average pooling. Max pooling can increase both the invariance to spatial perturbations and the non-linearity of the networks. Average pooling, on the other hand, is sensitive to spatial perturbations, but is a linear function. For semantic segmentation, we prefer both the preservation of detailed cues within a local feature region and non-linearity that increases a network's functional complexity. In this work, we propose a polynomial pooling (P-pooling) function that finds an intermediate form between max and average pooling to provide an optimally balanced and self-adjusted pooling strategy for semantic segmentation. The P-pooling is differentiable and can be applied into a variety of pre-trained networks. Extensive studies on the PASCAL VOC, Cityscapes and ADE20k datasets demonstrate the superiority of P-pooling over other poolings. Experiments on various network architectures and state-of-the-art training strategies also show that models with P-pooling layers consistently outperform those directly fine-tuned using pre-trained classification models.},
author = {Wei, Zhen and Zhang, Jingyi and Liu, Li and Zhu, Fan and Shen, Fumin and Zhou, Yi and Liu, Si and Sun, Yao and Shao, Ling},
doi = {10.1109/CVPR.2019.00728},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Deep Learning,Grouping and Shape,Segmentation},
pages = {7108--7116},
title = {{Building detail-sensitive semantic segmentation networks with polynomial pooling}},
url = {moz-extension://b0fced5a-38c2-4863-a3dc-9aa2ef40ac31/enhanced-reader.html?openApp{\&}pdf=http{\%}3A{\%}2F{\%}2Fopenaccess.thecvf.com{\%}2Fcontent{\_}CVPR{\_}2019{\%}2Fpapers{\%}2FWei{\_}Building{\_}Detail-Sensitive{\_}Semantic{\_}Segmentation{\_}Networks{\_}With{\_}Polynomial{\_}Pooling{\_}CVPR{\_}2019{\_}paper.pd},
volume = {2019-June},
year = {2019}
}
@article{Zhang2020,
abstract = {For steganalysis, many studies showed that convolutional neural network (CNN) has better performances than the two-part structure of traditional machine learning methods. Existing CNN architectures use various tricks to improve the performance of steganalysis, such as fixed convolutional kernels, the absolute value layer, data augmentation and the domain knowledge. However, some designing of the network structure were not extensively studied so far, such as different convolutions (inception, xception, etc.) and variety ways of pooling(spatial pyramid pooling, etc.). In this paper, we focus on designing a new CNN network structure to improve detection accuracy of spatial-domain steganography. First, we use 3× 3 kernels instead of the traditional 5× 5 kernels and optimize convolution kernels in the preprocessing layer. The smaller convolution kernels are used to reduce the number of parameters and model the features in a small local region. Next, we use separable convolutions to utilize channel correlation of the residuals, compress the image content and increase the signal-to-noise ratio (between the stego signal and the image signal). Then, we use spatial pyramid pooling (SPP) to aggregate the local features and enhance the representation ability of features by multi-level pooling. Finally, data augmentation is adopted to further improve network performance. The experimental results show that the proposed CNN structure is significantly better than other five methods such as SRM, Ye-Net, Xu-Net, Yedroudj-Net and SRNet, when it is used to detect three spatial algorithms such as WOW, S-UNIWARD and HILL with a wide variety of datasets and payloads.},
author = {Zhang, Ru and Zhu, Feng and Liu, Jianyi and Liu, Gongshen},
doi = {10.1109/TIFS.2019.2936913},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2020 - Depth-Wise Separable Convolutions and Multi-Level Pooling for an Efficient Spatial CNN-Based Steganalysis.pdf:pdf},
issn = {15566021},
journal = {IEEE Transactions on Information Forensics and Security},
keywords = {Image steganalysis,convolutional neural networks,separable convolution,spatial pyramid pooling},
pages = {1138--1150},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Depth-Wise Separable Convolutions and Multi-Level Pooling for an Efficient Spatial CNN-Based Steganalysis}},
volume = {15},
year = {2020}
}
@techreport{VanDenOord,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:pdf},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@inproceedings{Viola2001,
abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "Integral Image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
author = {Viola, Paul and Jones, Michael},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/cvpr.2001.990517},
issn = {10636919},
title = {{Rapid object detection using a boosted cascade of simple features}},
volume = {1},
year = {2001}
}
@article{Liu2015,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300 input, SSD achieves 74.3{\%} mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/ tree/ssd.},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng Yang and Berg, Alexander C.},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {1512.02325},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2016 - SSD Single shot multibox detector.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural network,Real-time object detection},
month = {dec},
pages = {21--37},
title = {{SSD: Single shot multibox detector}},
url = {http://arxiv.org/abs/1512.02325 http://dx.doi.org/10.1007/978-3-319-46448-0{\_}2},
volume = {9905 LNCS},
year = {2016}
}
@techreport{SpaceCenter2014,
abstract = {The advanced inspection system is an autonomous control and analysis system that improves the inspection and remediation operations for ground and surface systems. It uses optical imaging technology with intelligent computer vision algorithms to analyze physical features of the real-world environment to make decisions and learn from experience. The advanced inspection system plans to control a robotic manipulator arm, an unmanned ground vehicle and cameras remotely, automatically and autonomously. There are many computer vision, image processing and machine learning techniques available as open source for using vision as a sensory feedback in decision-making and autonomous robotic movement. My responsibilities for the advanced inspection system are to create a software architecture that integrates and provides a framework for all the different subsystem components; identify open-source algorithms and techniques; and integrate robot hardware. Nomenclature AIS = advanced inspection system HMI = human machine interface UGV = unmanned ground vehicle VMSS = video management server system IHM = integrated health management ROS = robot operating system OS = operating system API = application programing interface RPC = remote procedure call VMS = video management software SDK = software development kit GUI = graphical user interface AIROS = advanced inspection and repair operating system SDF = simulation description format},
author = {Center, Kennedy Space and Wehner, Walter S},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Center, Wehner - 2014 - NASA KSC – Internship Final Report Vision based autonomous robotic control for advanced inspection and repair Vi.pdf:pdf},
pages = {1--7},
title = {{NASA KSC – Internship Final Report Vision based autonomous robotic control for advanced inspection and repair Vision based autonomous robotic control for advanced inspection and repair}},
url = {https://ntrs.nasa.gov/search.jsp?R=20140010507},
year = {2014}
}
@article{Feng2019,
abstract = {The field of computer vision is experiencing a great-leap-forward development today. This paper aims at providing a comprehensive survey of the recent progress on computer vision algorithms and their corresponding hardware implementations. In particular, the prominent achievements in computer vision tasks such as image classification, object detection and image segmentation brought by deep learning techniques are highlighted. On the other hand, review of techniques for implementing and optimizing deep-learning-based computer vision algorithms on GPU, FPGA and other new generations of hardware accelerators are presented to facilitate real-time and/or energy-efficient operations. Finally, several promising directions for future research are presented to motivate further development in the field.},
author = {Feng, Xin and Jiang, Youni and Yang, Xuejiao and Du, Ming and Li, Xin},
doi = {10.1016/j.vlsi.2019.07.005},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feng et al. - 2019 - Computer vision algorithms and hardware implementations A survey.pdf:pdf},
issn = {01679260},
journal = {Integration},
keywords = {Artificial intelligence,Computer vision,Deep convolutional neural network,Hardware accelerator},
month = {nov},
pages = {309--320},
publisher = {Elsevier B.V.},
title = {{Computer vision algorithms and hardware implementations: A survey}},
volume = {69},
year = {2019}
}
@techreport{Krizhevsky,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Communications of the ACM},
doi = {10.1145/3065386},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2017 - ImageNet classification with deep convolutional neural networks.pdf:pdf},
issn = {15577317},
number = {6},
pages = {84--90},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://code.google.com/p/cuda-convnet/},
volume = {60},
year = {2017}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. 15 Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell 16 and Witten et al. 24). However, much of the "folk knowledge" that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article. {\textcopyright} 2012 ACM.},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingos - 2012 - A few useful things to know about machine learning.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {oct},
number = {10},
pages = {78--87},
title = {{A few useful things to know about machine learning}},
volume = {55},
year = {2012}
}
@inproceedings{Griffiths2000,
abstract = {Autonomous Underwater Vehicles (AUVs) are becoming accepted data-gathering tools within the marine science community in Europe, the US and elsewhere. Technology can now provide vehicles with a useful range and depth envelope. For example, the Southampton Oceanography Centre's Autosub-1 vehicle has already covered 263 km in a single oceanograpic survey mission, reaching depths of 500 m off Bermuda in September 1998. The challenge now is to 'free the technology from the research community. Potential direct and indirect benefits from the offshore energy industry's use of AUVs have been quantified. For one company, the benefits of using survey class AUVs has been estimated at {\$}60m over 5 years. The economics become more attractive as the industry moves to deeper water. While today's AUVs of long endurance are limited in depth, developments on the horizon in composite materials and high capacity, lightweight secondary batteries will enable 1000 km range, eight-day endurance and at least 1600 m diving depth to be in active service within the next two years. This paper will review the oceanographic survey achievements of the Autosub AUV over its 216 missions (to December 1999), working in UK, US and Bermudan waters. Moving to the future, the paper will illustrate how such an AUV could be used in the offshore industry for survey, monitoring and emergency response. Finally, we will describe the present Autosub development programme, scheduled to deliver an operational 1000 km, 1600 m vehicle by mid 2000.},
author = {Griffiths, G. and Birch, K. G. and Millard, N. W. and McPhail, S. D. and Stevenson, P. and Pebody, M. and Perrett, J. R. and Webb, A. T. and Squires, M. and Harris, A.},
booktitle = {Proceedings of the Annual Offshore Technology Conference},
doi = {10.4043/12003-ms},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths et al. - 2000 - Oceanographic surveys with a 50 hour endurance autonomous underwater vehicle.pdf:pdf},
issn = {01603663},
pages = {609--613},
title = {{Oceanographic surveys with a 50 hour endurance autonomous underwater vehicle}},
volume = {2},
year = {2000}
}
@article{Griffiths2003,
abstract = {Over the last decade Autonomous Underwater Vehicles (AUVs) have become capable marine platforms. AUVs built in Europe are now operational tools in the global offshore business. Industry has seen significant cost savings from this new technology. Data quality has improved to the point where some customers in the offshore industry now prefer not to use shipboard systems. But what of AUVs for operational oceanography? Is the successful collaboration between research and industry in the offshore business a transferable model? This paper examines AUVs in scenarios relevant to EuroGOOS against the capabilities of existing technologies and those under development. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Griffiths, Gwyn and Edwards, Ia},
doi = {10.1016/S0422-9894(03)80038-7},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths, Edwards - 2003 - AUVs designing and operating next generation vehicles.pdf:pdf},
issn = {04229894},
journal = {Elsevier Oceanography Series},
keywords = {Autonomous vehicles,survey,technology},
number = {C},
pages = {229--236},
title = {{AUVs: designing and operating next generation vehicles}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0422989403800387},
volume = {69},
year = {2003}
}
@inproceedings{McLeod2011,
abstract = {Integrity Management of deepwater fields requires routine general inspection of critical infrastructure. To date, the only means of conducting such inspections is through the use of remotely operated vehicles (ROVs). Deepwater ROV spreads are big and heavy, requiring large support vessels with dynamic positioning capability and a significant number of personnel at sea. The capabilities of autonomous underwater vehicles (AUVs) have been enhanced through developments in autonomous technology progressing to the point that AUVs can now conduct general purpose inspection of subsea facilities. {\textcopyright} 2011 MTS.},
author = {McLeod, Dan and Jacobson, John},
booktitle = {OCEANS'11 - MTS/IEEE Kona, Program Book},
doi = {10.23919/oceans.2011.6106972},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McLeod, Jacobson - 2011 - Autonomous UUV inspection Revolutionizing undersea inspection.pdf:pdf},
isbn = {9781457714276},
title = {{Autonomous UUV inspection Revolutionizing undersea inspection}},
year = {2011}
}
@article{Ridao2015,
abstract = {While commercially available AUVs are routinely used in survey missions, a new set of applications exist which clearly demand intervention capabilities. The maintenance of permanent underwater observatories, submerged oil wells, cabled sensor networks, pipes and the deployment and recovery of benthic stations are a few of them. These tasks are addressed nowadays using manned submersibles or work-class ROVs, equipped with teleoperated arms under human supervision. Although researchers have recently opened the door to future I-AUVs, a long path is still necessary to achieve autonomous underwater interventions. This paper reviews the evolution timeline in autonomous underwater intervention systems. Milestone projects in the state of the art are reviewed, highlighting their principal contributions to the field. To the best of the authors' knowledge, only three vehicles have demonstrated some autonomous intervention capabilities so far: ALIVE, SAUVIM and GIRONA 500, being the last one the lightest one. In this paper GIRONA 500 I-AUV is presented and its software architecture discussed. Recent results in different scenarios are reported: (1) valve turning and connector plugging/unplugging while docked to a subsea panel, (2) free floating valve turning using learning by demonstration, and (3) multipurpose free-floating object recovery. The paper ends discussing the lessons learned so far.},
author = {Ridao, Pere and Carreras, Marc and Ribas, David and Sanz, Pedro J. and Oliver, Gabriel},
doi = {10.1016/j.arcontrol.2015.09.015},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ridao et al. - 2015 - Intervention AUVs The next challenge.pdf:pdf},
issn = {13675788},
journal = {Annual Reviews in Control},
keywords = {Autonomous Vehicles,Marine Systems,Robotic Manipulators},
pages = {227--241},
publisher = {Elsevier Ltd},
title = {{Intervention AUVs: The next challenge}},
volume = {40},
year = {2015}
}
@article{Ridolfi2015,
abstract = {This paper presents the modelling and the control architecture of an Autonomous Underwater Vehicle for Intervention (I-AUV). Autonomous underwater manipulation with free-floating base is still an open topic of research, far from reaching an industrial product. Dynamic manipulation tasks, where relevant vehicle velocities are required during manipulation, over an additional challenge. In this paper, the accurate modelling of an I-AUV is described, not neglecting the interaction with the fluid. A grasp planning strategy is proposed and integrated in the control of the whole system. The performances of the I-AUV have been analysed by means of simulations of a dynamic manipulation task.},
author = {{Conti R}, Ridolfi A and {Fanelli F}, Costanzi R},
doi = {10.4172/2168-9695.1000132},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Conti R, Fanelli F - 2015 - A Dynamic Manipulation Strategy for an Intervention Autonomous Underwater Vehicle(2).pdf:pdf},
issn = {21689695},
journal = {Advances in Robotics {\&} Automation},
number = {02},
publisher = {OMICS Publishing Group},
title = {{A Dynamic Manipulation Strategy for an Intervention: Autonomous Underwater Vehicle}},
url = {http://www.omicsgroup.org/journals/a-dynamic-manipulation-strategy-for-an-intervention-autonomous-underwater-vehicle-2168-9695-1000132.php?aid=60232},
volume = {04},
year = {2015}
}
@article{ContiR2015,
abstract = {This paper presents the modelling and the control architecture of an Autonomous Underwater Vehicle for Intervention (I-AUV). Autonomous underwater manipulation with free-floating base is still an open topic of research, far from reaching an industrial product. Dynamic manipulation tasks, where relevant vehicle velocities are required during manipulation, over an additional challenge. In this paper, the accurate modelling of an I-AUV is described, not neglecting the interaction with the fluid. A grasp planning strategy is proposed and integrated in the control of the whole system. The performances of the I-AUV have been analysed by means of simulations of a dynamic manipulation task.},
author = {{Conti R}, Ridolfi A and {Fanelli F}, Costanzi R},
doi = {10.4172/2168-9695.1000132},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Conti R, Fanelli F - 2015 - A Dynamic Manipulation Strategy for an Intervention Autonomous Underwater Vehicle.pdf:pdf},
journal = {Advances in Robotics {\&} Automation},
number = {02},
publisher = {OMICS Publishing Group},
title = {{A Dynamic Manipulation Strategy for an Intervention: Autonomous Underwater Vehicle}},
volume = {04},
year = {2015}
}
@inproceedings{Evans2001a,
abstract = {The SWIMMER Vehicle is a EU-Thermie funded project to develop a prototype autonomous vehicle capable of deploying existing work-class ROVs to subsea installations - removing the need for continuous surface support vessels and long umbilical cables (and the associated costs, power restraints and weather dependencies). To be able to accurately position and dock a large free swimming AUV (total vehicle and payload in-air mass exceeds 2.5 tonnes, and over 6 metres in length), at depths up to 1000m after transits of up to 10km is particularly challenging. The deployment configuration mounts the ROV above the SWIMMER AUV. The ROV is inactive during deployment, and the AUV is completely self-contained in terms of power, thrusters and computer-control. During transit, the LRPS (Long Range Positioning System) a combined acoustic Positioning/Data link provides autonomous navigation to the seabed, together with top station monitoring. During docking, the SRPS (Short Range Positioning System) directs the vehicle during the final 20 metres of descent and provides the necessary positional information for a precise alignment for a mechanical docking with a seabed station. When docked, the SWIMMER uses computer-controlled clamps that allow the ROV to be disengaged remotely from the SWIMMER while on the seabed docking station. In this configuration, power and communications are fed from the seabed network to allow the SWIMMER to recharge, and the ROV to be remotely controlled (completed conventionally by pilot's based, in theory, anywhere in the world).},
author = {Evans, J. C. and Keller, K. M. and Smith, J. S. and Marty, P. and Rigaud, V.},
booktitle = {Oceans Conference Record (IEEE)},
doi = {10.1109/oceans.2001.968776},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans et al. - 2001 - Docking techniques and evaluation trials of the SWIMMER AUV An autonomous deployment AUV for workclass ROVs(2).pdf:pdf},
issn = {01977385},
pages = {520--528},
title = {{Docking techniques and evaluation trials of the SWIMMER AUV: An autonomous deployment AUV for workclass ROVs}},
volume = {1},
year = {2001}
}
@techreport{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1409.1556},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - Very deep convolutional networks for large-scale image recognition.pdf:pdf},
keywords = {()},
title = {{Very deep convolutional networks for large-scale image recognition}},
url = {http://www.robots.ox.ac.uk/},
year = {2015}
}
@article{Bonin-Font2016,
abstract = {Posidonia Oceanica (P.O.) is a Mediterranean endemic seagrass strongly related to the health of the coastal ecosystems. Monitoring the presence and state of P.O. is essential not only for safeguarding the shallow-water life diversity, but also as an indicator of the water quality. Nowadays, the control of P.O. is done by divers in successive missions of a duration limited by the capacity of the scuba tanks. This paper proposes the application of robotic and computer vision technologies to upgrade these current methods, namely: 1) employing a lightweight Autonomous Underwater Vehicle (AUV) equipped with cameras to survey and image marine areas, 2) the automatic discrimination of P.O. from the rest of the seafloor, using several techniques based on image texture analysis and machine learning, and, 3) the fast computation of 2D maps (photo-mosaics) of the surveyed areas from all the images included in the grabbed video sequences; these mosaics are extremely useful to measure the real extension of the meadows and some of the descriptors needed for a biological analysis. Experiments conducted with an AUV in several marine areas of Mallorca reveal promising results in the discrimination of different patterns of P.O. and in the construction of highly realistic photo-mosaics of the surveyed areas.},
author = {Bonin-Font, Francisco and Campos, Miquel Massot and Codina, Gabriel Oliver},
doi = {10.1016/j.ifacol.2016.10.485},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonin-Font, Campos, Codina - 2016 - Towards Visual Detection, Mapping and Quantification of Posidonia Oceanica using a Lightweight AUV.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Autonomous Underwater Vehicles,Machine Learning,Visual Surveying and Mosaicking},
number = {23},
pages = {500--505},
publisher = {Elsevier B.V.},
title = {{Towards Visual Detection, Mapping and Quantification of Posidonia Oceanica using a Lightweight AUV}},
volume = {49},
year = {2016}
}
@article{Cao2019,
abstract = {The autonomous underwater vehicle (AUV) is widely used to search for unknown targets in the complex underwater environment. Due to the unpredictability of the underwater environment, this paper combines the traditional frontier exploration method with deep reinforcement learning (DRL) to enable the AUV to explore the unknown underwater environment autonomously. In this paper, a grid map of the search environment is built by the grid method. The designed asynchronous advantage actor-critic (A3C) network structure is used in the traditional frontier exploration method for target search tasks. This network structure enables the AUV to learn from its own experience and generate search strategies for the various unknown environment. At the same time, DRL and dual-stream Q-learning algorithms are used for AUV navigation to further optimize the search path. The simulations and experiments in an unknown underwater environment with different layouts show that the proposed algorithm can accomplish target search tasks with a high success rate, and it can adapt to different environments. In addition, compared to other search methods, the frontier exploration algorithm based on DRL can search a wider environment faster, which results in a higher search efficiency and reduced search time.},
author = {Cao, Xiang and Sun, Changyin and Yan, Mingzhong},
doi = {10.1109/ACCESS.2019.2929120},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao, Sun, Yan - 2019 - Target Search Control of AUV in Underwater Environment with Deep Reinforcement Learning.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Target search,deep learning,frontier exploration,reinforcement learning},
month = {jul},
pages = {96549--96559},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Target Search Control of AUV in Underwater Environment with Deep Reinforcement Learning}},
volume = {7},
year = {2019}
}
@inproceedings{Osaku2013,
abstract = {On the construction of Kanda port in Fukuoka prefecture, many harmful chemical bombs have been discovered beneath the sea bottom and they are needed to be dug up carefully and quickly as possible. So our group is trying to develop a new sub-bottom interferometric synthetic aperture imaging sonar (sub-bottom interferometric SAS) system to recognize chemical bombs as centimeters-resolution 3D-sub-bottom acoustic image. In this R{\&}D study, it is the reasonable methodology to use an autonomous underwater vehicle (AUV) which can survey the seafloor with sub-bottom interferometric SAS transmitter and receiver at a constant height. To accomplish this R{\&}D goal, positioning AUV accurately is needed, so we are trying to develop the technique which minimizes the error of positioning, using autonomous surface vehicle (ASV) which tracks AUV and surveys its absolute position by super short-baseline (SSBL) method. In development of tracking ASV, it is important to develop the controlling algorism which orders ASV to steer stably and control adequately its velocity according to the result of SSBL positioning of the AUV. Based on machine learning method, we are trying to develop an algorism which infers appropriate control of ASV from precious controlling log. Implementation of this algorism will improve the precision of underwater positioning. This paper reports the development status of our ASV and controlling algorism. {\textcopyright} 2013 IEEE.},
author = {Osaku, Junichiro and Asada, Akira and Maeda, Fumitaka and Yamagata, Yozo and Kanamaru, Takashi},
booktitle = {2013 IEEE International Underwater Technology Symposium, UT 2013},
doi = {10.1109/UT.2013.6519900},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Osaku et al. - 2013 - Implementation of machine learning algorism to autonomous surface vehicle for tracking and navigating AUV.pdf:pdf},
isbn = {9781467359481},
title = {{Implementation of machine learning algorism to autonomous surface vehicle for tracking and navigating AUV}},
year = {2013}
}
@inproceedings{Wang2019,
abstract = {This work studies online learning-based trajectory planning for multiple autonomous underwater vehicles (AUVs) to estimate a water parameter field of interest in the under-ice environment. A centralized system is considered, where several fixed access points (APs) on the ice layer are introduced as gateways for communications between the AUVs and a remote data fusion center (FC). We model the water parameter field of interest as a Gaussian process (GP) with unknown hyper-parameters. The AUV trajectories for sampling are determined on an epoch-by-epoch basis. At the end of each epoch, the APs relay the observed field samples from all the AUVs to the FC which computes the posterior distribution of the field based on the Gaussian process regression (GPR) and estimates the field hyper-parameters. The optimal trajectories of all the AUVs in the next epoch are determined to minimize a long-term cost that is defined based on the field uncertainty reduction and the AUV mobility cost, subject to the kinematics constraint, the communication range constraint and the sensing area constraint. We formulate the adaptive trajectory planning problem as a Markov decision process (MDP). A reinforcement learning (RL)-based online learning method is designed to determine the optimal AUV trajectories in a constrained continuous space. Simulation results show that the proposed learning-based trajectory planning algorithm has performance similar to a benchmark method that assumes perfect knowledge of the field hyper-parameters.},
author = {Wang, Chaofeng and Wei, Li and Wang, Zhaohui and Song, Min and Mahmoudian, Nina},
booktitle = {OCEANS 2018 MTS/IEEE Charleston, OCEAN 2018},
doi = {10.1109/OCEANS.2018.8604754},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2019 - Reinforcement Learning-based Adaptive Trajectory Planning for AUVs in Under-ice Environments.pdf:pdf},
isbn = {9781538648148},
month = {jan},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Reinforcement Learning-based Adaptive Trajectory Planning for AUVs in Under-ice Environments}},
year = {2019}
}
@inproceedings{Iwanowski1994,
author = {Iwanowski, M.D.},
doi = {10.1109/oceans.1994.363965},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Iwanowski - 2002 - Surveillance unmanned underwater vehicle.pdf:pdf},
month = {dec},
pages = {I/116--I/119},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Surveillance unmanned underwater vehicle}},
year = {2002}
}
@inproceedings{Kunz2010,
abstract = {Visual maps of the seafloor should ideally provide the ability to measure individual features of interest in real units. Two-dimensional photomosaics cannot provide this capability without making assumptions that often fail over 3-D terrain, and are generally used for visualization, but not for measurement. Full 3-D structure can be recovered using stereo vision, structure from motion (SFM), or simultaneous localization and mapping (SLAM); of these techniques, only stereo vision is suitable for fully dense (a distance measurement for each imaged pixel) 3-D structure in the absence of significant frame-to-frame overlap. Stereo vision is notoriously dependent on camera calibration, however, which is difficult to compute and maintain in the field. The fewer dependencies an AUV mapping system has on camera calibration, the more reliably it will be able to produce useful maps of the seafloor. We present a system for recovering the 7-DOF relationship between the AUV's estimation frame and the camera rig (Euclidean offsets plus scale), which reconciles the robot's odometry-based pose estimate with stereo visual odome-try. The combination of robust frame-to-frame visual feature matching, subpixel stereo correspondence estimation, and high-accuracy on-board vehicle navigation sensors enables us to self-calibrate the extrinsic parameters of the stereo rig including scale, and produce metric maps using only vehicle navigation and the computed camera calibration. Using data acquired in the Bering Sea by the SeaBED AUV in August, 2009, our initial results indicate that accumulated navigation drift is less than 0.5{\%} of distance travelled, suggesting that a visual SLAM system for correcting drift and building a final map would only require the robot's path to cross itself every few hundred meters. In addition to providing a large-scale metric 3-D map, the corrected stereo calibration enables scientists to measure the sizes of imaged objects without additional hardware such as laser points or acoustic ranging systems. {\textcopyright} 2010 IEEE.},
author = {Kunz, Clayton and Singh, Hanumant},
booktitle = {2010 IEEE/OES Autonomous Underwater Vehicles, AUV 2010},
doi = {10.1109/AUV.2010.5779655},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kunz, Singh - 2010 - Stereo self-calibration for seafloor mapping using AUVs.pdf:pdf},
isbn = {9781612849812},
title = {{Stereo self-calibration for seafloor mapping using AUVs}},
year = {2010}
}
@inproceedings{Negahdaripour1991,
abstract = {A particular advantage of an optical stationkeeping system is its ability to use natural rather than man-made beacons. Improvements to previously reported optical flow methods for detecting vehicle motion are presented. Experimental results indicate that an adaptation of Newton-Raphson search combined with the use of a low-noise, high-accuracy camera drastically reduces the number of points at which computations need be done. Experiments with an algorithm which accounts for illumination variations one encounters in undersea environments show significant improvement in the estimation of vehicle motion.},
author = {Negahdaripour, S. and Shokrollahi, A. and Fox, J. and Arora, S.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/robot.1991.132048},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Negahdaripour et al. - 1991 - Improved methods for undersea optical stationkeeping.pdf:pdf},
issn = {10504729},
month = {dec},
pages = {2752--2758},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Improved methods for undersea optical stationkeeping}},
volume = {3},
year = {1991}
}
@inproceedings{Dunbabin2007,
abstract = {This paper overviews the development of a vision-based AUY along with a set of complementary operational strategies to allow reliable autonomous data collection in relatively shallow water and coral reef environments. The development of the AUV, called Starbug, encountered many challenges in terms of vehicle design, navigation and control. Some of these challenges are discussed with focus on operational strategies for estimating and reducing the total navigation error when Using lower-resolution sensing modalities. Results are presented from recent field trials which illustrate the ability of the vehicle and associated operational strategies to enable rapid collection of Visual data sets suitable for marine research applications. {\textcopyright} 2007 IEEE.},
author = {Dunbabin, Matthew D. and Allen, Simon S.},
booktitle = {OCEANS 2007 - Europe},
doi = {10.1109/oceanse.2007.4302309},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dunbabin, Allen - 2007 - Large-scale habitat mapping using vision-based AUVs Experiences, challenges {\&} vehicle design.pdf:pdf},
isbn = {1424406358},
title = {{Large-scale habitat mapping using vision-based AUVs: Experiences, challenges {\&} vehicle design}},
year = {2007}
}
@inproceedings{Zhao2012,
abstract = {In this paper, an overview of stereo vision, stereo image processing is introduced as well as a method of navigation based on stereo vision for mobile robot is provided. Making use of 3-d stereo reconstruction, the recognition of visible terrain in front of the mobile robot is solved for successive obstacle avoidance. The area-based stereo reconstruction algorithm which combines the pyramidal data structure and dynamic programming technique has been used for the recognition of the local environment. The vision system can thus be used to identify, locate and approach mechanical objects autonomously. {\textcopyright} 2012 IEEE.},
author = {Zhao, Yong Guo and Cheng, Wei and Liu, Guang Liang},
booktitle = {Proceedings - 2012 5th International Conference on Intelligent Computation Technology and Automation, ICICTA 2012},
doi = {10.1109/ICICTA.2012.173},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Cheng, Liu - 2012 - The navigation of mobile robot based on stereo vision.pdf:pdf},
isbn = {9780769546377},
keywords = {Mobile robot,Obstacle avoidance,Stereo vision},
pages = {670--673},
title = {{The navigation of mobile robot based on stereo vision}},
year = {2012}
}
@inproceedings{Negahdaripour1998,
abstract = {A vision system has been developed based on the application of a 3D direct motion estimation algorithm to facilitate autonomous or operated-assisted missions of AUVS and ROVs near the ocean floor. The main capabilities of interest are vision-based vehicle positioning, navigation and trajectory following, as well as mosaicking of sea bed images. The system performance in enabling these functions has been demonstrated on a three-thruster surface vehicle that operates in a 6'×12'×6' water tank, with the bottom surface set up to simulate a sea floor environment. Images from a down-look camera installed on the vehicle are digitized and processed on a Windows NT Dual Pentium-200 to estimate the vehicle's motion, which is transmitted via the serial link to the vehicle control system running on a 386 processor. This information is employed to maintain or move to a desired position, follow a specified trajectory, and to construct in (near) real-time a composite image of the scene surface.},
author = {Negahdaripour, S. and Xu, X. and Khamene, A.},
booktitle = {Proceedings - 4th IEEE Workshop on Applications of Computer Vision, WACV 1998},
doi = {10.1109/ACV.1998.732892},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Negahdaripour, Xu, Khamene - 1998 - A vision system for real-time positioning, navigation, and video mosaicing of sea floor imagery in t.pdf:pdf},
isbn = {0818686065},
pages = {248--249},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A vision system for real-time positioning, navigation, and video mosaicing of sea floor imagery in the application of ROVs/AUVs}},
volume = {1998-Octob},
year = {1998}
}
@article{Zelasco2000,
abstract = {The work presented has been developed in the frame of a project of stereo vision for autonomous underwater vehicles (AUV) provided with optic sensors, simulating the system of human binocular vision which gave an origin to several work lines. The study involves three steps. The first one concerning with the obtention of relative orientation among components of stereo couple. That relative orientation will help to find an optimal plane to minimize deformation when projecting images onto. Finally, it follows the projection itself of the couple over optimal plane found.},
author = {Zelasco, Jos{\'{e}} F. and Dagum, Diego Amin and Donayo, Judith and Arcomano, Teresa},
doi = {10.1109/OCEANS.2000.882255},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zelasco et al. - 2000 - Computer vision in AUVs Automatic roto-rectification of stereo images.pdf:pdf},
issn = {01977385},
journal = {Oceans Conference Record (IEEE)},
pages = {2169--2176},
title = {{Computer vision in AUVs: Automatic roto-rectification of stereo images}},
volume = {3},
year = {2000}
}
@book{Szeliski2010a,
abstract = {The goal of computer vision is to extract information from images. For example, structure from motion methods can recover a three-dimensional model of an object from a sequence of views, for use in robot grasping, medical imaging, and graphical modeling; model-based recognition methods can determine the best matches of stored models to image data, for use in visual inspection and image database searches; and visual motion analysis can recover image motion patterns for use in vehicle guidance and processing digital video. Computer vision is closely related to the field of image processing. In computer vision the focus is on extracting information from image data, whereas in image processing the focus is on transforming images. For instance, extracting a three-dimensional model from two-dimensional images is more of a computer vision problem than an image processing one, whereas image enhancement is more of an image processing problem than a computer vision one.},
author = {Szeliski, Richard},
booktitle = {Computer Science Handbook, Second Edition},
doi = {10.4324/9780429042522-10},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szeliski - 2010 - Computer Vision Algorithms and Applications(2).pdf:pdf},
isbn = {9780203494455},
issn = {1340-5551},
keywords = {computer vision},
mendeley-tags = {computer vision},
pages = {979},
publisher = {Springer},
title = {{Computer Vision: Algorithms and Applications}},
url = {https://link.springer.com/book/10.1007{\%}2F978-1-84882-935-0},
year = {2010}
}
@inproceedings{Carrera2014,
abstract = {Intervention autonomous underwater vehicles (I-AUVs) are a promising platform to perform intervention task in underwater environments, replacing current methods like remotely operate underwater vehicles (ROVs) and manned sub-mersibles that are more expensive. This article proposes a complete system including all the necessary elements to perform a valve turning task using an I-AUV. The knowledge of an operator to perform the task is transmitted to an I-AUV by a learning by demonstration (LbD) algorithm. The algorithm learns the trajectory of the vehicle and the end-effector to accomplish the valve turning. The method has shown its feasibility in a controlled environment repeating the learned task with different valves and configurations.},
author = {Carrera, Arnau and Palomeras, Narcis and Ribas, David and Kormushev, Petar and Carreras, Marc},
booktitle = {Oceans 2014 - Taipei},
doi = {10.1109/OCEANS-TAIPEI.2014.6964483},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carrera et al. - 2014 - An Intervention-AUV learns how to perform an underwater valve turning.pdf:pdf},
isbn = {9781479936465},
month = {nov},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{An Intervention-AUV learns how to perform an underwater valve turning}},
year = {2014}
}
@incollection{Schaal1997,
abstract = {Creating robots that can easily learn new skills as effectively as humans (or dogs or ants) is the holly grail of intelligent robotics. Several approaches to achieve this goal have appeared over the years.},
author = {Mohammad, Yasser and Nishida, Toyoaki},
booktitle = {Advanced Information and Knowledge Processing},
chapter = {Mohammad, },
doi = {10.1007/978-3-319-25232-2_13},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohammad, Nishida - 2015 - Learning from demonstration.pdf:pdf},
isbn = {0262100657},
issn = {21978441},
keywords = {2-2 Hikaridai,619-02 Kyoto,801 Atlantic Drive,Atlanta,GA 30332-0280 ATR Human Information Processing,Georgia Tech,Seiko-cho,Soraku-gun,http://wwwccgatechedulfac/StefanSchaal College of},
number = {9783319252308},
pages = {293--317},
title = {{Learning from demonstration}},
url = {http://www.cc.gatech.edulfac/Stefan.Schaal},
year = {2015}
}
@inproceedings{Ribas2011,
abstract = {This paper outlines the specifications and basic design approach taken on the development of the Girona 500, an autonomous underwater vehicle whose most remarkable characteristic is its capacity to reconfigure for different tasks. The capabilities of this new vehicle range from different forms of seafloor survey to inspection and intervention tasks. {\textcopyright} 2011 IEEE.},
author = {Ribas, David and Ridao, Pere and Mag{\'{i}}, Llu{\'{i}}s and Palomeras, Narc{\'{i}}s and Carreras, Marc},
booktitle = {OCEANS 2011 IEEE - Spain},
doi = {10.1109/Oceans-Spain.2011.6003395},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribas et al. - 2011 - The Girona 500, a multipurpose autonomous underwater vehicle.pdf:pdf},
isbn = {9781457700866},
month = {jun},
pages = {1--5},
publisher = {IEEE},
title = {{The Girona 500, a multipurpose autonomous underwater vehicle}},
url = {http://ieeexplore.ieee.org/document/6003395/},
year = {2011}
}
@inproceedings{Palomeras2007,
abstract = {This paper presents the design and implementation of a Mission Control System (MCS) for an AUV. The mission is easily described using an imperative-like pseudo-code called Mission Control Language (MCL) that allows sequential/parallel, conditional and iterative task execution. MCL can be automatically translated into a Petri net, to formally describe the mission thread of execution. Then the MCS executes the Petri net in real-time over a generic layer that communicates with a particular control architecture using predefined actions and events. Concepts are illustrated with a simple mission. Copyright {\textcopyright} 2007 International Federation of Automatic Control All Rights Reserved.},
author = {Palomeras, Narcis and Ridao, Pere and Carreras, Marc and Silvestre, Carlos},
booktitle = {IFAC Proceedings Volumes (IFAC-PapersOnline)},
doi = {10.3182/20080706-5-KR-1001.2209},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Palomeras et al. - 2008 - Towards a mission control language for AUVs.pdf:pdf},
isbn = {9783902661005},
issn = {14746670},
keywords = {Autonomous underwater vehicles,Control architectures in marine systems,Marine system navigation, guidance and control},
number = {1 PART 1},
pages = {123--128},
title = {{Towards a mission control language for AUVs}},
volume = {17},
year = {2008}
}
@article{Palomeras2012,
abstract = {This paper presents a control architecture for an autonomous underwater vehicle (AUV) named the Component Oriented Layer-based Architecture for Autonomy (COLA2). The proposal implements a component-oriented layer-based control architecture structured in three layers: the reactive layer, the execution layer, and the mission layer. Concerning the reactive layer, to improve the vehicle primitives' adaptability to unknown changing environments, reinforcement learning (RL) techniques have been programmed. Starting from a learned-in-simulation policy, the RL-based primitive cableTracking has been trained to follow an underwater cable in a real experiment inside a water tank using the Ictineu AUV. The execution layer implements a discrete event system (DES) based on Petri nets (PNs). PNs have been used to safely model the primitives' execution flow by means of Petri net building block (PNBBs) that have been designed according to some reachability properties showing that it is possible to compose them preserving these qualities. The mission layer describes the mission phases using a high-level mission control language (MCL), which is automatically compiled into a PN. The MCL presents agreeable properties of simplicity and structured programming. MCL can be used to describe offline imperative missions or to describe planning operators, in charge of solving a particular phase of a mission. If planning operators are defined, an onboard planner will be able to sequence them to achieve the proposed goals. The whole architecture has been validated in a cable tracking mission divided in two main phases. First, the cableTracking primitive of the reactive layer has been trained to follow a cable in a water tank with the Ictineu AUV, one of the research platforms available in the Computer Vision and Robotics Group (VICOROB), University of Girona, Girona, Spain. Second, the whole architecture has been proved in a realistic simulation of a whole cable tracking mission. {\textcopyright} 1976-2012 IEEE.},
author = {Palomeras, Narcis and El-Fakdi, Andres and Carreras, Marc and Ridao, Pere},
doi = {10.1109/JOE.2012.2205638},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Palomeras et al. - 2012 - COLA2 A control architecture for AUVs.pdf:pdf},
issn = {03649059},
journal = {IEEE Journal of Oceanic Engineering},
keywords = {Mission programming,Petri nets (PNs),Reinforcement learning (RL),Robot control architectures,Underwater vehicles},
month = {oct},
number = {4},
pages = {695--716},
title = {{COLA2: A control architecture for AUVs}},
url = {http://ieeexplore.ieee.org/document/6263248/},
volume = {37},
year = {2012}
}
@inproceedings{Ribas2011a,
abstract = {This paper outlines the specifications and basic design approach taken on the development of the Girona 500, an autonomous underwater vehicle whose most remarkable characteristic is its capacity to reconfigure for different tasks. The capabilities of this new vehicle range from different forms of seafloor survey to inspection and intervention tasks. {\textcopyright} 2011 IEEE.},
author = {Ribas, David and Ridao, Pere and Mag{\'{i}}, Llu{\'{i}}s and Palomeras, Narc{\'{i}}s and Carreras, Marc},
booktitle = {OCEANS 2011 IEEE - Spain},
doi = {10.1109/Oceans-Spain.2011.6003395},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribas et al. - 2011 - The Girona 500, a multipurpose autonomous underwater vehicle.pdf:pdf},
isbn = {9781457700866},
title = {{The Girona 500, a multipurpose autonomous underwater vehicle}},
year = {2011}
}
@inproceedings{Ribas2011b,
abstract = {This paper outlines the specifications and basic design approach taken on the development of the Girona 500, an autonomous underwater vehicle whose most remarkable characteristic is its capacity to reconfigure for different tasks. The capabilities of this new vehicle range from different forms of seafloor survey to inspection and intervention tasks. {\textcopyright} 2011 IEEE.},
author = {Ribas, David and Ridao, Pere and Mag{\'{i}}, Llu{\'{i}}s and Palomeras, Narc{\'{i}}s and Carreras, Marc},
booktitle = {OCEANS 2011 IEEE - Spain},
doi = {10.1109/Oceans-Spain.2011.6003395},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribas et al. - 2011 - The Girona 500, a multipurpose autonomous underwater vehicle.pdf:pdf},
isbn = {9781457700866},
month = {jun},
pages = {1--5},
publisher = {IEEE},
title = {{The Girona 500, a multipurpose autonomous underwater vehicle}},
url = {http://ieeexplore.ieee.org/document/6003395/},
year = {2011}
}
@article{Ribas2012,
abstract = {This paper outlines the specifications and basic design approach taken on the development of the Girona 500, an autonomous underwater vehicle whose most remarkable characteristic is its capacity to reconfigure for different tasks. The capabilities of this new vehicle range from different forms of seafloor survey to inspection and intervention tasks. {\textcopyright} 2011 IEEE.},
author = {Ribas, David and Palomeras, Narc{\'{i}}s and Ridao, Pere and Carreras, Marc and Mallios, Angelos},
doi = {10.1109/TMECH.2011.2174065},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribas et al. - 2012 - Girona 500 AUV From survey to intervention.pdf:pdf},
issn = {10834435},
journal = {IEEE/ASME Transactions on Mechatronics},
keywords = {Autonomous underwater vehicle (AUV),intervention AUV,vehicle design},
month = {feb},
number = {1},
pages = {46--53},
title = {{Girona 500 AUV: From survey to intervention}},
volume = {17},
year = {2012}
}
@techreport{Redmon,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2016 - You only look once Unified, real-time object detection.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
pages = {779--788},
title = {{You only look once: Unified, real-time object detection}},
url = {http://pjreddie.com/yolo/},
volume = {2016-Decem},
year = {2016}
}
@inproceedings{Zoev2019,
abstract = {An important scientific direction is the development and study of computer vision systems (CVS) for mobile robotic complexes. Today, developers of CVS are most often using convolutional neural networks (CNN). For increasing the speed detection of objects on images in CVS, there has been a trend of using CNN, which are hardware-implemented on field-programmable gate array (FPGAs).This article shows that the perspective for hardware implementation on the FPGA is the tiny-YOLO CNN from the YOLO class. For reduce required FPGA computing resources in this CNN, was proposed to use Inception-ResNet modules. We was found that with high detection accuracy of objects in images with minimum resources requirements provide by the tiny-YOLO-Inception-ResNet2 network architecture. It is obtained from replacing the fifth tiny-YOLO convolutional layer of the tiny-YOLO CNN with two sequential processing Inception-ResNet modules. Also results of the study of the detection accuracy of objects using the CNN for this architecture with the lack of resource-intensive operations: batch normalization and bias from calculations were given. These studies were performed for different formats of representation numbers in the FPGA.},
author = {Zoev, Ivan V. and Beresnev, Alexey P. and Markov, Nikolay G.},
booktitle = {2019 International Siberian Conference on Control and Communications, SIBCON 2019 - Proceedings},
doi = {10.1109/SIBCON.2019.8729605},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoev, Beresnev, Markov - 2019 - Convolutional neural networks of the YOLO class in computer vision systems for mobile robotic complexes.pdf:pdf},
isbn = {9781538651414},
keywords = {Computer vision systems,Convolutional neural networks,Field-programmable gate array,Mobile robotic systems,Object detection on images},
month = {apr},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Convolutional neural networks of the YOLO class in computer vision systems for mobile robotic complexes}},
year = {2019}
}
@article{Thotakuri2017,
abstract = {Robotics and Vision demands Artificial Intelligence (AI) techniques to develop devices which are capable of interacting with the physical world. The accuracy of AI devices such as Robot interfacing with physical world purely depends on how effectively the devices perform the operations. The effectiveness of the Robot is sensitive to the vision techniques adopted in the application. So the vision techniques and methodologies play significant role in design of autonomous and self controlled Robots for real life applications. Thus this article presents the review of various Robot vision techniques and methodologies adopted currently for processing images captured by robot vision modules that further enhances the accuracy of Robot. Finally, the survey on image processing methodologies adopted by robot vision is summarized to identify the optimal method for future research on robot vision.},
author = {Thotakuri, Anuradha and Kalyani, T. and Vucha, Mahendra and Chinnaaiah, M. C. and Nagarjuna, T.},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thotakuri et al. - 2017 - Survey on robot vision Techniques, tools and methodologies.pdf:pdf},
issn = {09739769},
journal = {International Journal of Applied Engineering Research},
keywords = {Holonomic,Non-holonomic,Optimal navigation,Path planning,Path tracking,Robot Vision,Visual servoing},
number = {17},
pages = {6887--6896},
title = {{Survey on robot vision: Techniques, tools and methodologies}},
url = {https://www.researchgate.net/publication/319873112{\_}Survey{\_}on{\_}Robot{\_}Vision{\_}Techniques{\_}Tools{\_}and{\_}Methodologies},
volume = {12},
year = {2017}
}
@book{Goodfellow2016,
abstract = {"Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Page 4 of cover. Introduction -- Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models.},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {MIT press},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow, Bengio, Courville - 2016 - Deep Learning.pdf:pdf},
isbn = {9780262035613},
title = {{Deep Learning}},
year = {2016}
}
@incollection{Mou2018,
abstract = {In this chapter, we introduce the general framework of the tree-based convolutional neural network (TBCNN). We first present the design philosophy and the general formula of TBCNN. Then we introduce several applications of TBCNN that will be analyzed in this book. We also highlight the technical difficulties of designing TBCNN in different scenarios, which will be addressed in future chapters.},
author = {Mou, Lili and Jin, Zhi},
booktitle = {SpringerBriefs in Computer Science},
doi = {10.1007/978-981-13-1870-2_3},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mou, Jin - 2018 - General framework of tree-based convolutional neural networks (TBCNNs).pdf:pdf},
issn = {21915776},
keywords = {Convolutional neural network,Tree-based convolution},
pages = {37--40},
publisher = {Springer},
title = {{General framework of tree-based convolutional neural networks (TBCNNs)}},
year = {2018}
}
@inproceedings{Sanz2012,
abstract = {The need for intervention in underwater environments is significantly increasing in the last years. Possible applications include maintenance intervention in permanent observatories and offshore scenarios, and search {\&} recovery for collecting objects of interest for different application domains like biology, fishery, or marine rescue just to name a few. Nowadays, these kind of tasks are usually solved with "work class" ROVs (i.e. Remote Operated Vehicles) that are launched from support vessels, and remotely operated by expert pilots through an umbilical communications cable and complex control interfaces. These solutions present several drawbacks. Firstly, ROVs are normally large and heavy vehicles that need significant logistics for its transportation and handling. Secondly, the complex user interfaces and control methods require skilled pilots for their use. These two facts significantly increase the cost of the applications. Moreover, the need of an umbilical cable introduces additional problems of control, or range limitation. The fatigue and high stress that users of remotely operated systems normally suffer supposes another serious drawback. All the pointed questions justify the need of more autonomous, cheap and easy-to-use solutions for underwater intervention missions, and this is the aim of the current FP7-TRIDENT project. So, in this paper an overview concerning the main research ongoing under this project will be presented and discussed.},
author = {Sanz, Pedro J. and Ridao, Pere and Oliver, Gabriel and Casalino, Giuseppe and Insaurralde, Carlos and Silvestre, Carlos and Melchiorri, Claudio and Turetta, Alessio},
booktitle = {IFAC Proceedings Volumes},
doi = {10.3182/20120410-3-pt-4028.00059},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanz et al. - 2012 - TRIDENT Recent improvements about autonomous underwater intervention missions.pdf:pdf},
isbn = {9783902823199},
issn = {14746670},
keywords = {Autonomous marine vehicles,Dexterous manipulation,Multi-robot cooperation},
number = {PART 1},
pages = {355--360},
title = {{TRIDENT: Recent improvements about autonomous underwater intervention missions}},
volume = {3},
year = {2012}
}
@inproceedings{Evans2003,
abstract = {An Intervention-AUV (or I-AUV), is a hover capable AUV whose primary role is direct contact with subsea structures for measurement or physical manipulation of components. The aim of the ALIVE project is to develop an Intervention-AUV capable of docking to a subsea structure which has not been specifically modified for AUV use. This paper describes the modular structure of the ALIVE AUV, including its distributed software architecture and in particular the ADS (Autonomous Docking System). It then outlines the sonar and video sensor processing techniques used for real-time control of the AUV to perform tracking and 3D pose reconstruction. In addition, details of the system tests and practical trials used in the development process are described.},
author = {Evans, Jonathan and Redmond, Paul and Plakas, Costas and Hamilton, Kelvin and Lane, David},
booktitle = {Oceans 2003: Celebrating the Past... Teaming Toward the Future},
doi = {10.1109/OCEANS.2003.178243},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans et al. - 2003 - Autonomous docking for intervention-AUVs using sonar and video-based real-time 3D pose estimation.pdf:pdf},
isbn = {0933957300},
issn = {01977385},
pages = {2201--2210},
publisher = {IEEE},
title = {{Autonomous docking for intervention-AUVs using sonar and video-based real-time 3D pose estimation}},
url = {http://ieeexplore.ieee.org/document/1282820/},
volume = {4},
year = {2003}
}
@inproceedings{Evans2001b,
abstract = {The SWIMMER Vehicle is a EU-Thermie funded project to develop a prototype autonomous vehicle capable of deploying existing work-class ROVs to subsea installations - removing the need for continuous surface support vessels and long umbilical cables (and the associated costs, power restraints and weather dependencies). To be able to accurately position and dock a large free swimming AUV (total vehicle and payload in-air mass exceeds 2.5 tonnes, and over 6 metres in length), at depths up to 1000m after transits of up to 10km is particularly challenging. The deployment configuration mounts the ROV above the SWIMMER AUV. The ROV is inactive during deployment, and the AUV is completely self-contained in terms of power, thrusters and computer-control. During transit, the LRPS (Long Range Positioning System) a combined acoustic Positioning/Data link provides autonomous navigation to the seabed, together with top station monitoring. During docking, the SRPS (Short Range Positioning System) directs the vehicle during the final 20 metres of descent and provides the necessary positional information for a precise alignment for a mechanical docking with a seabed station. When docked, the SWIMMER uses computer-controlled clamps that allow the ROV to be disengaged remotely from the SWIMMER while on the seabed docking station. In this configuration, power and communications are fed from the seabed network to allow the SWIMMER to recharge, and the ROV to be remotely controlled (completed conventionally by pilot's based, in theory, anywhere in the world).},
author = {Evans, J. C. and Keller, K. M. and Smith, J. S. and Marty, P. and Rigaud, V.},
booktitle = {Oceans Conference Record (IEEE)},
doi = {10.1109/oceans.2001.968776},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans et al. - 2001 - Docking techniques and evaluation trials of the SWIMMER AUV An autonomous deployment AUV for workclass ROVs(2).pdf:pdf},
issn = {01977385},
pages = {520--528},
title = {{Docking techniques and evaluation trials of the SWIMMER AUV: An autonomous deployment AUV for workclass ROVs}},
volume = {1},
year = {2001}
}
@inproceedings{Evans2001,
abstract = {The SWIMMER Vehicle is a EU-Thermie funded project to develop a prototype autonomous vehicle capable of deploying existing work-class ROVs to subsea installations - removing the need for continuous surface support vessels and long umbilical cables (and the associated costs, power restraints and weather dependencies). To be able to accurately position and dock a large free swimming AUV (total vehicle and payload in-air mass exceeds 2.5 tonnes, and over 6 metres in length), at depths up to 1000m after transits of up to 10km is particularly challenging. The deployment configuration mounts the ROV above the SWIMMER AUV. The ROV is inactive during deployment, and the AUV is completely self-contained in terms of power, thrusters and computer-control. During transit, the LRPS (Long Range Positioning System) a combined acoustic Positioning/Data link provides autonomous navigation to the seabed, together with top station monitoring. During docking, the SRPS (Short Range Positioning System) directs the vehicle during the final 20 metres of descent and provides the necessary positional information for a precise alignment for a mechanical docking with a seabed station. When docked, the SWIMMER uses computer-controlled clamps that allow the ROV to be disengaged remotely from the SWIMMER while on the seabed docking station. In this configuration, power and communications are fed from the seabed network to allow the SWIMMER to recharge, and the ROV to be remotely controlled (completed conventionally by pilot's based, in theory, anywhere in the world).},
author = {Evans, J. C. and Keller, K. M. and Smith, J. S. and Marty, P. and Rigaud, V.},
booktitle = {Oceans Conference Record (IEEE)},
doi = {10.1109/oceans.2001.968776},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans et al. - 2001 - Docking techniques and evaluation trials of the SWIMMER AUV An autonomous deployment AUV for workclass ROVs.pdf:pdf},
issn = {01977385},
pages = {520--528},
title = {{Docking techniques and evaluation trials of the SWIMMER AUV: An autonomous deployment AUV for workclass ROVs}},
volume = {1},
year = {2001}
}
@article{Lane1997,
abstract = {AMADEUS is a dexterous subsea robot hand incorporating force and slip contact sensing, using fluid filled tentacles for fingers. Hydraulic pressure variations in each of three flexible tubes (bellows) in each finger create a bending moment, and consequent motion or increase in contact force during grasping. Such fingers have inherent passive compliance, no moving parts, and are naturally depth pressure-compensated, making them ideal for reliable use in the deep ocean. In addition to the mechanical design, development of the hand has also considered closed loop finger position and force control, coordinated finger motion for grasping, force and slip sensor development/signal processing, and reactive world modeling/planning for supervisory 'blind grasping'. Initially, the application focus is for marine science tasks, but broader roles in offshore oil and gas, salvage, and military use are foreseen. Phase I of the project is complete, with the construction of a first prototype. Phase II is now underway, to deploy the hand from an underwater robot arm, and carry out wet trials with users.},
author = {Lane, D. M. and O'Brien, D. J. and Pickett, M. and Davies, J. B.C. and Robinson, G. and Jones, D. and Scott, E. and Casalino, G. and Bartolini, G. and Cannata, G. and Ferrara, A. and Angelleti, D. and Coccoli, M. and Veruggio, G. and Bono, R. and Virgili, P. and Canals, M. and Pallas, R. and Gracia, E. and Smith, C.},
doi = {10.1109/100.637804},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lane et al. - 1997 - AMADEUS Advanced Manipulation for Deep Underwater Sampling.pdf:pdf},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
keywords = {AMADEUS,Robot hands,Undersea robotics},
month = {dec},
number = {4},
pages = {34--45},
title = {{AMADEUS: Advanced Manipulation for Deep Underwater Sampling}},
volume = {4},
year = {1997}
}
@inproceedings{Wang1995,
abstract = {Automated retrieval of instrumentation packages from the sea floor is a task well-suited for autonomous underwater vehicles. By constraining the generic object-manipulation problem to one in which the object is specifically designed to be picked up by a particular robotic system, both sensing and manipulation can be simplified. Readily available vision processing hardware along with well-established vision algorithms can be used to locate the package. The strategy for automated object retrieval by an underwater vehicle is encoded using a finite-state machine (FSM). Each phase of the automatic pickup is represented by a state. As monitors detect the successful completion or failure of a phase, events are issued that trigger the FSM to transition to the next phase of the task or to an alternative, corrective action. These events cause the vehicle control system to change control modes or desired setpoints within the activated controller. The automatic retrieval task will be demonstrated experimentally with the OTTER testbed. These results will be presented at the conference.},
author = {Wang, Howard H. and Rock, Stephen M. and Lee, Michael J.},
booktitle = {Oceans Conference Record (IEEE)},
doi = {10.1109/oceans.1995.526796},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Rock, Lee - 1995 - Experiments in automatic retrieval of underwater objects with an AUV.pdf:pdf},
isbn = {0-933957-14-9},
issn = {01977385},
pages = {366--373},
publisher = {IEEE},
title = {{Experiments in automatic retrieval of underwater objects with an AUV}},
url = {http://ieeexplore.ieee.org/document/526796/},
volume = {2},
year = {1995}
}
@inproceedings{Choi1994,
abstract = {As underwater application activities mature, the utilization of underwater robotic vehicles becomes more notable. The sophistication required by these URVs have significantly increased, and the development of AUVs are becoming imminent. The Autonomous Systems Laboratory of the University of Hawaii has designed the Omni-Directional Intelligent Navigator (ODIN), its control systems, and its graphic workstation to develop an integrated, real-time, 3-dimensional graphic test platform.},
author = {Choi, S. K. and Takashige, G. Y. and Yuh, J.},
booktitle = {IEEE Sympsium on Autonomous Underwater Vehicle Technology},
doi = {10.1109/auv.1994.518610},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Takashige, Yuh - 1994 - Experimental study on an underwater robotic vehicle ODIN.pdf:pdf},
pages = {79--84},
publisher = {IEEE},
title = {{Experimental study on an underwater robotic vehicle: ODIN}},
year = {1994}
}
@inproceedings{Rushfeldt1987,
abstract = {This paper is about the "Thruster Module", a highly advanced ROV designed and built as a component of a Deep-Water-Pipe-Repair-System. The subjects discussed are applicable to large subsea work systems, such as command and control of complex tasks using touch screen technology, precision automatic control systems, and high power hydraulic systems. The results of comprehensive dry and wet tests are di scussed in preparation for deep water sea trials.},
author = {Rushfeldt, P. O. and Conter, A. and McLeod, D. L.},
booktitle = {Proceedings of the Annual Offshore Technology Conference},
doi = {10.4043/5367-ms},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rushfeldt, Conter, McLeod - 1987 - State-of-the-art ROV and control system for deepwater pipe repair.pdf:pdf},
isbn = {9781613990803},
issn = {01603663},
pages = {101--115},
title = {{State-of-the-art ROV and control system for deepwater pipe repair}},
volume = {1987-April},
year = {1987}
}
@article{Schjølberg2015,
abstract = {This paper presents an on-going research project focusing on the development of technology to enable autonomy in ROV operations. The project is a collaborative project between Norwegian offshore industry and academia. Currently, there is a large focus in research on the development of navigation, guidance and control for autonomous underwater vehicles (AUV). This is important as there will be a future demand for subsea inspection, maintenance and repair (IMR) operations with non-cabled systems. A future scenario is to have AUVs stationed on the seafloor in subsea garages. However, state of the art for IMR operations on the Norwegian Continental Shelf is to apply vessel supported ROVs in IMR operations. Efficiency in such operations will imply large cost and time savings. Increased autonomy enables the ROV operator to shift from manual to automatic control utilizing autonomous functions for a number of specific tasks. The research project presented in this paper is novel and the goal is to improve the capabilities of the ROV leaving the operator mainly to supervise operation. The paper discusses different aspects of the technology requirements. This may be useful for researchers working in the area of AUV research, relating this research to industrial needs. The presented project will develop novel integrated sensor platforms with robust perception methods and collision-free motion planning algorithms for subsea inspection and light intervention operations. Moreover, the project will also focus on subsea factory design enabling autonomous operations. The results will be tested, verified and demonstrated in full-scale test beds, as well as at an offshore location.},
author = {Schj{\o}lberg, Ingrid and Utne, Ingrid Bouwer},
doi = {10.1016/j.ifacol.2015.06.030},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schj{\o}lberg, Utne - 2015 - Towards autonomy in ROV operations.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Autonomy,ROV operations,Safety,Subsea},
month = {may},
number = {2},
pages = {183--188},
title = {{Towards autonomy in ROV operations}},
volume = {28},
year = {2015}
}
@article{Capocci2017,
abstract = {This paper presents a review of inspection-class Remotely Operated Vehicles (ROVs). The review divides the classification of inspection-class ROVs; categorising the vehicles in order of size and capability. A state of the art technology review is undertaken, discussing various common subsystems of the ROV. Standard and novel ROV shapes and designs are reviewed, with emphasis on buoyancy, frame materials and hydrodynamics. Several power considerations and designs are discussed, accounting for battery fed and mains fed systems. ROV telemetry is split into a discussion on the various transmission hardware systems and the communication protocols that are most widely used in industry and research today. A range of thruster technologies is then introduced with consideration taken of the various thruster architectures available. Finally, the navigation and positioning sensors employed for ROV navigation and control are reviewed. The author has also created a number of comparison tables throughout the review; tables include comparison of wired data transmission technology, comparison of common ROV communication protocols and comparisons of various inertial navigation systems. By the end of the review the reader will have clearer understanding on the fundamentals of inspection-class ROV technologies and can use this as an introduction to further paper investigation.},
author = {Capocci, Romano and Dooly, Gerard and Omerdi{\'{c}}, Edin and Coleman, Joseph and Newe, Thomas and Toal, Daniel},
doi = {10.3390/jmse5010013},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Capocci et al. - 2017 - Inspection-class remotely operated vehicles-a review.pdf:pdf},
issn = {20771312},
journal = {Journal of Marine Science and Engineering},
keywords = {Marine robotics,ROV,Remotely operated vehicle,Subsea vehicle,Underwater navigation,Underwater robot},
month = {mar},
number = {1},
publisher = {MDPI AG},
title = {{Inspection-class remotely operated vehicles-a review}},
volume = {5},
year = {2017}
}
@article{Macreadie2018,
abstract = {For thousands of years humankind has sought to explore our oceans. Evidence of this early intrigue dates back to 130,000 BCE, but the advent of remotely operated vehicles (ROVs) in the 1950s introduced technology that has had significant impact on ocean exploration. Today, ROVs play a critical role in both military (e.g. retrieving torpedoes and mines) and salvage operations (e.g. locating historic shipwrecks such as the RMS Titanic), and are crucial for oil and gas (O{\&}G) exploration and operations. Industrial ROVs collect millions of observations of our oceans each year, fueling scientific discoveries. Herein, we assembled a group of international ROV experts from both academia and industry to reflect on these discoveries and, more importantly, to identify key questions relating to our oceans that can be supported using industry ROVs. From a long list, we narrowed down to the 10 most important questions in ocean science that we feel can be supported (whole or in part) by increasing access to industry ROVs, and collaborations with the companies that use them. The questions covered opportunity (e.g. what is the resource value of the oceans?) to the impacts of global change (e.g. which marine ecosystems are most sensitive to anthropogenic impact?). Looking ahead, we provide recommendations for how data collected by ROVs can be maximised by higher levels of collaboration between academia and industry, resulting in win-win outcomes. What is clear from this work is that the potential of industrial ROV technology in unravelling the mysteries of our oceans is only just beginning to be realised. This is particularly important as the oceans are subject to increasing impacts from global change and industrial exploitation. The coming decades will represent an important time for scientists to partner with industry that use ROVs in order to make the most of these ‘eyes in the sea'.},
author = {Macreadie, Peter I. and McLean, Dianne L. and Thomson, Paul G. and Partridge, Julian C. and Jones, Daniel O.B. and Gates, Andrew R. and Benfield, Mark C. and Collin, Shaun P. and Booth, David J. and Smith, Luke L. and Techera, Erika and Skropeta, Danielle and Horton, Tammy and Pattiaratchi, Charitha and Bond, Todd and Fowler, Ashley M.},
doi = {10.1016/j.scitotenv.2018.04.049},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Macreadie et al. - 2018 - Eyes in the sea Unlocking the mysteries of the ocean using industrial, remotely operated vehicles (ROVs).pdf:pdf},
issn = {18791026},
journal = {Science of the Total Environment},
keywords = {Biodiversity,Decommissioning,Deep sea,Exploration,Gas,Marine,Oceans,Offshore,Oil,Petroleum,Remotely operated vehicles (ROVs)},
month = {sep},
pages = {1077--1091},
publisher = {Elsevier B.V.},
title = {{Eyes in the sea: Unlocking the mysteries of the ocean using industrial, remotely operated vehicles (ROVs)}},
volume = {634},
year = {2018}
}
@incollection{ChristR.D.andWernli2008,
abstract = {This chapter describes the different types of underwater systems, the basic theory behind vehicle design/communication/propulsion/integration, and explains the means by which a typical ROV gets everyday underwater tasks performed. Underwater vehicles, in the broadest sense, cover manned and unmanned vehicles, with the unmanned vehicles being divided into autonomous underwater vehicles (AUVs) which are nontethered, and remotely operated vehicles (ROVs) which are tethered. The manned versions include submarines and passenger carrying submersibles. The roles of the unmanned vehicles include the use of AUVs by oceanographers to map the features of the ocean and operators such as the oil and gas industry to map the seabed. ROVs are used for many purposes including underwater observation, exploration of the seabed, underwater construction and maintenance of subsea projects and underwater inspection and cleaning of ships' hulls. Simplistically, an ROV is a camera mounted in a waterproof enclosure, with thrusters for maneuvering, attached to a cable to the surface over which a video signal is transmitted. Practically all of today's vehicles use common consumer industry standards for commercial off-the-shelf (COTS) components. The basic issues involved with underwater vehicle power and control can be divided into the following categories: power source for the vehicle, degree of autonomy (operator controlled or program controlled), and communications linkage to the vehicle.},
address = {Oxford, UK},
author = {{Christ, R.D. and Wernli}, R.L. (2007).},
booktitle = {The Maritime Engineering Reference Book},
chapter = {10 Underwa},
doi = {10.1016/b978-0-7506-8987-8.00010-x},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Christ, R.D. and Wernli - 2008 - Underwater vehicles.pdf:pdf},
pages = {728--783},
publisher = {Elsevier},
title = {{Underwater vehicles}},
year = {2008}
}
@inproceedings{Kriangkhajorn2019,
abstract = {this paper presents the implementation of underwater computer vision of the ZEABUS Autonomous Underwater Vehicle (AUV) at Kasetsart University. The purpose of the implementation is to augment previously used algorithm for the 2018 International RoboSub Competition and Singapore AUV 2018 Challenge. The result shows that new algorithm has higher precision level and can achieve better object detection.},
author = {Kriangkhajorn, Supakit and Patchararungruang, Akrapong and Numprasertchai, Somchai},
booktitle = {2019 1st International Symposium on Instrumentation, Control, Artificial Intelligence, and Robotics, ICA-SYMP 2019},
doi = {10.1109/ICA-SYMP.2019.8645995},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kriangkhajorn, Patchararungruang, Numprasertchai - 2019 - Underwater Computer Vision of the ZEABUS AUV.pdf:pdf},
isbn = {9781538677742},
keywords = {autonomous underwater vehicle,computer vision,image processing,underwater computer vision},
month = {feb},
pages = {135--138},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Underwater Computer Vision of the ZEABUS AUV}},
year = {2019}
}
@article{Voulodimos2018,
abstract = {Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.},
author = {Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
doi = {10.1155/2018/7068349},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Voulodimos et al. - 2018 - Deep Learning for Computer Vision A Brief Review.pdf:pdf},
issn = {16875273},
journal = {Computational Intelligence and Neuroscience},
publisher = {Hindawi Limited},
title = {{Deep Learning for Computer Vision: A Brief Review}},
volume = {2018},
year = {2018}
}
@techreport{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern. {\textcopyright} 1980 Springer-Verlag.},
author = {Fukushima, Kunihiko},
booktitle = {Biological Cybernetics},
doi = {10.1007/BF00344251},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fukushima - 1980 - Neocognitron A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in pos.pdf:pdf},
issn = {03401200},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
volume = {36},
year = {1980}
}
@article{Hall2016,
abstract = {Traditional architectures for solving computer vision problems and the degree of success they enjoyed have been heavily reliant on hand-crafted features. However, of late, deep learning techniques have offered a compelling alternative - that of automatically learning problem-specific features. With this new paradigm, every problem in computer vision is now being re-examined from a deep learning perspective. Therefore, it has become important to understand what kind of deep networks are suitable for a given problem. Although general surveys of this fast-moving paradigm (i.e., deep-networks) exist, a survey specific to computer vision is missing. We specifically consider one form of deep networks widely used in computer vision - convolutional neural networks (CNNs). We start with "AlexNet" as our base CNN and then examine the broad variations proposed over time to suit different applications. We hope that our recipe-style survey will serve as a guide, particularly for novice practitioners intending to use deep-learning techniques for computer vision.},
archivePrefix = {arXiv},
arxivId = {1601.06615},
author = {Srinivas, Suraj and Sarvadevabhatla, Ravi Kiran and Mopuri, Konda Reddy and Prabhu, Nikita and Kruthiventi, Srinivas S.S. and Babu, R. Venkatesh},
doi = {10.3389/frobt.2015.00036},
eprint = {1601.06615},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srinivas et al. - 2016 - A taxonomy of deep convolutional neural nets for computer vision.pdf:pdf},
issn = {22969144},
journal = {Frontiers Robotics AI},
keywords = {Convolutional neural networks,Deep learning,Object classification,Recurrent neural networks,Supervised learning},
number = {JAN},
title = {{A taxonomy of deep convolutional neural nets for computer vision}},
url = {www.frontiersin.org},
volume = {2},
year = {2016}
}
@article{Bradski2008,
abstract = {Learning OpenCV puts you right in the middle of the rapidly expanding field of computer vision. Written by the creators of OpenCV, the widely used free open- source library, this book introduces you to computer vision and demonstrates how you can quickly build applications that enable computers to "see" and make decisions based on the data. Computer vision is everywhere - in security systems, manufacturing inspection systems, medical image analysis, Unmanned Aerial Vehicles, and more. It helps robot cars drive by themselves, stitches Google maps and Google Earth together, checks the pixels on your laptop's LCD screen, and makes sure the stitches in your shirt are OK. OpenCV provides an easy-to-use computer vision infrastructure along with a comprehensive library containing more than 500 functions that can run vision code in real time. With Learning OpenCV, any developer or hobbyist can get up and running with the framework quickly, whether it's to build simple or sophisticated vision applications. The book includes: A thorough introduction to OpenCV Getting input from cameras Transforming images Shape matching Pattern recognition, including face detection Segmenting images Tracking and motion in 2 and 3 dimensions Machine learning algorithms Hands-on exercises at the end of each chapter help you absorb the concepts, and an appendix explains how to set up an OpenCV project in Visual Studio. OpenCV is written in performance optimized C/C++ code, runs on Windows, Linux, and Mac OS X, and is free for commercial and research use under a BSD license. Getting machines to see is a challenging but entertaining goal. If you're intrigued by the possibilities, Learning OpenCV gets you started onbuilding computer vision applications of your own.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zelinsky, Alex},
doi = {10.1109/MRA.2009.933612},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zelinsky - 2009 - Learning OpenCV—Computer Vision with the OpenCV Library.pdf:pdf},
isbn = {978-1-4493-1465-1},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {3},
pages = {100},
pmid = {25246403},
title = {{Learning OpenCV—Computer Vision with the OpenCV Library}},
url = {http://shop.oreilly.com/product/0636920022497.do},
volume = {16},
year = {2009}
}
@book{Gollapudi2019,
author = {Gollapudi, Sunila},
booktitle = {Learn Computer Vision Using OpenCV},
doi = {10.1007/978-1-4842-4261-2},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gollapudi - 2019 - Learn Computer Vision Using OpenCV.pdf:pdf},
isbn = {9781484242605},
title = {{Learn Computer Vision Using OpenCV}},
url = {https://doi.org/10.1007/978-1-4842-4261-2},
year = {2019}
}
@book{Ponce2012,
abstract = {The accessible presentation of this book gives both a general view of the entire computer vision enterprise and also offers sufficient detail to be able to build useful applications. Users learn techniques that have proven to be useful by first-hand experience and a wide range of mathematical methods. A CD-ROM with every copy of the text contains source code for programming practice, color images, and illustrative movies. Comprehensive and up-to-date, this book includes essential topics that either reflect practical significance or are of theoretical importance. Topics are discussed in substantial and increasing depth. Application surveys describe numerous important application areas such as image based rendering and digital libraries. Many important algorithms broken down and illustrated in pseudo code. Appropriate for use by engineers as a comprehensive reference to the computer vision enterprise.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Reiner, Elsa},
booktitle = {Chemico-Biological Interactions},
doi = {10.1016/j.cbi.2010.05.017},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reiner - 2010 - Preface.pdf:pdf},
isbn = {9780136085928},
issn = {00092797},
number = {1-3},
pages = {1--2},
pmid = {20595049},
title = {{Preface}},
url = {http://www.inria.fr/centre/paris-rocquencourt/actualites/computer-vision-a-modern-approach},
volume = {187},
year = {2010}
}
@book{Robert2014,
abstract = {This books adopts the view that the best way to solve such problems is to use the tools of probability theory. Probability theory can be applied to any problem involving uncertainty. In machine learning, uncertainty comes in many forms: what is the best prediction about the future given some past data? what is the best model to explain some data? what measurement should I perform next? etc. The probabilistic approach to machine learning is closely related to the field of statistics, but differs slightly in terms of its emphasis and terminology3. We},
author = {Robert, Christian},
booktitle = {Chance},
doi = {10.1080/09332480.2014.914768},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Robert - 2014 - Machine Learning, a Probabilistic Perspective.pdf:pdf},
isbn = {9780262018029},
issn = {0933-2480},
number = {2},
pages = {62--63},
title = {{Machine Learning, a Probabilistic Perspective}},
url = {http://mitpress.mit.edu},
volume = {27},
year = {2014}
}
@misc{Mohammadpoor2019,
abstract = {This paper reviews the utilization of Big Data analytics, as an emerging trend, in the upstream and downstream oil and gas industry. Big Data or Big Data analytics refers to a new technology which can be employed to handle large datasets which include six main characteristics of volume, variety, velocity, veracity, value, and complexity. With the recent advent of data recording sensors in exploration, drilling, and production operations, oil and gas industry has become a massive data intensive industry. Analyzing seismic and micro-seismic data, improving reservoir characterization and simulation, reducing drilling time and increasing drilling safety, optimization of the performance of production pumps, improved petrochemical asset management, improved shipping and transportation, and improved occupational safety are among some of the applications of Big Data in oil and gas industry. Although the oil and gas industry has become more interested in utilizing Big Data analytics recently, but, there are still challenges mainly due to lack of business support and awareness about the Big Data within the industry. Furthermore, quality of the data and understanding the complexity of the problem are also among the challenging parameters facing the application of Big Data.},
author = {Mohammadpoor, Mehdi and Torabi, Farshid},
booktitle = {Petroleum},
doi = {10.1016/j.petlm.2018.11.001},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohammadpoor, Torabi - 2019 - Big Data analytics in oil and gas industry An emerging trend.pdf:pdf},
issn = {24055816},
keywords = {Big Data,Hadoop,Oil and gas industry,R},
title = {{Big Data analytics in oil and gas industry: An emerging trend}},
year = {2019}
}
@article{Schettini2010,
abstract = {The underwater image processing area has received considerable attention within the last decades, showing important achievements. In this paper we review some of the most recent methods that have been specifically developed for the underwater environment. These techniques are capable of extending the range of underwater imaging, improving image contrast and resolution. After considering the basic physics of the light propagation in the water medium, we focus on the different algorithms available in the literature. The conditions for which each of them have been originally developed are highlighted as well as the quality assessment methods used to evaluate their performance. Copyright {\textcopyright} 2010 Sheng-Fu Liang et al.},
author = {Corchs, Silvia and Schettini, Raimondo},
doi = {10.1155/2010/746052},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Corchs, Schettini - 2010 - Underwater image processing State of the art of restoration and image enhancement methods.pdf:pdf},
issn = {16876172},
journal = {Eurasip Journal on Advances in Signal Processing},
pages = {14},
publisher = {Hindawi Publishing Corporation},
title = {{Underwater image processing: State of the art of restoration and image enhancement methods}},
volume = {2010},
year = {2010}
}
@techreport{Solem,
abstract = {If you want a basic understanding of computer vision's underlying theory and algorithms, this hands-on introduction is the ideal place to start. You'll learn techniques for object recognition, {\{}3D{\}} reconstruction, stereo imaging, augmented ...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Jan Erik Solem}},
booktitle = {Programming Computer Vision with Python},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jan Erik Solem - 2012 - Programming Computer Vision with Python.pdf:pdf},
isbn = {1-4493-1653-0},
issn = {1098-6596},
pages = {264},
pmid = {25246403},
title = {{Programming Computer Vision with Python}},
year = {2012}
}
@techreport{Shi2019,
abstract = {Pipeline inspection is a very human intensive task and automation could improve efficiencies significantly. We propose a system that could allow an autonomous underwater vehicle (AUV), to detect pipeline damage in a stream of images. Our classifiers were based on transfer learning from pre-trained convolutional neural networks (CNN). This allows us to achieve good results despite relatively few training examples of damage. We test the approach using data from an actual pipeline inspection.},
author = {Shi, Jiajun and Yin, Wenjie and Du, Yipai and Folkesson, John},
booktitle = {ICRA 2019 Workshop on Underwater Robotics Perception},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - 2019 - Automated Underwater Pipeline Damage Detection using Neural Nets.pdf:pdf},
title = {{Automated Underwater Pipeline Damage Detection using Neural Nets}},
year = {2019}
}
@inproceedings{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.},
author = {Lowe, David G.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/iccv.1999.790410},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe - 1999 - Object recognition from local scale-invariant features.pdf:pdf},
pages = {1150--1157},
title = {{Object recognition from local scale-invariant features}},
volume = {2},
year = {1999}
}
@techreport{Marr,
author = {Marr, David},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marr - 1982 - VISION Tomaso Poggio.pdf:pdf},
isbn = {9780262514620},
keywords = {026228961X,0262514621,9780262289610,9780262514620},
title = {{VISION Tomaso Poggio}},
url = {https://mechanism.ucsd.edu/teaching/f18/David{\_}Marr{\_}Vision{\_}A{\_}Computational{\_}Investigation{\_}into{\_}the{\_}Human{\_}Representation{\_}and{\_}Processing{\_}of{\_}Visual{\_}Information.chapter1.pdf},
year = {1982}
}
@article{Jasper2012,
abstract = {Mechanical pressure clamps are examples of innovative tools commonly used in the oil and gas industry for arresting leaks from damaged oil and gas pipelines. However, if leaks result from pipeline rupture, clamps are not usually recommended. It is therefore obvious that inspection of the leaking pipeline is very crucial in deciding the strategy for repair. For subsea pipelines where underwater poor visibility is pronounced, this important aspect of the pipeline repair process becomes difficult to implement. The result is a repair-leak-repair cycle. This challenge is commonly found in repairs of old pipelines in unclear water conditions. Old pipelines and their vulnerability to fractures that often lead to ruptures are discussed. In this paper, the challenges and technologies available for visualisation and examination in such unclear water conditions are discussed. There appears to be a gap in the existing pipeline integrity management system with respect to inspection and repair of pipelines in unclear water conditions. This gap needs to be filled in order to minimise spills and pollution. For pipelines installed in unclear water condition, a perspective is suggested to extend the capability of existing remotely operated vehicles to employ the use of clear laminar water system or a related technique to provide integrity engineers and operators with close visual assess to inspect leaking pipelines and effect adequate repairs. This paper suggests that the use of optical eye as the main tool for examination remains valuable in managing the challenges in underwater pipeline repairs in unclear water condition.},
author = {Jasper, Agbakwuru},
doi = {10.4236/jep.2012.35049},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jasper - 2012 - OilGas Pipeline Leak Inspection and Repair in Underwater Poor Visibility Conditions Challenges and Perspectives.pdf:pdf},
issn = {2152-2197},
journal = {Journal of Environmental Protection},
keywords = {Close-Visual Inspection,Detection,Leaks,Pipeline,Pipeline Inspection,Pipeline Repair,Spill},
number = {05},
pages = {394--399},
title = {{Oil/Gas Pipeline Leak Inspection and Repair in Underwater Poor Visibility Conditions: Challenges and Perspectives}},
url = {http://dx.doi.org/10.4236/jep.2012.35049PublishedOnlineMay2012 http://www.scirp.org/journal/jep},
volume = {03},
year = {2012}
}
@book{Szeliski2010,
author = {Szeliski, Richard},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szeliski - 2010 - Computer Vision Algorithms and Applications.pdf:pdf},
publisher = {Springer Science $\backslash${\&} Business Media},
title = {{Computer Vision: Algorithms and Applications}},
url = {http://szeliski.org/Book/.},
year = {2010}
}
@incollection{Kisacanin2009,
abstract = {Embedded computer vision is a challenging application domain, requiring high computation rates, high memory bandwidth, and support for a wide range of algorithms. This chapter reviews basic concepts in computer vision, design methodologies for embedded computer vision, platform architectures, and application-specific architectures.},
address = {London},
author = {Wolf, Marilyn},
booktitle = {Handbook of Hardware/Software Codesign},
doi = {10.1007/978-94-017-7267-9_40},
editor = {Kisa{\v{c}}anin, Branislav and Bhattacharyya, Shuvra S. and Chai, Sek},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolf - 2017 - Embedded computer vision.pdf:pdf},
isbn = {9789401772679},
pages = {1339--1352},
publisher = {Springer London},
title = {{Embedded computer vision}},
url = {http://link.springer.com/10.1007/978-1-84800-304-0},
year = {2017}
}
@techreport{Mitchell,
abstract = {Mitchell covers the field of machine learning, the study of algorithms that allow computer programs to automatically improve through experience and that automatically infer general laws from specific data. 1. Introduction -- 2. Concept Learning and the General-to-Specific Ordering -- 3. Decision Tree Learning -- 4. Artificial Neural Networks -- 5. Evaluating Hypotheses -- 6. Bayesian Learning -- 7. Computational Learning Theory -- 8. Instance-Based Learning -- 9. Genetic Algorithms -- 10. Learning Sets of Rules -- 11. Analytical Learning -- 12. Combining Inductive and Analytical Learning -- 13. Reinforcement Learning.},
author = {Boyarshinov, Victor},
booktitle = {Computer},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boyarshinov - 1997 - Machine Learning Machine Learning.pdf:pdf},
isbn = {0071154671},
number = {April},
pages = {414},
title = {{Machine Learning Machine Learning}},
url = {https://books.google.ca/books?id=EoYBngEACAAJ{\&}dq=mitchell+machine+learning+1997{\&}hl=en{\&}sa=X{\&}ved=0ahUKEwiomdqfj8TkAhWGslkKHRCbAtoQ6AEIKjAA},
volume = {2005},
year = {1997}
}
@article{Zhang2012,
abstract = {This paper describes a new framework for detection and tracking of underwater pipeline, which includes software system and hardware system. It is designed for vision system of AUV based on monocular CCD camera. First, the real-time data flow from image capture card is pre-processed and pipeline features are extracted for navigation. The region saturation degree is advanced to remove false edge point group after Sobel operation. An appropriate way is proposed to clear the disturbance around the peak point in the process of Hough transform. Second, the continuity of pipeline layout is taken into account to improve the efficiency of line extraction. Once the line information has been obtained, the reference zone is predicted by Kalman filter. It denotes the possible appearance position of the pipeline in the image. Kalman filter is used to estimate this position in next frame so that the information of pipeline of each frame can be known in advance. Results obtained on real optic vision data in tank experiment are displayed and discussed. They show that the proposed system can detect and track the underwater pipeline online, and is effective and feasible. {\textcopyright} 2012 Chinese Ocean Engineering Society and Springer-Verlag Berlin Heidelberg.},
author = {dong Zhang, Tie and jing Zeng, Wen and Wan, Lei and bai Qin, Zai},
doi = {10.1007/s13344-012-0041-1},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2012 - Vision-based system of AUV for an underwater pipeline tracker.pdf:pdf},
issn = {08905487},
journal = {China Ocean Engineering},
keywords = {AUV navigation,Hough transform,pipeline detection,reference zone},
number = {3},
pages = {547--554},
title = {{Vision-based system of AUV for an underwater pipeline tracker}},
volume = {26},
year = {2012}
}
@inproceedings{Vidal2018,
abstract = {When it is not possible to use remotely operated vehicles (ROVs) or autonomous underwater vehicles (AUVs) with predefined missions to explore complex underwater structures, efficient and safe algorithms for autonomous online exploration are required. In this work we present a robotic exploration algorithm for AUVs which is able to autonomously explore 3D underwater structures. In our proposal, the explored structure must have vertical relief, and the exploration is performed in 2D at a user defined depth. No assumptions are made about the shape of the object, so this makes the algorithm particularly useful to explore unstructured environments. Our approach is able to plan the robot maneuvers to achieve full coverage of the scene with data from two sensors: a scanning profiling sonar, and a camera. The algorithm first incorporates the exteroceptive data from the profiler sonar into a labeled grid map. Then, different candidate viewpoints are generated and the best one is selected according to a metric that balances exploration and trajectory length. Once the best viewpoint has been selected, the robot navigates in the scene to achieve the selected viewpoint configuration. This procedure is repeated until the desired area has been fully explored. To validate our approach, we present simulated and real autonomous explorations of an underwater seamount.},
author = {Vidal, Eduard and Hernandez, Juan David and Palomeras, Narcis and Carreras, Marc},
booktitle = {2018 OCEANS - MTS/IEEE Kobe Techno-Oceans, OCEANS - Kobe 2018},
doi = {10.1109/OCEANSKOBE.2018.8559224},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vidal et al. - 2018 - Online robotic exploration for autonomous underwater vehicles in unstructured environments.pdf:pdf},
isbn = {9781538616543},
month = {dec},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Online robotic exploration for autonomous underwater vehicles in unstructured environments}},
year = {2018}
}
@inproceedings{Sehgal2004,
abstract = {TOUCH is a real-time computer vision system that has been developed in order to aid the operation of autonomous underwater vehicles (AUVs) and remotely operated vehicles (ROVs). The algorithm is based on a modified 4-connectivity approach that scans the incoming frames for a target hue range and based upon the largest connected blobs of hue, tracks an object and provides the two dimensional Cartesian coordinates of the desired object. Since few algorithms exist for tracking and recognizing objects in underwater environments, this algorithm provides a way for A UVs and ROVs to acquire, track or recognize objects by using a relatively fast and inexpensive vision system. This paper presents the details on TOUCH, its testing, some areas that warrant improvement and possible course of action for the future.},
author = {Sehgal, Anuj and Kadarusman, Jason and Fife, Leslie D.},
booktitle = {2004 IEEE Conference on Robotics, Automation and Mechatronics},
doi = {10.1109/ramech.2004.1438963},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sehgal, Kadarusman, Fife - 2004 - TOUCH A robotic vision system for underwater object tracking.pdf:pdf},
isbn = {0780386469},
pages = {455--460},
title = {{TOUCH: A robotic vision system for underwater object tracking}},
year = {2004}
}
@article{Bonin-Font2015,
abstract = {Underwater activities, such as surveying or interventions, carried out by autonomous robots, can benefit greatly from using a vision system. Optics based systems provide information at a spatial and temporal resolution higher than their acoustic counterparts. At present, they are the best option when high precision maneuvering and manipulation is needed, if there is good visibility. This paper presents a new system designed to provide visual information in submarine tasks such as navigation, surveying, mapping and intervention. The main advantages of our system, called Fugu-f (Fugu flexible), are its robustness in both the mechanical structure and the software components, its flexibility, since it is installed as an external module and is adaptable to different vehicles and missions, and its capacity to operate in real-time. Experiments of surveying and object manipulation carried out in real conditions in the context of the TRIDENT project show the suitability of the system and its scientific and industrial potential applications.},
author = {Bonin-Font, Francisco and Oliver, Gabriel and Wirth, Stephan and Massot, Miquel and {Lluis Negre}, Pep and Beltran, Joan Pau},
doi = {10.1016/j.oceaneng.2014.11.005},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonin-Font et al. - 2015 - Visual sensing for autonomous underwater exploration and intervention tasks.pdf:pdf},
issn = {00298018},
journal = {Ocean Engineering},
keywords = {Underwater intervention and exploration,Vision-based navigation,Visual object detection and tracking},
pages = {25--44},
title = {{Visual sensing for autonomous underwater exploration and intervention tasks}},
volume = {93},
year = {2015}
}
@article{Yuh2001,
abstract = {As the ocean attracts great attention on environmental issues and resources as well as scientific and military tasks, the need for and use of underwater robotic systems has become more apparent. Underwater robotics represents a fast growing research area and promising industry as advanced technologies in various subsystems develop and potential application areas are explored. Great efforts have been made in developing autonomous underwater vehicles (AUVs) to overcome challenging scientific and engineering problems caused by the unstructured and hazardous ocean environment. With the development of new materials, advanced computing and sensory technology, as well as theoretical advancements, R {\&} D activities in the AUV community have increased. This paper describes current state-of-the art in the area of underwater robotics focusing on some key subsystems.},
author = {Yuh, J. and West, M.},
doi = {10.1163/156855301317033595},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuh, West - 2001 - Underwater robotics.pdf:pdf},
issn = {01691864},
journal = {Advanced Robotics},
keywords = {Autonomous underwater vehicles,Control,Underwater navigation,Underwater robots},
number = {5},
pages = {609--639},
title = {{Underwater robotics}},
volume = {15},
year = {2001}
}
@article{Patron2008,
abstract = {This paper reviews the applications and challenges of robotic systems in the underwater domain. It focuses on the chal- lenges for achieving embedded situation awareness, adaptive trajectory planning and adaptive mission planning. These are required elements for providing true autonomy for delegation of tasks to unmanned underwater vehicles. The paper analyises current approaches to tackling these challenges and how planning plays a vital role in overcoming them. It includes a description of some key applications and future concepts of operations. Introduction},
address = {Edinburgh},
author = {Patr{\'{o}}n, Pedro and Petillot, Yvan R.},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Patr{\'{o}}n, Petillot - 2008 - The underwater environment A challenge for planning.pdf:pdf},
journal = {UK PlanSIG Edinburgh},
title = {{The underwater environment: A challenge for planning}},
url = {http://pastis2.eps.hw.ac.uk/OSLwiki/images/e/e5/2008{\_}12{\_}PlanSIG{\_}PPatron.pdf},
year = {2008}
}
@inproceedings{Zubair2016,
abstract = {Robotic manipulator, which is a kind of robot that interacts with its environment in order to manipulate objects, is the most extensively-employed kind of robot in practical field. Most of the robotic manipulators functioning in the industries and other fields are programmed to manipulate objects from fixed predefined points in space. However, this might create problems in real world situation where the location of the object is arbitrary and cannot be predefined. This problem can be solved if the robot is able to recognize the object by learning which one is the specific object and which one is not. The robot can then find the position where the object is located in order to drive the manipulator tool point towards that position. In this paper, an approach has been taken to design and analyze performance of a humanlike robotic vision system for robotic manipulator using artificial neural network along with image processing. Here, the robot captures image of the environment, analyzes it and determines the points in space where the object to be manipulated is locate.},
author = {Zubair, Kazi Abu and Singha, Subha and Alam, Sabrina and Ahmmed, Kazi Tanvir},
booktitle = {2nd International Conference on Electrical Information and Communication Technologies, EICT 2015},
doi = {10.1109/EICT.2015.7391914},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zubair et al. - 2016 - Humanlike robotic vision system using artificial neural network.pdf:pdf},
isbn = {9781467392570},
keywords = {Artificial Intelligence,Artificial Neural Network,Image Processing,Object Recognition,Robotics},
month = {jan},
pages = {13--18},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Humanlike robotic vision system using artificial neural network}},
year = {2016}
}
@article{Simetti2018,
abstract = {This paper presents a unifying task priority control architecture for underwater vehicle manipulator systems. The proposed control framework can be applied to different operative scenarios such as waypoint navigation, assisted teleoperation, interaction, landing and grasping. This work extends the results of the TRIDENT and MARIS projects, which were limited to the execution of grasping actions, to other applications taken from the DexROV and ROBUST projects. In particular, simulation results show how the control framework can be used, for example, for pipeline inspection scenarios and deep sea mining exploration.},
author = {Simetti, E. and Casalino, G. and Wanderlingh, F. and Aicardi, M.},
doi = {10.1016/j.oceaneng.2018.06.026},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simetti et al. - 2018 - Task priority control of underwater intervention systems Theory and applications.pdf:pdf},
issn = {00298018},
journal = {Ocean Engineering},
keywords = {Intervention autonomous underwater vehicles,Kinematic control,Remotely Operated Vehicles,Task priority control,Underwater Vehicle Manipulator Systems},
month = {sep},
pages = {40--54},
publisher = {Elsevier Ltd},
title = {{Task priority control of underwater intervention systems: Theory and applications}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0029801818310503},
volume = {164},
year = {2018}
}
@article{Li2005,
abstract = {This paper presents a neural network adaptive controller for diving control of an autonomous underwater vehicle (AUV). In general, while deriving the diving equations of an AUV, the pitch angle of the vehicle is often assumed to be small in the diving motion. This is a somewhat strong restricting condition in many practical applications, and would be broken in this paper. Furthermore, because the dynamics of AUVs are highly nonlinear and the hydrodynamic coefficients of the vehicles are difficult to be accurately estimated a priori, the smooth unknown dynamics in the pitch motion of an AUV is approximated by a neural network, and the remaining unstructured uncertainties, such as disturbances and unmodeled dynamics, are assumed to be unbounded, although they still satisfy certain growth conditions. Under a certain relaxed assumptions on the control gain functions, proposed control scheme can guarantee that all the signals in the closed-loop system satisfy to be uniformly ultimately bounded (UUB). Simulation studies are included to illustrate the effectiveness of the proposed control scheme, and some practical features of the control laws are also discussed. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Li, Ji Hong and Lee, Pan Mook},
doi = {10.1016/j.robot.2005.04.004},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Lee - 2005 - A neural network adaptive controller design for free-pitch-angle diving behavior of an autonomous underwater vehicle.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {AUV,Backstepping,Neural networks,Nonlinear uncertain systems,Unstructured uncertainty},
number = {2-3},
pages = {132--147},
title = {{A neural network adaptive controller design for free-pitch-angle diving behavior of an autonomous underwater vehicle}},
volume = {52},
year = {2005}
}
@article{Katzschmann2018,
abstract = {Closeup  exploration  of  underwater  life  requires  new  forms  of  interaction,  using  biomimetic  creatures  that  are  capable of agile swimming maneuvers, equipped with cameras, and supported by remote human operation. Cur-rent robotic prototypes do not provide adequate platforms for studying marine life in their natural habitats. This work presents the design, fabrication, control, and oceanic testing of a soft robotic fish that can swim in three di-mensions to continuously record the aquatic life it is following or engaging. Using a miniaturized acoustic com-munication module, a diver can direct the fish by sending commands such as speed, turning angle, and dynamic vertical diving. This work builds on previous generations of robotic fish that were restricted to one plane in shallow water and lacked remote control. Experimental results gathered from tests along coral reefs in the Pacific Ocean show  that  the  robotic  fish  can  successfully  navigate  around  aquatic  life  at  depths  ranging  from  0  to  18  meters.  Furthermore, our robotic fish exhibits a lifelike undulating tail motion enabled by a soft robotic actuator design that can potentially facilitate a more natural integration into the ocean environment. We believe that our study advances beyond what is currently achievable using traditional thruster-based and tethered autonomous under-water vehicles, demonstrating methods that can be used in the future for studying the interactions of aquatic life and ocean dynamics.},
author = {Katzschmann, Robert K. and DelPreto, Joseph and MacCurdy, Robert and Rus, Daniela},
doi = {10.1126/scirobotics.aar3449},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Katzschmann et al. - 2018 - Exploration of underwater life with an acoustically controlled soft robotic fish.pdf:pdf},
issn = {2470-9476},
journal = {Science Robotics},
month = {mar},
number = {16},
pages = {eaar3449},
publisher = {American Association for the Advancement of Science (AAAS)},
title = {{Exploration of underwater life with an acoustically controlled soft robotic fish}},
volume = {3},
year = {2018}
}
@book{Kimura2013,
abstract = {Heavy disasters of oil spill bring enormous damage on the ocean environment as well as regional economies. Once gas blows out from seabed by an accident of subsea oil production system or by a seismic activity, it seriously damages not only ships and airplanes, but also natural environment. To know when, where and how much the spilled oil and gas float up on the sea surface, and where the floating oil on the sea surface drift ashore, we need information of advection, diffusion and dispersion of underwater oil and gas, and its prediction. This paper describes the guidance control simulation for designing the underwater robot SOTAB-I (SOTAB; Spilled Oil Tracking Autonomous Buoy) that tracks spilled oil autonomously and gathers oceanographic data. Numerical simulation was adopted to estimate the maneuverability of this robot. To compute the hydrodynamic derivatives of the robot, USAF DATCOM Method and CFD were used. Copyright {\textcopyright} 2013 by the International Society of Offshore and Polar Engineers (ISOPE).},
author = {Kimura, R. and Choyekh, M. and Kato, N. and Senga, H. and Suzuki, H. and Ukita, M. and Kamezuka, K.},
booktitle = {Proceedings of the International Offshore and Polar Engineering Conference},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kimura et al. - 2013 - Guidance and control of an autonomous underwater robot for tracking and monitoring spilled plumes of oil and gas.pdf:pdf},
isbn = {9781880653999},
issn = {10986189},
keywords = {Adcp,Cfd,Datcom,Guidance control,Sotab,Spilled oil,Ums},
pages = {366--371},
title = {{Guidance and control of an autonomous underwater robot for tracking and monitoring spilled plumes of oil and gas from seabed}},
url = {www.isope.org},
year = {2013}
}
@article{Jacobi2015,
abstract = {In the future our resource on land will be over exploited. The exploration of new resources in the ocean is in progress. Mining will be done on the bottom of the sea. The sea is also a big source of renewable energy. Off shore wind parks and tide plants are built. Also, the major world trade is handled over sea routes and several big harbors. All this maritime facilities are getting older and there are effects like corrosion or malfunctions. In general, they need to be inspected frequently. For deep sea applications, security reasons and cost reduction autonomous underwater vehicles (AUVs) will be the first choice. The project "CView" addresses one of these inspection problems, the harbor inspection. But the algorithms, we present in this article can be adapted to many other inspection tasks. One of the main goals in this project is to find cracks or damaged areas at the underwater buildings or to observe critical sections under water with cost effective methods. The platform for developing the guidance algorithms for inspection is the AUV "SeaCat". This underwater vehicle has a control software system with an user interface for mission planning, a mission control system, a precise navigation system, optimized motor control with an autopilot and sensors for obstacle detection and inspection. For obstacle and inspection target detection, a scanning sonar is used. The sonar images are automatically processed with edge detection and line extraction algorithms to get a simplified environment description, which is used by the guidance methods presented in this article. A pan-tilt enabled sensor head with camera, laser measurement and MBES (Multi Beam Echo Sounder) is used to inspect the detected objects. Additionally, these sensors provide distance information to the inspection object which can be used by the inspection guidance. This article presents methods for inspection using the online information from the vehicle sensors to guide the vehicle efficient and safely. It is also important to handle the interaction between mission planning and execution. During mission planning, the operator will define the type of the inspection object (wall, vessel, sluice, etc.). The algorithms we develop use the information from the mission planning and the online data from the vehicle sensors to guide the vehicle to get the optimal inspection results. Therefore, a precise distance control to the inspection object, collision avoidance and object recognition are needed.},
author = {Jacobi, Marco},
doi = {10.1016/j.robot.2014.10.006},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacobi - 2015 - Autonomous inspection of underwater structures.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {AUV,Autonomous,Guidance,Inspection,Underwater},
month = {may},
pages = {80--86},
publisher = {Elsevier},
title = {{Autonomous inspection of underwater structures}},
volume = {67},
year = {2015}
}
@article{Khan2018,
abstract = {Computer vision has become increasingly important and effective in recent years due to its wide-ranging applications in areas as diverse as smart surveillance and monitoring, health and medicine, sports and recreation, robotics, drones, and self-driving cars. Visual recognition tasks, such as image classification, localization, and detection, are the core building blocks of many of these applications, and recent developments in Convolutional Neural Networks (CNNs) have led to outstanding performance in these state-of-the-art visual recognition tasks and systems. As a result, CNNs now form the crux of deep learning algorithms in computer vision. is self-contained guide will benefit those who seek to both understand the theory be- hind CNNs and to gain hands-on experience on the application of CNNs in computer vision. It provides a comprehensive introduction to CNNs starting with the essential concepts behind neural networks: training, regularization, and optimization of CNNs. e book also discusses a wide range of loss functions, network layers, and popular CNN architectures, reviews the differ- ent techniques for the evaluation of CNNs, and presents some popular CNN tools and libraries that are commonly used in computer vision. Further, this text describes and discusses case stud- ies that are related to the application of CNN in computer vision, including image classification, object detection, semantic segmentation, scene understanding, and image generation. is book is ideal for undergraduate and graduate students, as no prior background knowl- edge in the field is required to follow the material, as well as new researchers, developers, engi- neers, and practitioners who are interested in gaining a quick understanding of CNN models.},
author = {Khan, Salman and Rahmani, Hossein and Shah, Syed Afaq Ali and Bennamoun, Mohammed},
doi = {10.2200/s00822ed1v01y201712cov015},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khan et al. - 2018 - A Guide to Convolutional Neural Networks for Computer Vision.pdf:pdf},
issn = {2153-1056},
journal = {Synthesis Lectures on Computer Vision},
month = {feb},
number = {1},
pages = {1--207},
publisher = {Morgan {\&} Claypool Publishers LLC},
title = {{A Guide to Convolutional Neural Networks for Computer Vision}},
volume = {8},
year = {2018}
}
@article{Hong2019,
abstract = {Underwater visual inspection is an important task for checking the structural integrity and biofouling of the ship hull surface to improve the operational safety and efficiency of ships and floating vessels. This paper describes the development of an autonomous in-water visual inspection system and its application to visual hull inspection of a full-scale ship. The developed system includes a hardware vehicle platform and software algorithms for autonomous operation of the vehicle. The algorithms for vehicle autonomy consist of the guidance, navigation, and control algorithms for real-time and onboard operation of the vehicle around the hull surface. The environmental perception of the developed system is mainly based on optical camera images, and various computer vision and optimization algorithms are used for vision-based navigation and visual mapping. In particular, a stereo camera is installed on the underwater vehicle to estimate instantaneous surface normal vectors, which enables high-precision navigation and robust visual mapping, not only on flat areas but also over moderately curved hull surface areas. The development process of the vehicle platform and the implemented algorithms are described. The results of the field experiment with a full-scale ship in a real sea environment are presented to demonstrate the feasibility and practical performance of the developed system.},
author = {Hong, Seonghun and Chung, Dongha and Kim, Jinwhan and Kim, Youngji and Kim, Ayoung and Yoon, Hyeon Kyu},
doi = {10.1002/rob.21841},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hong et al. - 2019 - In-water visual ship hull inspection using a hover-capable underwater vehicle with stereo vision.pdf:pdf},
issn = {15564967},
journal = {Journal of Field Robotics},
keywords = {autonomous underwater vehicles,computer vision,underwater navigation,underwater robotics,visual inspection},
month = {may},
number = {3},
pages = {531--546},
publisher = {John Wiley and Sons Inc.},
title = {{In-water visual ship hull inspection using a hover-capable underwater vehicle with stereo vision}},
volume = {36},
year = {2019}
}
@article{Sahoo2019,
abstract = {Autonomous Underwater Vehicles (AUVs) are robotic devices with a propulsion system for navigation and an onboard computer for decision making. AUV research is gaining popularity because of its extensive applications in fields from military to science. Robotic systems are need of the hour for exploration and environmental safety of the vast and deep oceans and water bodies. This paper presents current research trends in the field of AUVs and highlights future research directions. Here localization and navigation techniques such as inertial navigation to simultaneous localization and mapping being used in current AUVs are discussed in detail. Different optimal path planning and control methods are highlighted. Use of different sensor technology like sonar, laser, acoustic modems and stereo vision systems for localization, navigation and mapping is presented. Recent developments in underwater wireless communication along with the commercially available devices are discussed.},
author = {Sahoo, Avilash and Dwivedy, Santosha K. and Robi, P. S.},
doi = {10.1016/j.oceaneng.2019.04.011},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sahoo, Dwivedy, Robi - 2019 - Advancements in the field of autonomous underwater vehicle.pdf:pdf},
issn = {00298018},
journal = {Ocean Engineering},
keywords = {Autonomous underwater vehicles (AUVs),Control,Inertial navigation,Localization,Mapping,Underwater wireless communication},
pages = {145--160},
title = {{Advancements in the field of autonomous underwater vehicle}},
volume = {181},
year = {2019}
}
@inproceedings{Fowler2016,
abstract = {A team of undergraduate computer science students conducted research to create an economical autonomous underwater vehicle (AUV). The inspiration originated from the high cost associated with this branch of robotics. These vehicles are used in multiple fields including the exploration of large bodies of water and underwater equipment maintenance. These self-sufficient robots are able to keep individuals out of harsh and extreme environments. Our proposed version of an AUV is unique and it can function fully without being physically attached to the surface. The AUVs currently available in the market cost at least ten thousand dollars. Our AUV, as is, has the structure and capability to function on a small scale for just 400 dollars. Our design can be scaled up for more accuracy and longer battery life. This slightly scaled up version could drive the price of our AUV design to two thousand dollars compared to tens of thousands of dollars. This would make it more feasible for companies to have fleets of these underwater robots for a multitude of uses. Another aspect of this research would be creating an entirely new market for hobbyists who would like to send robots underwater for photography, amateur film making, or a number of things that only a consumer driven market can create. From a technical standpoint, this project required thinking up simple solutions to complex problems to keep the cost of the device down. It required the waterproofing of cheap water resistant sensors. It also required thinking outside of the box on parts and equipment that are waterproof but that are not used in robotics. For example, using modified bilge pumps as motors saved money when comparing them to waterproof servos.},
author = {Fowler, Michael C. and Bolding, Terianne L. and Hebert, Kyle M. and Ducrest, Frank and Kumar, Ashok},
booktitle = {10th Annual International Systems Conference, SysCon 2016 - Proceedings},
doi = {10.1109/SYSCON.2016.7490543},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fowler et al. - 2016 - Design of a cost-effective autonomous underwater vehicle.pdf:pdf},
isbn = {9781467395182},
keywords = {AI,Artificial Intelligence,Autonomous,Cost Effective,Educational,Research,Robot,Robotics,Undergraduate,Underwater,Vehicle,Waterproof},
month = {jun},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Design of a cost-effective autonomous underwater vehicle}},
year = {2016}
}
@article{Vinas2017,
author = {Vinas, Arnau},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vinas - 2017 - Robot Learning applied to Autonomous Underwater Vehicles for intervention tasks.pdf:pdf},
journal = {TDX (Tesis Doctorals en Xarxa)},
keywords = {68 - Ind{\'{u}}stries,Aprendizaje de robots,Aprendizaje por demostraci{\'{o}}n,Aprenentatge de robots,Aprenentatge per demostraci{\'{o}},Artificial intelligence,Autonomous underwater vehicle,Inteligencia artificial,Intel{\textperiodcentered}lig{\`{e}}ncia artificial,Intervenciones submarinas,Intervencions submarines,Learning by demonstration,Robot learning,Robotics,Rob{\`{o}}tica,Rob{\'{o}}tica,Underwater intervention task,Vehicles aut{\`{o}}noms submarins,Veh{\'{i}}culos aut{\'{o}}nomos submarinos,oficis i comer{\c{c}} d'articles acabats. Tecnologia cib},
title = {{Robot Learning applied to Autonomous Underwater Vehicles for intervention tasks}},
url = {http://hdl.handle.net/10803/450868{\%}0Ahttp://www.tdx.cat/handle/10803/450868},
year = {2017}
}
@inproceedings{DiyaDong2015,
abstract = {Most existing path planning algorithms focus on generating piecewise linear functions, while the smoothness of the path is usually ignored. Route stability and energy saving are key issues for an autonomous underwater vehicle (AUV) working in the underwater environment. To ensure that AUV can pass through the target area steadily and safely, a path smoother is required. This paper puts forward a novel path planning method based on extreme learning machine (ELM), which generates a smooth and safe path at a quite fast speed. First a Voronoi preprocessor is employed to label the obstacles in the map and generate a rough path connecting the initial and the goal positions. Due to the property of nonlinear separating surface as well as fast learning speed, ELM is then applied to regenerate and smooth the path to ensure that the vehicle drives automatically and safely. Related experiments also validate the performance of our proposed method as expected. More analysis and some possible limitations are also discussed.},
author = {Dong, Diya and He, Bo and Liu, Yang and Nian, Rui and Yan, Tianhong},
booktitle = {OCEANS 2015 - MTS/IEEE Washington},
doi = {10.23919/oceans.2015.7401951},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong et al. - 2016 - A novel path planning method based on extreme learning machine for autonomous underwater vehicle.pdf:pdf},
isbn = {9780933957435},
keywords = {autonomous underwater vehicle,extreme learning machine,path planning,smooth},
month = {oct},
pages = {1--7},
publisher = {IEEE},
title = {{A novel path planning method based on extreme learning machine for autonomous underwater vehicle}},
url = {http://ieeexplore.ieee.org/document/7401951/},
year = {2016}
}
@techreport{Gaskett,
abstract = {The field of nanotechnology may hold the promise of significant improvements in the health and well being of patients, as well as in manufacturing technologies. The knowledge of this impact of nanomaterials on public health is limited so far. This paper briefly reviews the unique size-controlled properties of nanomaterials, their disposition in the body after inhalation, and the factors influencing the fate of inhaled nanomaterials. The physiology of the lung makes it an ideal target organ for non-invasive local and systemic drug delivery, especially for protein and poorly water-soluble drugs that have low oral bioavailability via oral administration. The potential application of pulmonary drug delivery of nanoparticles to the lungs, specifically in context of published results reported on nanomaterials in environmental epidemiology and toxicology is reviewed in this paper.},
author = {Gaskett, Chris and Wettergreen, David and Zelinsky, Alexander},
booktitle = {Proc. of the Australian Conference on Robotics and Automation},
doi = {10.1016/j.ijpharm.2008.02.011},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gaskett, Wettergreen, Zelinsky - 1999 - Reinforcement Learning applied to the control of an Autonomous Underwater Vehicle.pdf:pdf},
isbn = {0378-5173},
issn = {0378-5173},
number = {2},
pages = {125--131},
pmid = {18358652},
title = {{Reinforcement Learning applied to the control of an Autonomous Underwater Vehicle}},
volume = {30},
year = {1999}
}
@inproceedings{Vibhute2018,
abstract = {In this paper, Adaptive Dynamic Programming (ADP) technique is utilized to achieve optimal motion control of Autonomous Underwater Vehicle (AUV) System. The paper proposes a model-free based method that takes into consideration the actuator input and obstacle position while tracing an optimal path. The concept of machine learning enables to develop a path-planner which aims to avoid collisions with static obstacles. The ADP approach is realized to approximate the solution of the cost functional for optimization purpose by which the positions of the locally situated obstacles need not be priori-known until they are within a designed approximation safety envelope. The methodology is implemented to achieve the path-planning objective using dynamic programming technique. The Least-squares policy method serves as a recursive algorithm to approximate the value function for the domain, providing an approach for the finite space discrete control system. The concept behind the design of an obstacle-free path finder is to generate an optimal action that minimizes the local cost, defined by a functional, under constrained optimization. The most advantageous value function is described by the Hamilton Jacobi Bellman (HJB) equation, that is impractical to solve using analytical methods. To overcome the complex calculations subject to HJB, a method based on Reinforcement Learning (RL), called ADP is implemented. This paper outlines the concept of machine learning to realize a real time obstacle avoidance system.},
author = {Vibhute, Siddhant},
booktitle = {2018 5th International Conference on Control, Decision and Information Technologies, CoDIT 2018},
doi = {10.1109/CoDIT.2018.8394934},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vibhute - 2018 - Adaptive Dynamic Programming Based Motion Control of Autonomous Underwater Vehicles.pdf:pdf},
isbn = {9781538650653},
month = {jun},
pages = {966--971},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Adaptive Dynamic Programming Based Motion Control of Autonomous Underwater Vehicles}},
year = {2018}
}
@article{Mi2014,
abstract = {The autonomous underwater vehicle (AUV) and the problems associated with its safe navigation have been studied for the last two decades. The real-time underwater obstacle recognition procedure still has many complications associated with it and the issue becomes worse with vague sensor data. These problems can be coped with the merger of a robust classification mechanism and a domain knowledge acquisition technique. In this paper, we introduce a hybrid mechanism to recognize underwater obstacles for AUV based on fuzzy domain ontology and support vector machine (SVM). SVM is an efficient algorithm that was developed for recognizing 3D object in recent years and is a new generation learning system based on recent advances in statistical learning theory. The amalgamation of fuzzy domain ontology with SVM boosts the performance of the obstacle recognition module by providing the timely semantic domain information of the surrounding circumstances. Also the reasoning ability of the fuzzy domain ontology can expedite the obstacle avoidance process. In order to evaluate the performance of the system, we developed a prototype simulator based on OpenGL and VC++. We compared the outcomes of our proposed technique with backpropagation algorithm and classic SVM based techniques.},
author = {Mi, Zhen Shu and Bukhari, Ahmad C. and Kim, Yong Gi},
doi = {10.1155/2014/676729},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mi, Bukhari, Kim - 2014 - An Obstacle Recognizing Mechanism for Autonomous Underwater Vehicles Powered by Fuzzy Domain Ontology and Supp.pdf:pdf},
issn = {15635147},
journal = {Mathematical Problems in Engineering},
publisher = {Hindawi Publishing Corporation},
title = {{An Obstacle Recognizing Mechanism for Autonomous Underwater Vehicles Powered by Fuzzy Domain Ontology and Support Vector Machine}},
volume = {2014},
year = {2014}
}
@inproceedings{Wehbe2017,
abstract = {This work addresses a data driven approach which employs a machine learning technique known as Support Vector Regression (SVR), to identify the coupled dynamical model of an autonomous underwater vehicle. To train the regressor, we use a dataset collected from the robot's on-board navigation sensors and actuators. To achieve a better fit to the experimental data, a variant of a radial-basis-function kernel is used in combination with the SVR which accounts for the different complexities of each of the contributing input features of the model. We compare our method to other explicit hydrodynamic damping models that were identified using the total least squares method and with less complex SVR methods. To analyze the transferability, we clearly separate training and testing data obtained in real-world experiments. Our presented method shows much better results especially compared to classical approaches.},
author = {Wehbe, Bilal and Krell, Mario Michael},
booktitle = {OCEANS 2017 - Aberdeen},
doi = {10.1109/OCEANSE.2017.8084596},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wehbe, Krell - 2017 - Learning coupled dynamic models of underwater vehicles using Support Vector Regression.pdf:pdf},
isbn = {9781509052783},
month = {oct},
pages = {1--7},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Learning coupled dynamic models of underwater vehicles using Support Vector Regression}},
volume = {2017-Octob},
year = {2017}
}
@article{Forouzantabar2012,
author = {Forouzantabar, Ahmad and Gholami, Babak and Azadi, Mohammad},
doi = {10.5281/ZENODO.1062880},
journal = {World Academy of Science, Engineering and Technology},
keywords = {Autonomous Underwater Vehicle (AUV),Composite Adaptation.,Neural
Network Controller},
month = {jul},
number = {7},
pages = {304--309},
title = {{Adaptive Neural Network Control of Autonomous Underwater Vehicles}},
volume = {6},
year = {2012}
}
@article{Borlaug2018,
abstract = {The articulated intervention autonomous underwater vehicle (AIAUV) is a slender, multi-articulated, underwater robot that is equipped with thrusters, i.e. an underwater swimming manipulator (USM). For the AIAUV to be able to move in confined spaces and perform intervention tasks, it is essential to achieve good station-keeping and trajectory tracking performance for the AIAUV by using the thrusters and by using the joints to attain the desired position and orientation of the vehicle. In this paper, we propose a sliding mode control (SMC) law, specifically the super-twisting algorithm with adaptive gains, for the trajectory tracking of the joints angles and position and orientation of the base of the AIAUV, and we consider this tracking problem of the AIAUV in 6 DOF. A higher-order sliding mode observer is proposed for state estimation. Furthermore, we show the ultimate boundedness of the tracking errors, and we demonstrate the applicability of the proposed control law with simulations.},
author = {Borlaug, I. L.G. and Pettersen, K. Y. and Gravdahl, J. T.},
doi = {10.1016/j.ifacol.2018.09.506},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Borlaug, Pettersen, Gravdahl - 2018 - Trajectory tracking for an articulated intervention AUV using a super-twisting algorithm in 6 DOF.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Autonomous Underwater Vehicle,Siding Mode Control,Sliding Mode Observer,Super-Twisting,Underwater Swimming Manipulator},
month = {jan},
number = {29},
pages = {311--316},
publisher = {Elsevier},
title = {{Trajectory tracking for an articulated intervention AUV using a super-twisting algorithm in 6 DOF ⁎}},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318321955},
volume = {51},
year = {2018}
}
@inproceedings{Gaskett1999,
abstract = {The field of nanotechnology may hold the promise of significant improvements in the health and well being of patients, as well as in manufacturing technologies. The knowledge of this impact of nanomaterials on public health is limited so far. This paper briefly reviews the unique size-controlled properties of nanomaterials, their disposition in the body after inhalation, and the factors influencing the fate of inhaled nanomaterials. The physiology of the lung makes it an ideal target organ for non-invasive local and systemic drug delivery, especially for protein and poorly water-soluble drugs that have low oral bioavailability via oral administration. The potential application of pulmonary drug delivery of nanoparticles to the lungs, specifically in context of published results reported on nanomaterials in environmental epidemiology and toxicology is reviewed in this paper. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Yang, Wei and Peters, Jay I. and Williams, Robert O.},
booktitle = {International Journal of Pharmaceutics},
doi = {10.1016/j.ijpharm.2008.02.011},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Peters, Williams - 2008 - Inhaled nanoparticles-A current review.pdf:pdf},
isbn = {0378-5173},
issn = {03785173},
keywords = {Nanoparticle,Nanotechnology,Nanotoxicology,Poorly water-soluble drug,Protein/peptide drug,Pulmonary delivery},
number = {1-2},
pages = {239--247},
pmid = {18358652},
title = {{Inhaled nanoparticles-A current review}},
volume = {356},
year = {2008}
}
@article{Palomeras2016,
abstract = {The use of commercially available autonomous underwater vehicles (AUVs) has increased during the last fifteen years. While they are mainly used for routine survey missions, there is a set of applications that nowadays can be only addressed by manned submersibles or work-class remotely operated vehicles (ROVs) equipped with teleoperated arms: the intervention applications. To allow these heavy vehicles controlled by human operators to perform intervention tasks, underwater structures like observatory facilities, subsea panels or oil-well Christmas trees have been adapted, making them more robust and easier to operate. The TRITON Spanish founded project proposes the use of a light-weight intervention AUV (I-AUV) to carry out intervention applications simplifying the adaptation of these underwater structures and drastically reducing the operational cost. To prove this concept, the Girona 500 I-AUV is used to autonomously dock into an adapted subsea panel and once docked perform an intervention composed of turning a valve and plugging in/unplugging a connector. The techniques used for the autonomous docking and manipulation as well as the design of an adapted subsea panel with a funnel-based docking system are presented in this article together with the results achieved in a water tank and at sea.},
author = {Palomeras, Narc{\'{i}}s and Pe{\~{n}}alver, Antonio and Massot-Campos, Miquel and Negre, Pep Llu{\'{i}}s and Fern{\'{a}}ndez, Jos{\'{e}} Javier and Ridao, Pere and Sanz, Pedro J. and Oliver-Codina, Gabriel},
doi = {10.3390/s16101673},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Autonomous underwater vehicles,Field robotics,Manipulation,Underwater intervention},
number = {10},
pages = {1673},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{I-AUV docking and panel intervention at sea}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5087461/pdf/sensors-16-01673.pdf},
volume = {16},
year = {2016}
}
@article{Casalino2016,
abstract = {The Italian national project MARIS (Marine Robotics for Interventions) pursues the strategic objective of studying, developing, and integrating technologies and methodologies to enable the development of autonomous underwater robotic systems employable for intervention activities. These activities are becoming progressively more typical for the underwater offshore industry, for search-and-rescue operations, and for underwater scientific missions. Within such an ambitious objective, the project consortium also intends to demonstrate the achievable operational capabilities at a proof-of-concept level by integrating the results with prototype experimental systems.},
author = {Casalino, Giuseppe and Caccia, Massimo and Caselli, Stefano and Melchiorri, Claudio and Antonelli, Gianluca and Caiti, Andrea and Indiveri, Giovanni and Cannata, Giorgio and Simetti, Enrico and Torelli, Sandro and Sperind{\`{e}}, Alessandro and Wanderlingh, Francesco and Muscolo, Giovanni and Bibuli, Marco and Bruzzone, Gabriele and Zereik, Enrica and Odetti, Angelo and Spirandelli, Edoardo and Ranieri, Andrea and Aleotti, Jacopo and Rizzini, Dario Lodi and Oleari, Fabio and Kallasi, Fabjan and Palli, Gianluca and Scarcia, Umberto and Moriello, Lorenzo and Cataldi, Elisabetta},
doi = {10.4031/MTSJ.50.4.7},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Casalino et al. - 2016 - Underwater intervention robotics An outline of the Italian national project Maris.pdf:pdf},
issn = {19481209},
journal = {Marine Technology Society Journal},
keywords = {Autonomous underwater vehicles,Cooperative underwater manipulation systems,Task priority-based control,Underwater manipulation},
number = {4},
pages = {98--107},
publisher = {Marine Technology Society},
title = {{Underwater intervention robotics: An outline of the Italian national project Maris}},
url = {https://www.researchgate.net/profile/Dario{\_}Lodi{\_}Rizzini/publication/307441847{\_}Underwater{\_}Intervention{\_}Robotics{\_}An{\_}Outline{\_}of{\_}the{\_}Italian{\_}National{\_}Project{\_}MARIS/links/580250c808ae6c2449f7f9f8/Underwater-Intervention-Robotics-An-Outline-of-the-Italian-Natio},
volume = {50},
year = {2016}
}
@article{Casalino2016a,
abstract = {The Italian national project MARIS (Marine Robotics for Interventions) pursues the strategic objective of studying, developing, and integrating technologies and methodologies to enable the development of autonomous underwater robotic systems employable for intervention activities. These activities are becoming progressively more typical for the underwater offshore industry, for search-and-rescue operations, and for underwater scientific missions. Within such an ambitious objective, the project consortium also intends to demonstrate the achievable operational capabilities at a proof-of-concept level by integrating the results with prototype experimental systems.},
author = {Casalino, Giuseppe and Caccia, Massimo and Caselli, Stefano and Melchiorri, Claudio and Antonelli, Gianluca and Caiti, Andrea and Indiveri, Giovanni and Cannata, Giorgio and Simetti, Enrico and Torelli, Sandro and Sperind{\`{e}}, Alessandro and Wanderlingh, Francesco and Muscolo, Giovanni and Bibuli, Marco and Bruzzone, Gabriele and Zereik, Enrica and Odetti, Angelo and Spirandelli, Edoardo and Ranieri, Andrea and Aleotti, Jacopo and Rizzini, Dario Lodi and Oleari, Fabio and Kallasi, Fabjan and Palli, Gianluca and Scarcia, Umberto and Moriello, Lorenzo and Cataldi, Elisabetta},
doi = {10.4031/MTSJ.50.4.7},
file = {:C$\backslash$:/Users/giann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Casalino et al. - 2016 - Underwater intervention robotics An outline of the Italian national project Maris.pdf:pdf},
issn = {19481209},
journal = {Marine Technology Society Journal},
keywords = {Autonomous underwater vehicles,Cooperative underwater manipulation systems,Task priority-based control,Underwater manipulation},
number = {4},
pages = {98--107},
publisher = {Marine Technology Society},
title = {{Underwater intervention robotics: An outline of the Italian national project Maris}},
url = {https://www.researchgate.net/profile/Dario{\_}Lodi{\_}Rizzini/publication/307441847{\_}Underwater{\_}Intervention{\_}Robotics{\_}An{\_}Outline{\_}of{\_}the{\_}Italian{\_}National{\_}Project{\_}MARIS/links/580250c808ae6c2449f7f9f8/Underwater-Intervention-Robotics-An-Outline-of-the-Italian-Natio},
volume = {50},
year = {2016}
}
